@article{Cauchy1847method,
author="Cauchy, A.",
title="Methode generale pour la resolution des systemes d'equations simultanees",
journal="C.R. Acad. Sci. Paris",
ISSN="",
publisher="",
year="1847",
month="",
volume="25",
number="",
pages="536-538",
URL="https://ci.nii.ac.jp/naid/10026863174/en/",
DOI="",
}

@article{Kullback51On,
author = {S. Kullback and R. A. Leibler},
title = {{On Information and Sufficiency}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {79 -- 86},
year = {1951},
doi = {10.1214/aoms/1177729694},
URL = {https://doi.org/10.1214/aoms/1177729694}
}


@article{Herbert51stochastic,
author = {Herbert Robbins and Sutton Monro},
title = {{A Stochastic Approximation Method}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {400 -- 407},
year = {1951},
doi = {10.1214/aoms/1177729586},
URL = {https://doi.org/10.1214/aoms/1177729586}
}
@article{Kiefer52stochastic,
author = {J. Kiefer and J. Wolfowitz},
title = {{Stochastic Estimation of the Maximum of a Regression Function}},
volume = {23},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {462 -- 466},
year = {1952},
doi = {10.1214/aoms/1177729392},
URL = {https://doi.org/10.1214/aoms/1177729392}
}

@article{Salton73On,
  title={On the Specification of Term Values in Automatic Indexing},
  author={G. Salton and C. S. Yang},
  journal={Journal of Documentation},
  year={1973},
  volume={29},
  pages={351-372}
}

@article {Tulving82Priming,
        title = {Priming effects in word-fragment completion are independent of recognition memory},
        journal = {Journal of Experimental Psychology: Learning, Memory, \& Cognition},
        volume = {8},
        year = {1982},
        pages = {336-342},
        author = {Tulving, E and D.L. Schacter and Stark, H.A.}
}

@inproceedings{Baum87Supervised,
 author = {Baum, Eric and Wilczek, Frank},
 booktitle = {Neural Information Processing Systems},
 editor = {D. Anderson},
 pages = {},
 publisher = {American Institute of Physics},
 title = {Supervised Learning of Probability Distributions by Neural Networks},
 url = {https://proceedings.neurips.cc/paper/1987/file/eccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf},
 year = {1988}
}

@inbook{Rumelhart88learning,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title = {Learning Representations by Back-Propagating Errors},
year = {1988},
isbn = {0262010976},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Neurocomputing: Foundations of Research},
pages = {696–699},
numpages = {4}
}
@article{Michael89catastrophic,
title = "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
author = "Michael McCloskey and Cohen, {Neal J.}",
year = "1989",
doi = "10.1016/S0079-7421(08)60536-8",
language = "English (US)",
volume = "24",
pages = "109--165",
journal = "Psychology of Learning and Motivation - Advances in Research and Theory",
issn = "0079-7421",
publisher = "Academic Press Inc.",
number = "C",
}

@article{Williams92simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{Wolpert92stacked,
title = "Stacked generalization",
journal = "Neural Networks",
volume = "5",
number = "2",
pages = "241 - 259",
year = "1992",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80023-1",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005800231",
author = "David H. Wolpert",
keywords = "Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction",
abstract = "This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory."
}

@inproceedings{Yarowsky93onesense,
 author = {Yarowsky, David},
 title = {One Sense Per Collocation},
 booktitle = {Proceedings of the Workshop on Human Language Technology},
 series = {HLT '93},
 year = {1993},
 isbn = {1-55860-324-7},
 location = {Princeton, New Jersey},
 pages = {266--271},
 numpages = {6},
 url = {https://doi.org/10.3115/1075671.1075731},
 doi = {10.3115/1075671.1075731},
 acmid = {1075731},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA}
} 
@article{Gage94anew,
author = {Gage, Philip},
title = {A New Algorithm for Data Compression},
year = {1994},
issue_date = {Feb. 1994},
publisher = {R &amp; D Publications, Inc.},
address = {USA},
volume = {12},
number = {2},
issn = {0898-9788},
journal = {C Users J.},
month = feb,
pages = {23–38},
numpages = {16}
}
@article{Caruana97multitask,
 author = {Caruana, Rich},
 title = {Multitask Learning},
 journal = {Mach. Learn.},
 issue_date = {July 1997},
 volume = {28},
 number = {1},
 month = jul,
 year = {1997},
 issn = {0885-6125},
 pages = {41--75},
 numpages = {35},
 url = {https://doi.org/10.1023/A:1007379606734},
 doi = {10.1023/A:1007379606734},
 acmid = {262872},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {backpropagation, generalization, inductive transfer, k-nearest neighbor, kernel regression, multitask learning, parallel transfer, supervised learning}
} 

@article{Hochreiter97long,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@inproceedings{Och98improving,
    title = "Improving Statistical Natural Language Translation with Categories and Rules",
    author = "Och, Franz Josef  and
      Weber, Hans",
    booktitle = "{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics",
    year = "1998",
    url = "https://www.aclweb.org/anthology/C98-2157",
}

@book{Thrun98learning,
 editor = {Thrun, Sebastian and Pratt, Lorien},
 title = {Learning to Learn},
 year = {1998},
 isbn = {0-7923-8047-9},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA}
} 

@article{Shimodaira00improving,
title = "Improving predictive inference under covariate shift by weighting the log-likelihood function",
journal = "Journal of Statistical Planning and Inference",
doi = "10.1016/S0378-3758(00)00115-4",
volume = "90",
number = "2",
pages = "227 - 244",
year = "2000",
issn = "0378-3758",
author = "Hidetoshi Shimodaira"
}

@article{Vilalta01perspective,
author = {Vilalta, Ricardo and Drissi, Youssef},
year = {2001},
month = {09},
pages = {},
title = {A Perspective View And Survey Of Meta-Learning},
volume = {18},
journal = {Artificial Intelligence Review},
doi = {10.1023/A:1019956318069}
}

@article{Lee01Genres,
  title={Genres, Registers, Text Types, Domains and Styles: Clarifying the Concepts and Navigating a Path through the BNC Jungle},
  author={D. Y. Lee},
  journal={Language Learning \altand Technology},
  year={2001},
  volume={5},
  pages={37-72}
}

@article{Jaeger02tutorial,
author = {Jaeger, Herbert},
year = {2002},
month = {01},
pages = {},
title = {Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the echo state network approach},
volume = {5},
journal = {GMD-Forschungszentrum Informationstechnik, 2002.}
}


@inproceedings{Papineni02bleu,
	Address = {Stroudsburg, PA, USA},
	Author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	Booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
	Location = {Philadelphia, Pennsylvania},
	Numpages = {8},
	Pages = {311--318},
	Series = {ACL '02},
	Title = {{BLEU:} a method for automatic evaluation of machine translation},
	Year = {2002}}

@article{Bengio03aneural,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1137–1155},
numpages = {19}
}

@InProceedings{Utiyama03reliable,
  author = 	"Utiyama, Masao
		and Isahara, Hitoshi",
  title = 	"Reliable Measures for Aligning Japanese-English News Articles and Sentences",
  booktitle = 	"Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
  year = 	"2003",
  url = 	"http://aclweb.org/anthology/P03-1010"
}

@article{Villani03topics,
author = {Villani, C},
year = {2003},
month = {01},
pages = {},
title = {Topics in Optimal Transportation Theory},
volume = {58},
doi = {10.1090/gsm/058}
}

@InProceedings{Utiyama03measure,
  author = 	"Utiyama, Masao
		and Isahara, Hitoshi",
  title = 	"Reliable Measures for Aligning {Japanese-English} News Articles and Sentences",
  booktitle = 	"Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
  address =    "Sapporo, Japan",
  year = 	"2003",
  url = 	"http://aclweb.org/anthology/P03-1010"
}
@inproceedings{Koehn04statistical,
    title = "Statistical Significance Tests for Machine Translation Evaluation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-3250",
    pages = "388--395",
}
@inproceedings{Koehn04pharaoh,
author = {Koehn, Philipp},
year = {2004},
month = {09},
pages = {115-124},
title = {Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models},
isbn = {978-3-540-23300-8},
doi = {10.1007/978-3-540-30194-3_13}
}

@InProceedings{Burago04metric,
author="Burago, Yuri
and Shoenthal, David",
editor="Bingham, Kenrick
and Kurylev, Yaroslav V.
and Somersalo, Erkki",
title="Metric Geometry",
booktitle="New Analytic and Geometric Methods in Inverse Problems",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="3--50",
abstract="Much of one's mathematical experience with regard to metric spaces begins at the level of the metric. Instead of starting with a metric, in many cases we must begin with the length of paths as the primary notion. From this, we will derive a distance function. More precisely, we can introduce a new distance which is measured along the shortest path between two points in a space (as opposed to simply measuring the Euclidean distance between the two points). One says that a distance function on a metric space is an intrinsic metric if the distance between two points can be realized by paths connecting the points (mathematically, it must be equal to the infimum of lengths of paths between the points---a shortest path may not exist). If the length of paths is to be our primary notion, we must ask for a rigorous definition, from where it may arise, and what the properties are of such structures (which we will call length structures).",
isbn="978-3-662-08966-8"
}

@inproceedings{Koehn05europarl,
	Address = {Phuket, Thailand},
	Author = {Philipp Koehn},
	Booktitle = {2nd Workshop on EBMT of MT-Summit X},
	Pages = {79--86},
	Title = {Europarl: A Parallel Corpus for {Statistical Machine Translation}},
	Year = 2005}

@article{Daume06domain,
  title={Domain adaptation for statistical classifiers},
  author={Daum\'e {III}, Hal and Marcu, Daniel},
  journal={Journal of Artificial Intelligence Research (JAIR)},
  volume={26},
  pages={101--126},
  year={2006}
}

@InProceedings{Steinberger06acquis,
  author = {Ralf Steinberger and Bruno Pouliquen and Anna Widiger and Camelia Ignat and Tomaž Erjavec and Dan Tufis and Dániel Varga },
  title = {The {JRC-Acquis}: A multilingual aligned parallel corpus with 20+ languages},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation},
  series = {LREC'06},
  year = {2006},
  month = {may},
  date = {24-26},
  address = {Genoa, Italy},
  publisher = {European Language Resources Association (ELRA)},
 }
@inproceedings{Blitzer06Domain,
    title = "Domain Adaptation with Structural Correspondence Learning",
    author = "Blitzer, John  and
      McDonald, Ryan  and
      Pereira, Fernando",
    booktitle = "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-1615",
    pages = "120--128",
}

@inproceedings{Jiang07instance,
    title = "Instance Weighting for Domain Adaptation in {NLP}",
    author = "Jiang, Jing and Zhai, ChengXiang",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-1034",
    pages = "264--271",
}
@phdthesis{Blitzer07domain,
  title={Domain adaptation of natural language processing systems},
  author={Blitzer, John},
  school={School of Computer Science, University of Pennsylvania},
  year={2007}
}
@incollection{Ben07analysis,
title = {Analysis of Representations for Domain Adaptation},
author = {Ben-David, Shai and John Blitzer and Crammer, Koby and Fernando Pereira},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {137--144},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation.pdf}
}

@InProceedings{Daume07frustratingly,
  author = 	"Daume III, Hal",
  title = 	"Frustratingly Easy Domain Adaptation",
  booktitle = 	"Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
  year = 	"2007",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"256--263",
  location = 	"Prague, Czech Republic",
  url = 	"http://aclweb.org/anthology/P07-1033"
}

@inproceedings{Foster07mixture,
	Address = {Prague, Czech Republic},
	Author = {Foster, George and Kuhn, Roland},
	Booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
	Pages = {128--135},
	Title = {Mixture-Model Adaptation for {SMT}},
	Url = {http://www.aclweb.org/anthology/W/W07/W07-0717},
	Year = {2007},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/W/W07/W07-0717}}

@inproceedings{Collobert08aunified,
author = {Collobert, Ronan and Weston, Jason},
title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390177},
doi = {10.1145/1390156.1390177},
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {160–167},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@Book{Quinonero08dataset,
  editor = 	 {Joaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer and Neil D. Lawrence},
  title = 	 {Dataset Shift in Machine Learning},
  publisher = 	 {MIT Press},
  year = 	 2008,
  series = 	 {Neural Information Processing series},
  note = 	 "ISBN : 9780262170055"}

@inproceedings{Dredze08online,
	Address = {Honolulu, Hawaii},
	Author = {Dredze, Mark and Crammer, Koby},
	Booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
	Month = oct,
	Pages = {689--697},
	series = {EMNLP},
	Title = {Online Methods for Multi-Domain Learning and Adaptation},
	Url = {https://www.aclweb.org/anthology/D08-1072},
	Year = {2008},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D08-1072}}

@inproceedings{Bertoldi09domain,
    title = "Domain Adaptation for Statistical Machine Translation with Monolingual Resources",
    author = "Bertoldi, Nicola and Federico, Marcello",
    booktitle = "Proceedings of the Fourth Workshop on Statistical Machine Translation",
    month = mar,
    year = "2009",
    address = "Athens, Greece",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W09-0432",
    pages = "182--189",
}

@incollection{Mansour09domain,
title = {Domain Adaptation with Multiple Sources},
author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1041--1048},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf}
}

@Article{Pan10asurvey,
author={Sinno Jialin {Pan} and Qiang {Yang}},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Survey on Transfer Learning},
doi = {doi: 10.1109/TKDE.2009.191},
year={2010},
volume={22},
number={10},
pages={1345-1359},
}

@InCollection{Tiedemann09news,
  author =	  {J\"org Tiedemann},
  title =	  {News from {OPUS} - {A} Collection of Multilingual
                  Parallel Corpora with Tools and Interfaces},
  booktitle =	  {Recent Advances in Natural Language Processing},
  publisher =	  {John Benjamins, Amsterdam/Philadelphia},
  year =          2009,
  pages =         {237--248},
  editor =        {N. Nicolov and K. Bontcheva and G. Angelova and
                  R. Mitkov},
  volume =	  {V},
  address =	  {Borovets, Bulgaria},
  isbn =          {978 90 272 4825 1},
  pdf =           {http://stp.lingfil.uu.se/~joerg/published/ranlp-V.pdf},
  topic  =        {Parallel corpora}
}

@inproceedings{Bengio09curriculum,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum Learning},
year = {2009},
isbn = {9781605585161},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{Finkel09hierarchical,
	Address = {Boulder, Colorado},
	Author = {Finkel, Jenny Rose and Manning, Christopher D.},
	Booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics},
	Month = jun,
	Pages = {602--610},
	Title = {Hierarchical {B}ayesian Domain Adaptation},
	Url = {https://www.aclweb.org/anthology/N09-1068},
	Year = {2009},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/N09-1068}}

@InProceedings{Finkel09hierarchical,
  author = 	"Finkel, Jenny Rose
		and Manning, Christopher D.",
  title = 	"Hierarchical Bayesian Domain Adaptation",
  booktitle = 	"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
  year = 	"2009",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"602--610",
  location = 	"Boulder, Colorado",
  url = 	"http://aclweb.org/anthology/N09-1068"
}

@inproceedings{Mansour09multiple,
title	= {Multiple Source Adaptation and the Renyi Divergence},
author	= {Yishay Mansour and Mehryar Mohri and Afshin Rostamizadeh},
year	= {2009},
URL	= {http://www.cs.nyu.edu/~mohri/postscript/renyi.pdf},
booktitle	= {Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence  (UAI 2009)},
address	= {Montr\'eal, Canada}
}
@conference{Yishay09domain,
title = "Domain adaptation: Learning bounds and algorithms",
abstract = "This paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data. Building on previous work by Ben-David et al. (2007), we introduce a novel distance between distributions, discrepancy distance, that is tailored to adaptation problems with arbitrary loss functions. We give Rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions. Using this distance, we derive new generalization bounds for domain adaptation for a wide family of loss functions. We also present a series of novel adaptation bounds for large classes of regularization-based algorithms, including support vector machines and kernel ridge regression based on the empirical discrepancy. This motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give several algorithms. We report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation.",
author = "Yishay Mansour and Mehryar Mohri and Afshin Rostamizadeh",
year = "2009",
month = dec,
day = "1",
language = "English (US)",
note = "22nd Conference on Learning Theory, COLT 2009 ; Conference date: 18-06-2009 Through 21-06-2009",
}

@inproceedings{Daume09bayes,
 author = {Daum{\'e},III, Hal},
 title = {Bayesian Multitask Learning with Latent Hierarchies},
 booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI '09},
 year = {2009},
 isbn = {978-0-9749039-5-8},
 location = {Montreal, Quebec, Canada},
 pages = {135--142},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1795114.1795131},
 acmid = {1795131},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States}
} 

@book{Mccandless2010Lucene,
 author = {McCandless, Michael and Hatcher, Erik and Gospodnetic, Otis},
 title = {Lucene in Action, Second Edition: Covers Apache Lucene 3.0},
 year = {2010},
 isbn = {1933988177, 9781933988177},
 publisher = {Manning Publications Co.},
 address = {Greenwich, CT, USA}
}

@article{Ben10A,
	Author = {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jenn Wortman},
	Issue = {Special Issue on Learning from Multiple Sources},
	Volume = {79},
	pages = {151-175},
	Journal = {Machine Learning},
	Title = {A Theory of Learning from Different Domains},
	Number = {1},
        url = {https://doi.org/10.1007/s10994-009-5152-4},
	Year = {2010}}

@inproceedings{Nair10rectified,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{Koehn10convergence,
  added-at = {2015-12-09T21:07:47.000+0100},
  address = {Denver},
  author = {Koehn, Philipp and Senellart, Jean},
  biburl = {https://www.bibsonomy.org/bibtex/2b402083a345a017e98f3c39131e4cf08/evalopezsainz},
  booktitle = {Proceedings of AMTA Workshop on MT Research and the Translation Industry},
  interhash = {ee8b34c75d3ec178b8fc84f014b82b94},
  intrahash = {b402083a345a017e98f3c39131e4cf08},
  keywords = {memorias_de_traducci<U+0097>n tecnolog<U+0092>as_de_la_traducci<U+0097>n traducci<U+0097>n_autom<U+0087>tica},
  mendeley-tags = {machine translation,translation memory,translation technologies},
  pages = {21--31},
  timestamp = {2015-12-10T10:02:09.000+0100},
  title = {{Convergence of Translation Memory and Statistical Machine Translation}},
  url = {http://homepages.inf.ed.ac.uk/pkoehn/publications/tm-smt-amta2010.pdf},
  year = 2010
}

@InProceedings{Banerjee10combining,
  author = 	 {Banerjee, Pratyush and Du, Jinhua and Li, Baoli and Kumar Naskar, Sudip and Way, Andy and van Genabith, Josef},
  title = 	 {Combining multi-domain statistical machine translation models using automatic classifiers},
  booktitle = {Proceedings of the  9th Conference of the Association for Machine Translation in the Americas},
  year = 	 2010,
  series = 	 {AMTA 2010},
  address = 	 {Denver, CO, USA}
}

@InProceedings{Moore10intelligent,
  author = 	"Moore, Robert C.
		and Lewis, William",
  title = 	"Intelligent Selection of Language Model Training Data",
  booktitle = 	"Proceedings of the ACL 2010 Conference Short Papers",
  year = 	"2010",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"220--224",
  location = 	"Uppsala, Sweden",
  url = 	"http://aclweb.org/anthology/P10-2041"
}

@inproceedings{Paul10overview,
  title={Overview of the {IWSLT} 2010 evaluation campaign},
  author={Paul, Michael and Federico, Marcello and St{\"u}ker, Sebastian},
  booktitle={International Workshop on Spoken Language Translation (IWSLT) 2010},
  series = {IWSLT},
  address = {Paris, France},
  pages     = {3--27},
  year      = {2010},
  url = {https://www.isca-speech.org/archive/iwslt_10/papers/slta_003.pdf}
}  

@article{Shai10A,
title	= {A theory of learning from different domains},
author	= {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jennifer Vaughan},
year	= {2010},
URL	= {http://www.springerlink.com/content/q6qk230685577n52/},
journal	= {Machine Learning},
pages	= {151--175},
volume	= {79}
}

@inproceedings{Chang10necessity,
	Address = {Cambridge, MA},
	Author = {Chang, Ming-Wei and Connor, Michael and Roth, Dan},
	Booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	Month = oct,
	Pages = {767--777},
	Title = {The Necessity of Combining Adaptation Methods},
	Url = {https://www.aclweb.org/anthology/D10-1075},
	Year = {2010},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D10-1075}}

@INPROCEEDINGS{Bottou10large,
    author = {Léon Bottou},
    title = {Large-scale machine learning with stochastic gradient descent},
    booktitle = {in COMPSTAT},
    year = {2010}
}


@InProceedings{Glorot10understanding, 
title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Glorot, Xavier and Bengio, Yoshua}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {249--256}, year = {2010}, editor = {Teh, Yee Whye and Titterington, Mike}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = { http://proceedings.mlr.press/v9/glorot10a.html }, abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} }

@inproceedings{Foster10discriminative,
    title = "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation",
    author = "Foster, George  and
      Goutte, Cyril  and
      Kuhn, Roland",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D10-1044",
    pages = "451--459",
}

@inproceedings{Daume11domain,
    title = "Domain Adaptation for Machine Translation by Mining Unseen Words",
    author = "Daum{\'e} III, Hal  and
      Jagarlamudi, Jagadeesh",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P11-2071",
    pages = "407--412",
}

@inproceedings{Allauzen11bayesian,
author = {Allauzen, Cyril and Riley, Michael},
year = {2011},
month = {01},
pages = {1429-1432},
title = {Bayesian Language Model Interpolation for Mobile Speech Input.},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH}
}

	
@InProceedings{Axelrod11domain,
  author = 	"Axelrod, Amittai
		and He, Xiaodong
		and Gao, Jianfeng",
  title = 	"Domain Adaptation via Pseudo In-Domain Data Selection",
  booktitle = 	"Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2011",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"355--362",
  location = 	"Edinburgh, Scotland, UK.",
  url = 	"http://aclweb.org/anthology/D11-1033"
}

@article{Collobert11natural,
  author  = {Ronan Collobert and Jason Weston and L{{\'e}}on Bottou and Michael Karlen and Koray Kavukcuoglu and Pavel Kuksa},
  title   = {Natural Language Processing (Almost) from Scratch},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {76},
  pages   = {2493-2537},
  url     = {http://jmlr.org/papers/v12/collobert11a.html}
}

@InProceedings{Lambert11investigation,
  author = 	"Lambert, Patrik
		and Schwenk, Holger
		and Servan, Christophe
		and Abdul-Rauf, Sadaf",
  title = 	"Investigations on Translation Model Adaptation Using Monolingual Data",
  booktitle = 	"Proceedings of the Sixth Workshop on Statistical Machine Translation",
  year = 	"2011",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"284--293",
  location = 	"Edinburgh, Scotland",
  url = 	"http://aclweb.org/anthology/W11-2132"
}

@article{Memoli11gromov,
author = {Mémoli, Facundo},
year = {2011},
month = {08},
pages = {417-487},
title = {Gromov–Wasserstein Distances and the Metric Approach to Object Matching},
volume = {11},
journal = {Foundations of Computational Mathematics},
doi = {10.1007/s10208-011-9093-5}
}
@InProceedings{Mansour12simple,
author= {Mansour, Saab and Ney, Hermann},
title= {A Simple and Effective Weighted Phrase Extraction for Machine Translation Adaptation},
booktitle= {International Workshop on Spoken Language Translation},
year= 2012,
pages= {193-200},
address= {Hong Kong},
month= dec,
booktitlelink= {http://hltc.cs.ust.hk/iwslt/},
pdf = {https://www-i6.informatik.rwth-aachen.de/publications/downloader.php?id=832&row=pdf}
}

@inproceedings{tiedemann12parallel,
    title = "Parallel Data, Tools and Interfaces in {OPUS}",
    author = {Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf",
    pages = "2214--2218",
}

@inproceedings{Cettolo12wit,
        Address = {Trento, Italy},
        Author = {Mauro Cettolo and Christian Girardi and Marcello Federico},
        Booktitle = {Proceedings of the 16$^{th}$ Conference of the European Association for Machine Translation (EAMT)},
        Date = {28-30},
        Month = {May},
        Pages = {261--268},
        Title = {WIT$^3$: Web Inventory of Transcribed and Translated Talks},
        Year = {2012},
}

@inproceedings{Sennrich12perplexity,
    title = "Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation",
    author = "Sennrich, Rico",
    booktitle = "Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2012",
    address = "Avignon, France",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E12-1055",
    pages = "539--549",
}

@InProceedings{Tiedemann12parallel,
  author = {J\"org Tiedemann},
  title = {Parallel Data, Tools and Interfaces in {OPUS}},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation},
  series = {LREC'12},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
 }
 
@inproceedings{Schwenk12continuous,
    title = "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation",
    author = "Schwenk, Holger",
    booktitle = "Proceedings of {COLING} 2012: Posters",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C12-2104",
    pages = "1071--1080",
}

@inproceedings{Eidelman12topic,
    title = "Topic Models for Dynamic Translation Model Adaptation",
    author = "Eidelman, Vladimir and Boyd-Graber, Jordan and Resnik, Philip",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-2023",
    pages = "115--119",
}

@inproceedings{Le12continuous,
    title = "Continuous Space Translation Models with Neural Networks",
    author = "Le, Hai Son  and
      Allauzen, Alexandre  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N12-1005",
    pages = "39--48",
}

@inproceedings{Mike12japanese,
title	= {Japanese and Korean Voice Search},
author	= {Mike Schuster and Kaisuke Nakajima},
year	= {2012},
booktitle	= {International Conference on Acoustics, Speech and Signal Processing},
pages	= {5149--5152}
}

@inproceedings{Sennrich12mixture,
    title = "Mixture-Modeling with Unsupervised Clusters for Domain Adaptation in Statistical Machine Translation",
    author = "Sennrich, Rico",
    booktitle = "Proceedings of the 16th Annual conference of the European Association for Machine Translation",
    month = may # " 28{--}30",
    year = "2012",
    address = "Trento, Italy",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2012.eamt-1.43",
    pages = "185--192",
}

@inproceedings{Clark12onesystem,
  title={One system, many domains: Open-domain statistical machine translation via feature augmentation},
  author={Clark, Jonathan H. and Lavie, Alon and Dyer, Chris},
  year={2012},
  Address = {San Diego, CA},
  Booktitle = {Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas},
  Series = {(AMTA 2012)}
}

@inproceedings{Joshi12multidomain,
        Abstract = {We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multi-domain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced label setting, although in practice many multi-domain settings have domain-specific label biases. When multi-domain learning is applied to these settings, (2) are multi-domain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.},
        Author = {Mahesh Joshi and Mark Dredze and William W Cohen and Carolyn P Rose},
        Booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
        Pages = {1302-1312},
        Title = {Multi-Domain Learning: When Do Domains Matter?},
        Year = {2012}
} 

@inproceedings{Pascanu13onthe,
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
title = {On the Difficulty of Training Recurrent Neural Networks},
year = {2013},
publisher = {JMLR.org},
abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–1310–III–1318},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{BenTal13robust,
  title={Robust Solutions of Optimization Problems Affected by Uncertain Probabilities},
  author={A. Ben-Tal and D. D. Hertog and A. D. Waegenaere and B. Melenberg and G. Rennen},
  journal={Manag. Sci.},
  year={2013},
  volume={59},
  pages={341-357}
}

@inproceedings{Cuturi13sinkhorn,
 author = {Cuturi, Marco},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {2292--2300},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999868},
 acmid = {2999868},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@inproceedings{Mikolov13distributed,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}

@inproceedings{Mikolov13efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and G. Corrado and J. Dean},
  booktitle={ICLR},
  year={2013}
}


@InProceedings{Duh13adaptation,
  author = 	"Duh, Kevin
		and Neubig, Graham
		and Sudoh, Katsuhito
		and Tsukada, Hajime",
  title = 	"Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation",
  booktitle = 	"Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"678--683",
  location = 	"Sofia, Bulgaria",
  url = 	"http://aclweb.org/anthology/P13-2119"
}

@InProceedings{Kalchbrenner13recurrent,
  author = 	"Kalchbrenner, Nal
		and Blunsom, Phil",
  title = 	"Recurrent Continuous Translation Models",
  booktitle = 	"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1700--1709",
  location = 	"Seattle, Washington, USA",
  url = 	"http://aclweb.org/anthology/D13-1176"
}
@article{Irvine13measuring,
author = {Irvine, Ann and Morgan, John and Carpuat, Marine and Daumé, Hal and Munteanu, Dragos},
title = {Measuring Machine Translation Errors in New Domains},
journal = {Transactions of the Association for Computational Linguistics},
volume = {1},
number = {},
pages = {429-440},
year = {2013},
doi = {10.1162/tacl\_a\_00239},
URL = {https://doi.org/10.1162/tacl_a_00239},
abstract = { We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation. }
}
@inproceedings{Sennrich13multidomain,
	Address = {Sofia, Bulgaria},
	Author = {Sennrich, Rico and Schwenk, Holger and Aransa, Walid},
	Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Date-Added = {2019-05-14 22:42:31 +0200},
	Date-Modified = {2019-05-14 22:42:31 +0200},
	Month = aug,
	Pages = {832--840},
	Publisher = {Association for Computational Linguistics},
	Title = {A Multi-Domain Translation Model Framework for Statistical Machine Translation},
	Url = {https://www.aclweb.org/anthology/P13-1082},
	Year = {2013},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P13-1082}}

@inproceedings{Rico13multi,
  author    = {Rico Sennrich and
               Holger Schwenk and
               Walid Aransa},
  title     = {A Multi-Domain Translation Model Framework for Statistical Machine
               Translation},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2013, 4-9 August 2013, Sofia, Bulgaria, Volume
               1: Long Papers},
  pages     = {832--840},
  year      = {2013},
  crossref  = {DBLP:conf/acl/2013-1},
  url       = {https://www.aclweb.org/anthology/P13-1082/},
  timestamp = {Mon, 19 Aug 2019 18:10:05 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/SennrichSA13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Duh13selection,
  author = 	"Duh, Kevin
		and Neubig, Graham
		and Sudoh, Katsuhito
		and Tsukada, Hajime",
  title = 	"Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation",
  booktitle = 	"Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"678--683",
  address = 	"Sofia, Bulgaria",
  url = 	"http://aclweb.org/anthology/P13-2119"
}

@article{Ann13measuring,
  author    = {Ann Irvine and
               John Morgan and
               Marine Carpuat and
               Hal Daum{\'{e}} III and
               Dragos Stefan Munteanu},
  title     = {Measuring Machine Translation Errors in New Domains},
  journal   = {{TACL}},
  volume    = {1},
  pages     = {429--440},
  year      = {2013},
  url       = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/111},
  timestamp = {Thu, 28 May 2015 17:23:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tacl/IrvineMCDM13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mikolov13efficient,
  author    = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  editor    = {Yoshua Bengio and Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Thu, 25 Jul 2019 14:25:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{mikolov13distributed,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@phdthesis{Rico13domain,
           title = {Domain adaptation for translation models in statistical machine translation},
          school = {University of Zurich},
          author = {Rico Sennrich},
            year = {2013},
        language = {english},
             url = {https://doi.org/10.5167/uzh-88574},
        abstract = {We investigate methods to adapt translation models in SMT to a specific target domain. We discuss two major problems, unknown words because of data sparseness in the (in-domain) training data, and ambiguities arising from out-of-domain parallel texts with different domain-specific translations. We propose novel solutions to both problems.
The main contributions of this thesis are as follows:
* We present a novel translation model architecture that supports domain adaptation at decoding time from a vector of component models. The combination is implemented through instance weighting, and all statistics necessary for the computation of translation probabilities are stored in the models.
* We present an architecture to combine multiple MT systems, using techniques and ideas from domain adaptation. The hypotheses by external MT systems are treated as out-of-domain knowledge, and combined with in-domain data through instance weighting.
* We introduce a sentence alignment algorithm that is able to robustly align even noisy parallel texts. We found that higher-quality sentence alignment of the in-domain parallel text has a significant effect on translation quality in our target domain.
* We propose new translation model features that express how flexible, or general, translation units are, in order to prevent translations that only occur in the context of multiword expressions from being overgeneralised.}
}

@inproceedings{Pennington14glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@inproceedings{Cho14properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@InProceedings{Wang14neural,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation ",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"189--195",
  location = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1023",
  url = 	"http://aclweb.org/anthology/D14-1023"
}
@inproceedings{Hasler14dynamic,
    title = "Dynamic Topic Adaptation for Phrase-based {MT}",
    author = "Hasler, Eva  and
      Blunsom, Phil  and
      Koehn, Philipp  and
      Haddow, Barry",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E14-1035",
    doi = "10.3115/v1/E14-1035",
    pages = "328--337",
}
@inproceedings{Cuong14latent,
  author    = {Cuong Hoang and
               Khalil Sima'an},
  title     = {Latent Domain Translation Models in Mix-of-Domains Haystack},
  booktitle = {{COLING} 2014, 25th International Conference on Computational Linguistics,
               Proceedings of the Conference: Technical Papers, August 23-29, 2014,
               Dublin, Ireland},
  pages     = {1928--1939},
  year      = {2014},
  crossref  = {DBLP:conf/coling/2014},
  url       = {https://www.aclweb.org/anthology/C14-1182/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/coling/HoangS14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Carpuat14linear,
    title = "Linear Mixture Models for Robust Machine Translation",
    author = "Carpuat, Marine  and
      Goutte, Cyril  and
      Foster, George",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3363",
    doi = "10.3115/v1/W14-3363",
    pages = "499--509",
}

@InProceedings{Hoang14latent,
  author = 	"Hoang, Cuong
		and Sima'an, Khalil",
  title = 	"Latent Domain Translation Models in Mix-of-Domains Haystack",
  booktitle = 	"Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2014",
  publisher = 	"Dublin City University and Association for Computational Linguistics",
  pages = 	"1928--1939",
  location = 	"Dublin, Ireland",
  url = 	"http://aclweb.org/anthology/C14-1182"
}

@inproceedings{Bahdanau14neural,
	Author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proc. {ICLR}},
	Date-Added = {2015-10-05 20:27:54 +0000},
	Date-Modified = {2015-10-08 23:13:02 +0000},
	Title = {Neural machine translation by jointly learning to align and translate},
	Year = {2015}
}

@inproceedings{Sutskever14sequence,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
 title = {Sequence to Sequence Learning with Neural Networks},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {3104--3112},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
 acmid = {2969173},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
 } 
 
@book{Nesterov14introductory,
author = {Nesterov, Yurii},
title = {Introductory Lectures on Convex Optimization: A Basic Course},
year = {2014},
isbn = {1461346916},
publisher = {Springer Publishing Company, Incorporated},
edition = {1},
abstract = {It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].}
}

@InProceedings{Bojar14findings,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\v{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@inproceedings{Diederick14auto,
  author    = {Diederik P. Kingma and
               Max Welling},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  crossref  = {DBLP:conf/iclr/2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
	
@InProceedings{Wang14neural,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation ",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"189--195",
  address = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1023",
  url = 	"http://aclweb.org/anthology/D14-1023"
}
@inproceedings{Chen14systematic,
    title = "A Systematic Comparison of Smoothing Techniques for Sentence-Level {BLEU}",
    author = "Chen, Boxing  and
      Cherry, Colin",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3346",
    doi = "10.3115/v1/W14-3346",
    pages = "362--367",
}

@article{Srivastava14dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{Huck15mixed,
  title={Mixed-Domain vs. Multi-Domain Statistical Machine Translation},
  author={Matthias Huck and Alexandra Birch and Barry Haddow},
  year={2015}
}

@inproceedings{Luong15stanford,
  Address = {Da Nang, Vietnam},
  Author = {Luong, Minh-Thang  and Manning, Christopher D.},
  Booktitle = {International Workshop on Spoken Language Translation},
  Title = {Stanford Neural Machine Translation Systems for Spoken Language Domain},
  Year = {2015}}

@inproceedings{Ranzato15sequence,
  author    = {Marc'Aurelio Ranzato and
               Sumit Chopra and
               Michael Auli and
               Wojciech Zaremba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06732},
  timestamp = {Thu, 25 Jul 2019 14:25:39 +0200}
}

@inproceedings{Kingma15adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Durrani15using,
  title={Using Joint Models for Domain Adaptation in Statistical Machine Translation},
  author={Nadir Durrani and Hassan Sajjad and Shafiq R. Joty and Ahmed Abdelali and Stephan Vogel},
  year={2015}
}  
@InProceedings{Huck15mixeddomain,
  author = 	 {Matthias Huck and Alexandra Birch and Barry Haddow},
  title = 	 {Mixed domain vs. multi-domain statistical machine translation},
  year = 	 2015,
  booktitle = 	 {Procedings of the Machine Translation Summit},
  series = 	 {MT Summit XV},
  address = 	 {Miami Florida},
  url = 	 {http://www.mt-archive.info/15/MTS-2015-Huck.pdf},
  pages = 	 {240--255}
}
@article{Wang15character,
  author    = {Wang Ling and
               Isabel Trancoso and
               Chris Dyer and
               Alan W. Black},
  title     = {Character-based Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1511.04586},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.04586},
  archivePrefix = {arXiv},
  eprint    = {1511.04586},
  timestamp = {Mon, 13 Aug 2018 16:48:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LingTDB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hinton15Distilling,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
URL	= {http://arxiv.org/abs/1503.02531},
booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}

@inproceedings{Wees15whats,
    title = "What{'}s in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Weerkamp, Wouter  and
      Monz, Christof",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2092",
    doi = "10.3115/v1/P15-2092",
    pages = "560--566",
}

@inproceedings{Ioffe15batch,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{Yang15unified,
	title = {A unified perspective on multi-domain and multi-task learning},
	author = {Yongxin Yang and Timothy M. Hospedales},
	booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	year = {2015},
	url = {https://arxiv.org/abs/1412.7489},
}

@inproceedings{Bahdanau15learning,
	Author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	Title = {Neural machine translation by jointly learning to align and translate},
	Year = {2015},
	url = {https://arxiv.org/pdf/1409.0473.pdf},
}

@inproceedings{Sennrich16neural,
	Address = {Berlin, Germany},
	Author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Doi = {10.18653/v1/P16-1162},
	Month = aug,
	Pages = {1715--1725},
	Title = {Neural Machine Translation of Rare Words with Subword Units},
	Url = {https://www.aclweb.org/anthology/P16-1162},
	Year = {2016},
}

@InProceedings{Sennrich16politeness,
  author = 	"Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  address = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Zhang16topicinformed,
  author = 	"Zhang, Jian and Li, Liangyou and Way, Andy and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers",
  series =      "COLING 2016",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@inproceedings{Chen16guided,
  title={Guided Alignment Training for Topic-Aware Neural Machine Translation},
  author={Wenhu Chen and Evgeny Matusov and Shahram Khadivi and Jan-Thorsten Peter},
  address = {Austin, Texas},
  year={2016},
  Booktitle = {Proceedings of the Twelth Biennial Conference of the Association for Machine Translation in the Americas},
  Series = {AMTA 2012}
}

@inproceedings{He16deep,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
year = {2016},
month = {06},
pages = {770-778},
title = {Deep Residual Learning for Image Recognition},
doi = {10.1109/CVPR.2016.90}
}

@article{Jimmy16layer,
  title={Layer Normalization},
  author={Jimmy Ba and J. Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@InProceedings{Wang16connecting,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Connecting Phrase based Statistical Machine Translation Adaptation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"3135--3145",
  location = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1295"
}

@inproceedings{Bousmalis16domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA}
} 

@inproceedings{Costa16character,
    title = "Character-based Neural Machine Translation",
    author = "Costa-juss{\`a}, Marta R.  and
      Fonollosa, Jos{\'e} A. R.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2058",
    doi = "10.18653/v1/P16-2058",
    pages = "357--361",
}

@inproceedings{Luong16achieving,
    title = "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models",
    author = "Luong, Minh-Thang  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1100",
    doi = "10.18653/v1/P16-1100",
    pages = "1054--1063",
}

@inproceedings{Liu16stein,
 author = {Liu, Qiang and Wang, Dilin},
 title = {Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {2378--2386},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157362},
 acmid = {3157362},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@inproceedings{Brebisson16anexploration,
  author={Alexandre de Brébisson and Pascal Vincent},
  title={An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family},
  year={2016},
  url={http://arxiv.org/abs/1511.05042},
  booktitle={Proceedings of the 4th International Conference on Learning Representation},
  series = {ICLR},
  editor   = {Yoshua Bengio and Yann LeCun},
  address  = {San Juan, Puerto Rico},
  year      = {2016},
}

@InProceedings{Sennrich16controlling,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  location = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Sennrich16improving,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Improving Neural Machine Translation Models with Monolingual Data",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"86--96",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1009",
  url = 	"http://aclweb.org/anthology/P16-1009"
}

@InProceedings{Sennrich16neural,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Neural Machine Translation of Rare Words with Subword Units",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1715--1725",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1162",
  url = 	"http://aclweb.org/anthology/P16-1162"
}

@InProceedings{Liu16deep,
  author = 	"Liu, Pengfei
		and Qiu, Xipeng
		and Huang, Xuanjing",
  title = 	"Deep Multi-Task Learning with Shared Memory for Text Classification",
  booktitle = 	"Proceedings of the 2016 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--127",
  location = 	"Austin, Texas",
  doi = 	"10.18653/v1/D16-1012",
  url = 	"http://aclweb.org/anthology/D16-1012"
}

@ARTICLE{Gulcehre16monolingual,
    author = {G{\"{u}}l{\c c}ehre, {\c C}ağlar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Lo{\"{\i}}c and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computer Science - Computation and Language},
     month = mar,
     title = {On Using Monolingual Corpora in Neural Machine Translation},
   journal = {arXiv e-prints},
    volume = {abs/1503.03535},
      year = {2015},
       url = {https://arxiv.org/abs/1503.03535}
}

@InProceedings{Zhang16topic,
  author = 	"Zhang, Jian
		and Li, Liangyou
		and Way, Andy
		and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  location = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@inproceedings{Ha16towards,
	Address = {Vancouver, Canada},
	Author = {Ha, Thanh-He and Niehues, Jan and Waibel, Alex},
	Booktitle = {Proceedings of the International Workshop on Spoken Language Translation},
	Date-Added = {2019-05-19 16:26:43 +0200},
	Date-Modified = {2019-05-19 16:28:13 +0200},
	Organization = {IWSLT},
	Title = {Toward Multilingual Neural Machine Translationwith Universal Encoder and Decoder},
	Year = {2016}}

@inproceedings{Firat16multiway,
	Author = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	Doi = {10.18653/v1/N16-1101},
	Location = {San Diego, California},
	Pages = {866--875},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism},
	Url = {http://www.aclweb.org/anthology/N16-1101},
	Year = {2016},
}
@InProceedings{Sennrich16politeness,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  address = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Wang16connecting,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Connecting Phrase based Statistical Machine Translation Adaptation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"3135--3145",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1295"
}

@inproceedings{rebecca16neural,
author = "Rebecca Knowles and Philipp Koehn",
title = "Neural Interactive Translation Prediction",
year = 2016,
booktitle = "Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA)"
}

@ARTICLE{Gulcehre16monolingual,
    author = {G{\"{u}}l{\c c}ehre, {\c C}ağlar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Lo{\"{\i}}c and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computer Science - Computation and Language},
     month = mar,
     title = {On Using Monolingual Corpora in Neural Machine Translation},
   journal = {arXiv e-prints},
    volume = {abs/1503.03535},
      year = {2015},
       url = {https://arxiv.org/abs/1503.03535}
}

@inproceedings{Sennrich16improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@article{Papernot16distillation,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Nicolas Papernot and P. Mcdaniel and Xi Wu and S. Jha and A. Swami},
  journal={2016 IEEE Symposium on Security and Privacy (SP)},
  year={2016},
  pages={582-597}
}

@InProceedings{Zhang16topicinformed,
  author = 	"Zhang, Jian
		and Li, Liangyou
		and Way, Andy
		and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers",
  series =      "COLING 2016",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@article{Freitag16fast,
  title={Fast Domain Adaptation for Neural Machine Translation},
  author={Markus Freitag and Yaser Al-Onaizan},
  journal={CoRR},
  year={2016},
  volume={abs/1612.06897}
}
@ARTICLE{Kirk16overcoming,
       author = {{Kirkpatrick}, James and {Pascanu}, Razvan and {Rabinowitz}, Neil and
         {Veness}, Joel and {Desjardins}, Guillaume and {Rusu}, Andrei A. and
         {Milan}, Kieran and {Quan}, John and {Ramalho}, Tiago and
         {Grabska-Barwinska}, Agnieszka and {Hassabis}, Demis and
         {Clopath}, Claudia and {Kumaran}, Dharshan and {Hadsell}, Raia},
        title = "{Overcoming catastrophic forgetting in neural networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = "2016",
        month = "Dec",
          eid = {arXiv:1612.00796},
        pages = {arXiv:1612.00796},
archivePrefix = {arXiv},
       eprint = {1612.00796},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161200796K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Niehues16pretranslation,
    title = "Pre-Translation for Neural Machine Translation",
    author = "Niehues, Jan  and
      Cho, Eunah  and
      Ha, Thanh-Le  and
      Waibel, Alex",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1172",
    pages = "1828--1836",
}

@inproceedings{Chung16character,
    title = "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation",
    author = "Chung, Junyoung  and
      Cho, Kyunghyun  and
      Bengio, Yoshua",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1160",
    doi = "10.18653/v1/P16-1160",
    pages = "1693--1703",
}

@inproceedings{Konstantinos16domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 address = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA}
} 

@article{Li16mutual,
  title={Mutual Information and Diverse Decoding Improve Neural Machine Translation},
  author={Jiwei Li and Dan Jurafsky},
  journal={ArXiv},
  year={2016},
  volume={abs/1601.00372}
}

@inproceedings{Bousmalis16Domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@article{Servan16Domain,
  title={Domain specialization: a post-training domain adaptation for Neural Machine Translation},
  author={Christophe Servan and J. Crego and Jean Senellart},
  journal={ArXiv},
  year={2016},
  volume={abs/1612.06141}
}

@inproceedings{Chen16Bilingual,
  title={Bilingual Methods for Adaptive Training Data Selection for Machine Translation},
  author={Boxing Chen and Roland Kuhn and George F. Foster and Colin Cherry and Fei Huang},
  url = {https://amtaweb.org/wp-content/uploads/2016/10/AMTA2016_Research_Proceedings_v7.pdf#page=99},
  year={2016}
}

@article{Kenji17multi,
  title={Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation},
  author={Kenji Imamura and Eiichiro Sumita},
  journal={Journal of Natural Language Processing},
  volume={24},
  number={4},
  pages={597-618},
  year={2017},
  doi={10.5715/jnlp.24.597}
}

@inproceedings{Dakwle17fine,
  author    = {Dakwale, Praveen and Monz, Christof},
  title     = {Fine-Tuning for Neural Machine Translation with Limited Degradation across In- and Out-of-Domain Data},
  booktitle = {Proceedings of the 16th Machine Translation Summit (MT-Summit 2017)},
  pages = {156--169},
  year      = {2017}
}

@inproceedings{Tu17neural,
author = {Tu, Zhaopeng and Liu, Yang and Shang, Lifeng and Liu, Xiaohua and Li, Hang},
title = {Neural Machine Translation with Reconstruction},
year = {2017},
publisher = {AAAI Press},
abstract = {Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress
in the past two years, it suffers from a major drawback: translations generated by
NMT systems often lack of adequacy. It has been widely observed that NMT tends to
repeatedly translate some source words while mistakenly ignoring other words. To alleviate
this problem, we propose a novel encoder-decoder-reconstructor framework for NMT.
The reconstructor, incorporated into the NMT model, manages to reconstruct the input
source sentence from the hidden layer of the output target sentence, to ensure that
the information in the source side is transformed to the target side as much as possible.
Experiments show that the proposed framework significantly improves the adequacy of
NMT output and achieves superior translation result over state-of-the-art NMT and
statistical MT systems.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3097–3103},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@incollection{Vaswani17attention,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{Sajjad17neural,
  title={Neural Machine Translation Training in a Multi-Domain Scenario},
  author={Hassan Sajjad and Nadir Durrani and Fahim Dalvi and Yonatan Belinkov and Stephan Vogel},
  booktitle={Proceedings of the 14th International Workshop on Spoken Language Translation},
  series = {IWSLT 2017},
  address = {Tokyo, Japan},
  year={2017},
  url ={http://arxiv.org/abs/1708.08712}
}

@inproceedings{Costa17byte,
    title = "Byte-based Neural Machine Translation",
    author = "Costa-juss{\`a}, Marta R.  and
      Escolano, Carlos  and
      Fonollosa, Jos{\'e} A. R.",
    booktitle = "Proceedings of the First Workshop on Subword and Character Level Models in {NLP}",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4123",
    doi = "10.18653/v1/W17-4123",
    pages = "154--158",
    abstract = "This paper presents experiments comparing character-based and byte-based neural machine translation systems. The main motivation of the byte-based neural machine translation system is to build multi-lingual neural machine translation systems that can share the same vocabulary. We compare the performance of both systems in several language pairs and we see that the performance in test is similar for most language pairs while the training time is slightly reduced in the case of byte-based neural machine translation.",
}

@InProceedings{Kobus17domain,
  author = 	"Kobus, Catherine
		and Crego, Josep
		and Senellart, Jean",
  title = 	"Domain Control for Neural Machine Translation",
  booktitle = 	"Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017",
  year = 	"2017",
  publisher = 	"INCOMA Ltd.",
  pages = 	"372--378",
  location = 	"Varna, Bulgaria",
  doi = 	"10.26615/978-954-452-049-6_049",
  url = 	"https://doi.org/10.26615/978-954-452-049-6_049"
}

@incollection{Mccann17learn,
title = {Learned in Translation: Contextualized Word Vectors},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6294--6305},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf}
}

@article{Lee17fully,
    title = "Fully Character-Level Neural Machine Translation without Explicit Segmentation",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Hofmann, Thomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1026",
    doi = "10.1162/tacl_a_00067",
    pages = "365--378",
    abstract = "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT{'}15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.",
}

@article{Imamura17Multi,
author = {Imamura, Kenji and Sumita, Eiichiro},
year = {2017},
month = {09},
pages = {597-618},
title = {Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation},
volume = {24},
journal = {Journal of Natural Language Processing},
doi = {10.5715/jnlp.24.597}
}

@inproceedings{koehn17six,
    title = "Six Challenges for Neural Machine Translation",
    author = "Koehn, Philipp  and
      Knowles, Rebecca",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3204",
    doi = "10.18653/v1/W17-3204",
    pages = "28--39",
    abstract = "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",
}

@InProceedings{Chen17cost,
  author = 	"Chen, Boxing
		and Cherry, Colin
		and Foster, George
		and Larkin, Samuel",
  title = 	"Cost Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the First Workshop on Neural Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"40--46",
  location = 	"Vancouver",
  doi = 	"10.18653/v1/W17-3205",
  url = 	"http://aclweb.org/anthology/W17-3205"
}

@InProceedings{Miceli17regularization,
  author = 	"Miceli Barone, Antonio Valerio
		and Haddow, Barry
		and Germann, Ulrich
		and Sennrich, Rico",
  title = 	"Regularization techniques for fine-tuning in neural machine translation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1489--1494",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1156",
  url = 	"http://aclweb.org/anthology/D17-1156"
}

@InProceedings{Britz17effective,
  author = 	"Britz, Denny
		and Le, Quoc
		and Pryzant, Reid",
  title = 	"Effective Domain Mixing for Neural Machine Translation",
  booktitle = 	"Proceedings of the Second Conference on Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--126",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/W17-4712",
  url = 	"http://aclweb.org/anthology/W17-4712"
}

@InProceedings{Chu17empirical,
  author = 	"Chu, Chenhui
		and Dabre, Raj
		and Kurohashi, Sadao",
  title = 	"An Empirical Comparison of Domain Adaptation Methods for Neural Machine      Translation    ",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 2: Short Papers)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"385--391",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-2061",
  url = 	"http://aclweb.org/anthology/P17-2061"
}

@InProceedings{Wang17instance,
  author = 	"Wang, Rui
		and Utiyama, Masao
		and Liu, Lemao
		and Chen, Kehai
		and Sumita, Eiichiro",
  title = 	"Instance Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1482--1488",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1155",
  url = 	"http://aclweb.org/anthology/D17-1155"
}
@inproceedings{Matteo17neural,
  author    = {Matteo Negri and
               Marco Turchi and
               Marcello Federico and
               Nicola Bertoldi and
               M. Amin Farajian},
  title     = {Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the
               Association for Computational Linguistics, {EACL} 2017, Valencia,
               Spain, April 3-7, 2017, Volume 2: Short Papers},
  pages     = {280--284},
  year      = {2017},
  crossref  = {DBLP:conf/eacl/2017-2},
  url       = {https://www.aclweb.org/anthology/E17-2045/},
  timestamp = {Tue, 17 Sep 2019 13:40:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/eacl/NegriTFBF17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Zhang17context,
 author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
 title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {December 2017},
 volume = {25},
 number = {12},
 month = dec,
 year = {2017},
 issn = {2329-9290},
 pages = {2424--2432},
 numpages = {9},
 url = {https://doi.org/10.1109/TASLP.2017.2751420},
 doi = {10.1109/TASLP.2017.2751420},
 acmid = {3180106},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@article{Hassan17multi,
  author    = {Hassan Sajjad and
               Nadir Durrani and
               Fahim Dalvi and
               Yonatan Belinkov and
               Stephan Vogel},
  title     = {Neural Machine Translation Training in a Multi-Domain Scenario},
  journal   = {CoRR},
  volume    = {abs/1708.08712},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.08712},
  archivePrefix = {arXiv},
  eprint    = {1708.08712},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-08712},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Klein17opennmt,
  author = 	"Klein, Guillaume
		and Kim, Yoon
		and Deng, Yuntian
		and Senellart, Jean
		and Rush, Alexander",
  title = 	"OpenNMT: Open-Source Toolkit for Neural Machine Translation",
  booktitle = 	"Proceedings of ACL 2017, System Demonstrations",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"67--72",
  location = 	"Vancouver, Canada",
  url = 	"http://aclweb.org/anthology/P17-4012"
}

@incollection{Rebuffi17learning,
 title = {Learning multiple visual domains with residual adapters},
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 booktitle = {Advances in Neural Information Processing Systems 30},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {506--516},
 year = {2017},
 publisher = {Curran Associates, Inc.},
 url = {http://papers.nips.cc/paper/6654-learning-multiple-visual-domains-with-  residual-adapters.pdf}
}

@InProceedings{Peng17multitask,
  author = 	"Peng, Nanyun
		and Dredze, Mark",
  title = 	"Multi-task Domain Adaptation for Sequence Tagging",
  booktitle = 	"Proceedings of the 2nd Workshop on Representation Learning for NLP",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"91--100",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/W17-2612",
  url = 	"http://aclweb.org/anthology/W17-2612"
}

@InProceedings{Chelsea17modelagnostic,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author = 	 {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}

@InProceedings{Ghering17convolutional,
  title = 	 {Convolutional Sequence to Sequence Learning},
  author = 	 {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1243--1252},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  url = 	 {http://proceedings.mlr.press/v70/gehring17a.html},
  abstract = 	 {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT’14 English-German and WMT’14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.}
}

@article{Jiatao17non,
  author    = {Jiatao Gu and
               James Bradbury and
               Caiming Xiong and
               Victor O. K. Li and
               Richard Socher},
  title     = {Non-Autoregressive Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1711.02281},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02281},
  archivePrefix = {arXiv},
  eprint    = {1711.02281},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-02281.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Johnson17google,
	Author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernand a and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	Issn = {2307-387X},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {339--351},
	Title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	Url = {https://transacl.org/ojs/index.php/tacl/article/view/1081},
	Volume = {5},
	Year = {2017},
	Bdsk-Url-1 = {https://transacl.org/ojs/index.php/tacl/article/view/1081}}

@inproceedings{tiedemann17neural,
    title = "Neural Machine Translation with Extended Context",
    author = {Tiedemann, J{\"o}rg  and
      Scherrer, Yves},
    booktitle = "Proceedings of the Third Workshop on Discourse in Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4811",
    doi = "10.18653/v1/W17-4811",
    pages = "82--92",
    abstract = "We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.",
}

@inproceedings{Farajian17neural,
    title = "Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario",
    author = "Farajian, M. Amin  and Turchi, Marco and Negri, Matteo and Bertoldi, Nicola and Federico, Marcello",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2045",
    pages = "280--284",
    abstract = "State-of-the-art neural machine translation (NMT) systems are generally trained on specific domains by carefully selecting the training sets and applying proper domain adaptation techniques. In this paper we consider the real world scenario in which the target domain is not predefined, hence the system should be able to translate text from multiple domains. We compare the performance of a generic NMT system and phrase-based statistical machine translation (PBMT) system by training them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings?",
}

@inproceedings{Farajian17multidomain,
	Address = {Copenhagen, Denmark},
	Author = {Farajian, M. Amin and Turchi, Marco and Negri, Matteo and Federico, Marcello},
	Booktitle = {Proceedings of the Second Conference on Machine Translation},
	Doi = {10.18653/v1/W17-4713},
	Month = sep,
	Pages = {127--137},
	Title = {Multi-Domain Neural Machine Translation through Unsupervised Adaptation},
	Url = {https://www.aclweb.org/anthology/W17-4713},
	Year = {2017},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W17-4713},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/W17-4713}}

@inproceedings{Wang17sentence,
	Address = {Vancouver, Canada},
	Author = {Wang, Rui and Finch, Andrew and Utiyama, Masao and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	Date-Added = {2019-04-28 19:22:53 +0200},
	Date-Modified = {2019-04-28 19:22:53 +0200},
	Doi = {10.18653/v1/P17-2089},
	Pages = {560--566},
	Publisher = {Association for Computational Linguistics},
	Title = {Sentence Embedding for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/P17-2089},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/P17-2089},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P17-2089}}

@article{Biao17acontextaware,
 author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
 title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {December 2017},
 volume = {25},
 number = {12},
 month = dec,
 year = {2017},
 issn = {2329-9290},
 pages = {2424--2432},
 numpages = {9},
 url = {https://doi.org/10.1109/TASLP.2017.2751420},
 doi = {10.1109/TASLP.2017.2751420},
 acmid = {3180106},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA}
}

@inproceedings{Wang17instance,
	Author = {Wang, Rui and Utiyama, Masao and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2017-09-21 21:28:02 +0000},
	Date-Modified = {2017-09-21 21:28:10 +0000},
	Location = {Copenhagen, Denmark},
	Pages = {1483--1489},
	Publisher = {Association for Computational Linguistics},
	Title = {Instance Weighting for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/D17-1155},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D17-1155}}

@inproceedings{Domhan17using,
    title = "Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning",
    author = "Domhan, Tobias  and
      Hieber, Felix",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1158",
    doi = "10.18653/v1/D17-1158",
    pages = "1500--1505",
    abstract = "The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.",
}

@inproceedings{schwenk17learning,
    title = "Learning Joint Multilingual Sentence Representations with Neural Machine Translation",
    author = "Schwenk, Holger  and
      Douze, Matthijs",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-2619",
    doi = "10.18653/v1/W17-2619",
    pages = "157--167",
    abstract = "In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.",
}

@InProceedings{Graves17automated, title = {Automated Curriculum Learning for Neural Networks}, author = {Alex Graves and Marc G. Bellemare and Jacob Menick and R{\'e}mi Munos and Koray Kavukcuoglu}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1311--1320}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/graves17a/graves17a.pdf}, url = { http://proceedings.mlr.press/v70/graves17a.html }, abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.} }
@inproceedings{Wees17dynamic,
    title = "Dynamic Data Selection for Neural Machine Translation",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1147",
    doi = "10.18653/v1/D17-1147",
    pages = "1400--1410",
    abstract = "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce {`}dynamic data selection{'} for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call {`}gradual fine-tuning{'}, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.",
}

@inproceedings{Wees17whats,
  title={What’s in a domain?: Towards fine-grained adaptation for machine translation},
  author={M. V. D. Wees},
  year={2017}
}

@article{Lee17fully,
    title = "Fully Character-Level Neural Machine Translation without Explicit Segmentation",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Hofmann, Thomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1026",
    doi = "10.1162/tacl_a_00067",
    pages = "365--378",
    abstract = "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT{'}15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.",
}

@InProceedings{Britz2017mixing,
  author = 	"Britz, Denny
		and Le, Quoc
		and Pryzant, Reid",
  title = 	"Effective Domain Mixing for Neural Machine Translation",
  booktitle = 	"Proceedings of the Second Conference on Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--126",
  address = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/W17-4712",
  url = 	"http://aclweb.org/anthology/W17-4712"
}

@misc{Khresmoi17test,
 title = {Khresmoi Summary Translation Test Data 2.0},
 author = {Du{\v s}ek, Ond{\v r}ej and Haji{\v c}, Jan and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Libovick{\'y}, Jind{\v r}ich and Pecina, Pavel and Tamchyna, Ale{\v s} and Ure{\v s}ov{\'a}, Zde{\v n}ka},
 url = {http://hdl.handle.net/11234/1-2122},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Creative Commons - Attribution-{NonCommercial} 4.0 International ({CC} {BY}-{NC} 4.0)},
 year = {2017} }

@inproceedings{Assylbekov17syllable,
    title = "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones",
    author = "Assylbekov, Zhenisbek  and
      Takhanov, Rustem  and
      Myrzakhmetov, Bagdat  and
      Washington, Jonathan N.",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1199",
    doi = "10.18653/v1/D17-1199",
    pages = "1866--1872",
    abstract = "Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18{\%}-33{\%} fewer parameters and is trained 1.2-2.2 times faster.",
}

@article{Ataman17linguistically,
  title={Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English},
  author={Duygu Ataman and Matteo Negri and M. Turchi and Marcello Federico},
  journal={The Prague Bulletin of Mathematical Linguistics},
  year={2017},
  volume={108},
  pages={331 - 342}
}

@inproceedings{Huck17target,
    title = "Target-side Word Segmentation Strategies for Neural Machine Translation",
    author = "Huck, Matthias  and
      Riess, Simon  and
      Fraser, Alexander",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4706",
    doi = "10.18653/v1/W17-4706",
    pages = "56--67",
}

@InProceedings{Chu17comparison,
  author = 	"Chu, Chenhui
		and Dabre, Raj
		and Kurohashi, Sadao",
  title = 	"An Empirical Comparison of Domain Adaptation Methods for Neural Machine      Translation    ",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 2: Short Papers)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"385--391",
  address = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-2061",
  url = 	"http://aclweb.org/anthology/P17-2061"
}

@inproceedings{Currey17copied,
    title = "Copied Monolingual Data Improves Low-Resource Neural Machine Translation",
    author = "Currey, Anna  and
      Miceli Barone, Antonio Valerio  and
      Heafield, Kenneth",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4715",
    doi = "10.18653/v1/W17-4715",
    pages = "148--156",
}

@InProceedings{Khayrallah17neural,
  author    = {Khayrallah, Huda and Kumar, Gaurav and Duh, Kevin and Post, Matt and Koehn, Philipp},
  title     = {Neural Lattice Search for Domain Adaptation in Machine Translation},
  booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  year      = {2017},
  publisher = {Asian Federation of Natural Language Processing},
  month     = {November},
  pages     = {20--25},
  url       = {http://www.aclweb.org/anthology/I17-2004},
}

@inproceedings{Chelsea17model,
  author    = {Chelsea Finn and
               Pieter Abbeel and
               Sergey Levine},
  editor    = {Doina Precup and
               Yee Whye Teh},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {1126--1135},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/finn17a.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/FinnAL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Machcek18morphological,
  title={Morphological and Language-Agnostic Word Segmentation for NMT},
  author={Dominik Mach{\'a}cek and Jon{\'a}s Vidra and Ondrej Bojar},
  booktitle={TSD},
  year={2018}
}

@inproceedings{wuebker18compact,
    title = "Compact Personalized Models for Neural Machine Translation",
    author = "Wuebker, Joern  and
      Simianer, Patrick  and
      DeNero, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1104",
    doi = "10.18653/v1/D18-1104",
    pages = "881--886",
    abstract = "We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture{--}combining a state-of-the-art self-attentive model with compact domain adaptation{--}provides high quality personalized machine translation that is both space and time efficient.",
}

@inproceedings{Ott18scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
    abstract = "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT{'}14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT{'}14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
}

@inproceedings{Burlot18using,
    title = "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
    author = "Burlot, Franck  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6315",
    doi = "10.18653/v1/W18-6315",
    pages = "144--155",
    abstract = "Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation - a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.",
}

@inproceedings{Rebuffi18efficient,
  author    = {Sylvestre{-}Alvise Rebuffi and
               Hakan Bilen and
               Andrea Vedaldi},
  title     = {Efficient Parametrization of Multi-Domain Deep Neural Networks},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {8119--8127},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Rebuffi\_Efficient\_Parametrization\_of\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00847},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/RebuffiBV18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Rosenfeld18Priming,
  title={Priming Neural Networks},
  author={Amir Rosenfeld and Mahdi Biparva and John K. Tsotsos},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2018},
  pages={2092-209209}
}

@inproceedings{Post18call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}

@inproceedings{bawden18evaluating,
    title = "Evaluating Discourse Phenomena in Neural Machine Translation",
    author = "Bawden, Rachel  and
      Sennrich, Rico  and
      Birch, Alexandra  and
      Haddow, Barry",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1118",
    doi = "10.18653/v1/N18-1118",
    pages = "1304--1313",
    abstract = "For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models{'} ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50{\%} accuracy on our coreference test set and 53.5{\%} for coherence/cohesion (compared to a non-contextual baseline of 50{\%}). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5{\%} for coreference and 57{\%} for coherence/cohesion), highlighting the importance of target-side context.",
}

@inproceedings{Niu18multitask,
  author    = {Xing Niu and Sudha Rao and Marine Carpuat},
  title     = {Multi-Task Neural Models for Translating Between Styles Within and Across Languages},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  pages     = {1008--1021},
  series    = {COLING},
  address   = {Santa Fe, New Mexico, USA},
  year      = {2018},
  editor    = {Emily M. Bender and Leon Derczynski and Pierre Isabelle},
  url       = {https://aclanthology.info/papers/C18-1086/c18-1086},
}

@inproceedings{Zeng18multidomain,
	Address = {Brussels, Belgium},
	Author = {Zeng, Jiali and Su, Jinsong and Wen, Huating and Liu, Yang and Xie, Jun and Yin, Yongjing and Zhao, Jianqiang},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {447--457},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination},
	Url = {http://aclweb.org/anthology/D18-1041},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1041}
} 

@inproceedings{Neubig18rapid,
    title = "Rapid Adaptation of Neural Machine Translation to New Languages",
    author = "Neubig, Graham  and
      Hu, Junjie",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1103",
    doi = "10.18653/v1/D18-1103",
    pages = "875--880",
    abstract = "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual {``}seed models{''}, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of {``}similar-language regularization{''}, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",
}

@inproceedings{Poncelas18Feature,
  title={Feature decay algorithms for neural machine translation},
  author={Alberto Poncelas and G. M. D. B. Wenniger and A. Way},
  booktitle = {Proceedings of the 21st Annual Conference of the European Association for Machine Translation},
  pages = {239-248},
  editor= {European Association for Machine Translation},
  url = {http://rua.ua.es/dspace/handle/10045/76084},
  
  year={2018}
}

@inproceedings{Taku18subword,
  author    = {Taku Kudo},
  editor    = {Iryna Gurevych and
               Yusuke Miyao},
  title     = {Subword Regularization: Improving Neural Network Translation Models
               with Multiple Subword Candidates},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20, 2018, Volume
               1: Long Papers},
  pages     = {66--75},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://www.aclweb.org/anthology/P18-1007/},
  doi       = {10.18653/v1/P18-1007},
  timestamp = {Mon, 16 Sep 2019 13:46:41 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/Kudo18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Michel18extreme,
  author = 	"Michel, Paul
		and Neubig, Graham",
  title = 	"Extreme Adaptation for Personalized Neural Machine Translation",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"312--318",
  address = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-2050"
}

@inproceedings{Cherry18revisiting,
    title = "Revisiting Character-Based Neural Machine Translation with Capacity and Compression",
    author = "Cherry, Colin  and
      Foster, George  and
      Bapna, Ankur  and
      Firat, Orhan  and
      Macherey, Wolfgang",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1461",
    doi = "10.18653/v1/D18-1461",
    pages = "4295--4305",
    abstract = "Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.",
}

@inproceedings{Koehn18findings,
    title = "Findings of the {WMT} 2018 Shared Task on Parallel Corpus Filtering",
    author = "Koehn, Philipp  and
      Khayrallah, Huda  and
      Heafield, Kenneth  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6453",
    doi = "10.18653/v1/W18-6453",
    pages = "726--739",
    abstract = "We posed the shared task of assigning sentence-level quality scores for a very noisy corpus of sentence pairs crawled from the web, with the goal of sub-selecting 1{\%} and 10{\%} of high-quality data to be used to train machine translation systems. Seventeen participants from companies, national research labs, and universities participated in this task.",
}
@InProceedings{Tars18multidomain,
  author    = {Sander Tars and Mark Fishel},
  title     = {Multi-Domain Neural Machine Translation},
  booktitle = {Proceedings of the 21st Annual Conference of the European Association for Machine Translation},
  year = 	 2018,
  editor = 	 {Juan Antonio Pérez-Ortiz and Felipe Sánchez-Martínez and Miquel Esplà-Gomis and Maja Popović and Celia Rico and André Martins and Joachim Van den Bogaert and Mikel L. Forcada},
  series = 	 {EAMT},
  pages = 	 {259--269},
  address = 	 {Alicante, Spain},
  Url = {https://arxiv.org/pdf/1805.02282.pdf},
  organization = {EAMT}}
@inproceedings{Pham18fixing,
    title = "Fixing Translation Divergences in Parallel Corpora for Neural {MT}",
    author = "Pham, MinhQuang  and
      Crego, Josep  and
      Senellart, Jean  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1328",
    doi = "10.18653/v1/D18-1328",
    pages = "2967--2973",
    abstract = "Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered/corrected corpora actually improves MT performance.",
}

@InProceedings{Chu18asurvey,
  author = 	"Chu, Chenhui and Wang, Rui",
  title = 	"A Survey of Domain Adaptation for Neural Machine Translation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  series = "COLING 2018",
  year = 	"2018",
  pages = 	"1304--1319",
  address = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1111"
}

@article{Alex18first,
  author    = {Alex Nichol and
               Joshua Achiam and
               John Schulman},
  title     = {On First-Order Meta-Learning Algorithms},
  journal   = {CoRR},
  volume    = {abs/1803.02999},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.02999},
  archivePrefix = {arXiv},
  eprint    = {1803.02999},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-02999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Peters18deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{raganato18analysis,
    title = "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro  and
      Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5431",
    doi = "10.18653/v1/W18-5431",
    pages = "287--297",
    abstract = "The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the \textit{Transformer} is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.",
}

@incollection{Hoffman18algorithms,
title = {Algorithms and Theory for Multiple-Source Adaptation},
author = {Hoffman, Judy and Mohri, Mehryar and Zhang, Ningshan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8246--8256},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation.pdf}
}
@inproceedings{Chu18survey,
    title = "A Survey of Domain Adaptation for Neural Machine Translation",
    author = "Chu, Chenhui  and
      Wang, Rui",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1111",
    pages = "1304--1319",
    abstract = "Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domain-specific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT.",
}
@inproceedings{Pei18multi,
  author    = {Zhongyi Pei and
               Zhangjie Cao and
               Mingsheng Long and
               Jianmin Wang},
  title     = {Multi-Adversarial Domain Adaptation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3934--3941},
  year      = {2018},
  crossref  = {DBLP:conf/aaai/2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17067},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/PeiCLW18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Silva18extracting,
    title = "Extracting In-domain Training Corpora for Neural Machine Translation Using Data Selection Methods",
    author = "Silva, Catarina Cruz  and
      Liu, Chao-Hong  and
      Poncelas, Alberto  and
      Way, Andy",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6323",
    doi = "10.18653/v1/W18-6323",
    pages = "224--231",
}

@InProceedings{Zheng18multi,
  author = 	"Zeng, Jiali
		and Su, Jinsong
		and Wen, Huating
		and Liu, Yang
		and Xie, Jun
		and Yin, Yongjing
		and Zhao, Jianqiang",
  title = 	"Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"447--457",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1041"
}

@InProceedings{Zhang18sentence,
  author = 	"Zhang, Shiqi
		and Xiong, Deyi",
  title = 	"Sentence Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"3181--3190",
  location = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1269"
}

@InProceedings{Thompson18freezing,
  author = 	"Thompson, Brian
		and Khayrallah, Huda
		and Anastasopoulos, Antonios
		and McCarthy, Arya D.
		and Duh, Kevin
		and Marvin, Rebecca
		and McNamee, Paul
		and Gwinnup, Jeremy
		and Anderson, Tim
		and Koehn, Philipp",
  title = 	"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation",
  booktitle = 	"Proceedings of the Third Conference on Machine Translation: Research Papers",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"124--132",
  location = 	"Belgium, Brussels",
  url = 	"http://aclweb.org/anthology/W18-6313"
}

@InProceedings{Vilar18learning,
  author = 	"Vilar, David",
  title = 	"Learning Hidden Unit Contribution for Adapting Neural Machine Translation      Models    ",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies,      Volume 2 (Short Papers)    ",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"500--505",
  location = 	"New Orleans, Louisiana",
  doi = 	"10.18653/v1/N18-2080",
  url = 	"http://aclweb.org/anthology/N18-2080"
}

@InProceedings{Chu18survey,
  author = 	"Chu, Chenhui
		and Wang, Rui",
  title = 	"A Survey of Domain Adaptation for Neural Machine Translation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1304--1319",
  location = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1111"
}
@inproceedings{Li18onesentence,
    title = "One Sentence One Model for Neural Machine Translation",
    author = "Li, Xiaoqing  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1146",
}
@inproceedings{Zhongyi18multi,
  author    = {Zhongyi Pei and
               Zhangjie Cao and
               Mingsheng Long and
               Jianmin Wang},
  title     = {Multi-Adversarial Domain Adaptation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3934--3941},
  year      = {2018},  
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17067},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/PeiCLW18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Alvarez18gromov,
  author = 	"Alvarez-Melis, David
		and Jaakkola, Tommi",
  title = 	"Gromov-Wasserstein Alignment of Word Embedding Spaces",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1881--1890",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1214"
}

@inproceedings{Niu18multitask,
  author    = {Xing Niu and Sudha Rao and Marine Carpuat},
  title     = {Multi-Task Neural Models for Translating Between Styles Within and Across Languages},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  pages     = {1008--1021},
  series    = {COLING},
  address   = {Santa Fe, New Mexico, USA},
  year      = {2018},
  editor    = {Emily M. Bender and Leon Derczynski and Pierre Isabelle},
  crossref  = {DBLP:conf/coling/2018},
  url       = {https://aclanthology.info/papers/C18-1086/c18-1086},
}

@inproceedings{pagliardini18unsupervised,
    title = "Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features",
    author = "Pagliardini, Matteo  and
      Gupta, Prakhar  and
      Jaggi, Martin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1049",
    doi = "10.18653/v1/N18-1049",
    pages = "528--540",
    abstract = "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.",
}

@inproceedings{Post18A,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}

@InProceedings{Chu18multilingual,
  author = 	 {Chenhui Chu and Raj Dabre},
  title = 	 {Multilingual and multi-domain adaptation for neural machine translation},
  booktitle = {Proceedings of the 24st Annual Meeting of the Association for Natural Language Processing (NLP 2018)},
  year = 	 2018,
  pages = 	 {909-–912},
  address = 	 {Okayama, Japan}}

@inproceedings{Lample18unsupervised,
title={Unsupervised Machine Translation Using Monolingual Corpora Only},
author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
booktitle={International Conference on Learning Representations},
year={2018},
address = {Vancouver, Canada},
url={https://openreview.net/forum?id=rkYTTf-AZ},
}

@inproceedings{Artetxe18unsupervised,
title={Unsupervised Neural Machine Translation},
author={Mikel Artetxe and Gorka Labaka and Eneko Agirre and Kyunghyun Cho},
booktitle={International Conference on Learning Representations},
year={2018},
address = {Vancouver, Canada},
url={https://openreview.net/forum?id=Sy2ogebAW},
}

@inproceedings{Platanios18contextual,
	Address = {Brussels, Belgium},
	Author = {Platanios, Emmanouil Antonios and Sachan, Mrinmaya and Neubig, Graham and Mitchell, Tom},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {425--435},
	Publisher = {Association for Computational Linguistics},
	Title = {Contextual Parameter Generation for Universal Neural Machine Translation},
	Url = {http://aclweb.org/anthology/D18-1039},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1039}}

@inproceedings{Khayrallah2018regularized,
    title = "Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation",
    author = "Khayrallah, Huda  and
      Thompson, Brian  and
      Duh, Kevin  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2705",
    doi = "10.18653/v1/W18-2705",
    pages = "36--44",
    abstract = "Supervised domain adaptation{---}where a large generic corpus and a smaller in-domain corpus are both available for training{---}is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model{'}s output word distribution and that of the out-of-domain model to prevent the model{'}s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU.",
}

@inproceedings{Devlin19bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}

@incollection{Conneau19crosslingual,
title = {Cross-lingual Language Model Pretraining},
author = {Conneau, Alexis and Lample, Guillaume},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. {d'Alch\'{e}-Buc} and E. Fox and R. Garnett},
pages = {7059--7069},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf}
}

@article{schwenk2019ccmatrix,
  title={CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB},
  author={Schwenk, Holger and Wenzek, Guillaume and Edunov, Sergey and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1911.04944},
  url="https://arxiv.org/pdf/1911.04944.pdf",
  year={2019}
}

@InProceedings{houlsby19parameter,
  title =        {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  pages =        {2790--2799},
  year =         {2019},
  editor =       {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume =       {97},
  series =       {Proceedings of Machine Learning Research},
  address =      {Long Beach, California, USA},
  month =        {09--15 Jun},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url =          {http://proceedings.mlr.press/v97/houlsby19a.html},
  abstract =     {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8%$ of the performance of full fine-tuning, adding only $3.6%$ parameters per task. By contrast, fine-tuning trains $100%$ of the parameters per task.}
}

@inproceedings{Dou19unsupervised,
    title = "Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings",
    author = "Dou, Zi-Yi  and
      Hu, Junjie  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1147",
    doi = "10.18653/v1/D19-1147",
    pages = "1417--1422",
    abstract = "The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.",
}

@article{Arivazhagan19massively,
  author    = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Dmitry Lepikhin and
               Melvin Johnson and Maxim Krikun and Mia Xu Chen and Yuan Cao and
               George Foster and Colin Cherry and Wolfgang Macherey and Zhifeng Chen and Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
  journal   = {arXiv e-prints},
  volume    = {abs/1907.05019},
  year      = {2019},
  primaryClass = {cs.CL},
  url       = {http://arxiv.org/abs/1907.05019}
}

@ARTICLE{Yan19wordbased,
       author = {{Yan}, Shen and {Dahlmann}, Leonard and {Petrushkov}, Pavel and
         {Hewavitharana}, Sanjika and {Khadivi}, Shahram},
        title = "{Word-based Domain Adaptation for Neural Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = "2019",
        month = "Jun",
          eid = {arXiv:1906.03129},
        pages = {arXiv:1906.03129},
archivePrefix = {arXiv},
       eprint = {1906.03129},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190603129Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Jiang19multidomain,
    title = "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing",
    author = "Jiang, Haoming  and
      Liang, Chen  and
      Wang, Chong  and
      Zhao, Tuo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.165",
    doi = "10.18653/v1/2020.acl-main.165",
    pages = "1823--1834",
    abstract = "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
}

@inproceedings{Platanios19competence,
    title = "Competence-based Curriculum Learning for Neural Machine Translation",
    author = "Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1119",
    doi = "10.18653/v1/N19-1119",
    pages = "1162--1172",
    abstract = "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70{\%} decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",
}

@article{So19theevolved,
  author    = {David R. So and
               Chen Liang and
               Quoc V. Le},
  title     = {The Evolved Transformer},
  journal   = {CoRR},
  volume    = {abs/1901.11117},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.11117},
  archivePrefix = {arXiv},
  eprint    = {1901.11117},
  timestamp = {Mon, 04 Feb 2019 08:11:03 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-11117},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Junjie19domain,
    title = {Domain Adaptation of Neural Machine Translation by Lexicon Induction},
    author = {Junjie Hu and Mengzhou Xia and Graham Neubig and Jaime Carbonell},
    booktitle = {The 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
    address = {Florence, Italy},
    month = {July},
    url = {https://arxiv.org/abs/1906.00376},
    year = {2019}
}

@ARTICLE{Li19semisupervised,
       author = {{Li}, Yitong and {Baldwin}, Timothy and {Cohn}, Trevor},
        title = "{Semi-supervised Stochastic Multi-Domain Learning using Variational Inference}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = "2019",
        month = "Jun",
          eid = {arXiv:1906.02897},
        pages = {arXiv:1906.02897},
archivePrefix = {arXiv},
       eprint = {1906.02897},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190602897L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Wang19dynamically,
    title = "Dynamically Composing Domain-Data Selection with Clean-Data Selection by {``}Co-Curricular Learning{''} for Neural Machine Translation",
    author = "Wang, Wei  and
      Caswell, Isaac  and
      Chelba, Ciprian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1123",
    doi = "10.18653/v1/P19-1123",
    pages = "1282--1292",
    abstract = "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a {``}co-curricular learning{''} method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the {``}co-curriculum{''}. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",
}

@misc{sabet2019robust,
    title={Robust Cross-lingual Embeddings from Parallel Sentences},
    author={Ali Sabet and Prakhar Gupta and Jean-Baptiste Cordonnier and Robert West and Martin Jaggi},
    year={2019},
    eprint={1912.12481},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{Neubig19compare-mt,
  author    = {Graham Neubig and Zi{-}Yi Dou and Junjie Hu and Paul Michel and Danish Pruthi and Xinyi Wang and John Wieting},
  title     = {compare-mt: {A} Tool for Holistic Comparison of Language Generation Systems},
  journal   = {CoRR},
  volume    = {abs/1903.07926},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.07926},
}
@inproceedings{Saunders19ucam,
    title = "{UCAM} Biomedical Translation at {WMT}19: Transfer Learning Multi-domain Ensembles",
    author = "Saunders, Danielle and Stahlberg, Felix and Byrne, Bill",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-5421",
    pages = "169--174",
    abstract = "The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish.",
}

@inproceedings{bapna19non,
    title = "Non-Parametric Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1191",
    doi = "10.18653/v1/N19-1191",
    pages = "1921--1931",
    abstract = "Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our Semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The Semi-parametric nature of our approach also opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.",
}

@inproceedings{peters18deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
}

@inproceedings{Miller19simplified,
    title = "Simplified Neural Unsupervised Domain Adaptation",
    author = "Miller, Timothy",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1039",
    doi = "10.18653/v1/N19-1039",
    pages = "414--419",
    abstract = "Unsupervised domain adaptation (UDA) is the task of training a statistical model on labeled data from a source domain to achieve better performance on data from a target domain, with access to only unlabeled data in the target domain. Existing state-of-the-art UDA approaches use neural networks to learn representations that are trained to predict the values of subset of important features called {``}pivot features{''} on combined data from the source and target domains. In this work, we show that it is possible to improve on existing neural domain adaptation algorithms by 1) jointly training the representation learner with the task learner; and 2) removing the need for heuristically-selected {``}pivot features.{''} Our results show competitive performance with a simpler model.",
}

@article{Su19exploring,
author={Jinsong Su and Jiali Zeng and Jun Xie and Huating Wen and Yongjing Yin and Yang Liu},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)},
title={Exploring Discriminative Word-Level Domain Contexts for Multi-domain Neural Machine Translation},
year={2019},
volume={},
number={},
pages={1-1},
keywords={Multi-domain Neural Machine Translation;Word-Level Context;Adversarial Training},
doi={10.1109/TPAMI.2019.2954406},
ISSN={1939-3539},
month={},}

@inproceedings{Liu19reinforced,
    title = "Reinforced Training Data Selection for Domain Adaptation",
    author = "Liu, Miaofeng  and
      Song, Yan  and
      Zou, Hongbin  and
      Zhang, Tong",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1189",
    doi = "10.18653/v1/P19-1189",
    pages = "1957--1968",
    abstract = "Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make TDS self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks.",
}

@inproceedings{Oren19distributionally,
    title = "Distributionally Robust Language Modeling",
    author = "Oren, Yonatan  and
      Sagawa, Shiori  and
      Hashimoto, Tatsunori B.  and
      Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1432",
    doi = "10.18653/v1/D19-1432",
    pages = "4227--4237",
    abstract = "Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",
}

@inproceedings{santy19inmt,
    title = "{INMT}: Interactive Neural Machine Translation Prediction",
    author = "Santy, Sebastin  and
      Dandapat, Sandipan  and
      Choudhury, Monojit  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-3018",
    doi = "10.18653/v1/D19-3018",
    pages = "103--108",
    abstract = "In this paper, we demonstrate an Interactive Machine Translation interface, that assists human translators with on-the-fly hints and suggestions. This makes the end-to-end translation process faster, more efficient and creates high-quality translations. We augment the OpenNMT backend with a mechanism to accept the user input and generate conditioned translations.",
}

@inproceedings{Saunders19domain,
    title = "Domain Adaptive Inference for Neural Machine Translation",
    author = "Saunders, Danielle  and
      Stahlberg, Felix  and
      de Gispert, Adri{\`a}  and
      Byrne, Bill",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1022",
    doi = "10.18653/v1/P19-1022",
    pages = "222--228",
    abstract = "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",
}

@inproceedings{bulte19neural,
    title = "Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation",
    author = "Bult\'{e}, Bram  and Tezcan, Arda",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1175",
    doi = "10.18653/v1/P19-1175",
    pages = "1800--1809"
}

@ARTICLE{Dou19Unsupervised,
       author = {{Dou}, Zi-Yi and {Hu}, Junjie and {Anastasopoulos}, Antonios and
         {Neubig}, Graham},
        title = "{Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2019",
        month = "Aug",
          eid = {arXiv:1908.10430},
        pages = {arXiv:1908.10430},
archivePrefix = {arXiv},
       eprint = {1908.10430},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190810430D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Zhou19synchronous,
    title = "Synchronous Bidirectional Neural Machine Translation",
    author = "Zhou, Long  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1006",
    doi = "10.1162/tacl_a_00256",
    pages = "91--105",
    abstract = "Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional{--}neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese{--}English, WMT14 English{--}German, and WMT18 Russian{--}English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese{--}English and English{--}German translation tasks.",
}

@inproceedings{Bapna19simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}
@inproceedings{Aharoni19massively,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}
@misc{Wang19neural,
    title={Neural Machine Translation with Byte-Level Subwords},
    author={Changhan Wang and Kyunghyun Cho and Jiatao Gu},
    year={2019},
    eprint={1909.03341},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@InProceedings{Welleck19non, title = {Non-Monotonic Sequential Text Generation}, author = {Welleck, Sean and Brantley, Kiant{\'e} and Iii, Hal Daum{\'e} and Cho, Kyunghyun}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {6716--6726}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/welleck19a/welleck19a.pdf}, url = { http://proceedings.mlr.press/v97/welleck19a.html }, abstract = {Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.} }

@inproceedings{Wang19dynamically,
    title = "Dynamically Composing Domain-Data Selection with Clean-Data Selection by {``}Co-Curricular Learning{''} for Neural Machine Translation",
    author = "Wang, Wei  and
      Caswell, Isaac  and
      Chelba, Ciprian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1123",
    doi = "10.18653/v1/P19-1123",
    pages = "1282--1292",
    abstract = "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a {``}co-curricular learning{''} method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the {``}co-curriculum{''}. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",
}

@article{Naveen19massively,
  author    = {Naveen Arivazhagan and
               Ankur Bapna and
               Orhan Firat and
               Dmitry Lepikhin and
               Melvin Johnson and
               Maxim Krikun and
               Mia Xu Chen and
               Yuan Cao and
               George Foster and
               Colin Cherry and
               Wolfgang Macherey and
               Zhifeng Chen and
               Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings
               and Challenges},
  journal   = {CoRR},
  volume    = {abs/1907.05019},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05019},
  archivePrefix = {arXiv},
  eprint    = {1907.05019},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-05019},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Xuan19curriculum,
  author    = {Xuan Zhang and
               Pamela Shapiro and
               Gaurav Kumar and
               Paul McNamee and
               Marine Carpuat and
               Kevin Duh},
  title     = {Curriculum Learning for Domain Adaptation in Neural Machine Translation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {1903--1915},
  year      = {2019},
  crossref  = {DBLP:conf/naacl/2019-1},
  url       = {https://www.aclweb.org/anthology/N19-1189/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/naacl/ZhangSKMCD19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Brian19overcoming,
  author    = {Brian Thompson and
               Jeremy Gwinnup and
               Huda Khayrallah and
               Kevin Duh and
               Philipp Koehn},
  title     = {Overcoming Catastrophic Forgetting During Domain Adaptation of Neural
               Machine Translation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {2062--2068},
  year      = {2019},
  crossref  = {DBLP:conf/naacl/2019-1},
  url       = {https://www.aclweb.org/anthology/N19-1209/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/naacl/ThompsonGKDK19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Conneau19cross,
 author = {CONNEAU, Alexis and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Cross-lingual Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Saunders19domain,
    title = "Domain Adaptive Inference for Neural Machine Translation",
    author = "Saunders, Danielle  and
      Stahlberg, Felix  and
      de Gispert, Adri{\`a}  and
      Byrne, Bill",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1022",
    doi = "10.18653/v1/P19-1022",
    pages = "222--228",
    abstract = "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",
}

@article{Schwenk19wikimatrix,
  author    = {Holger Schwenk and
               Vishrav Chaudhary and
               Shuo Sun and
               Hongyu Gong and
               Francisco Guzm{\'{a}}n},
  title     = {WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs
               from Wikipedia},
  journal   = {CoRR},
  volume    = {abs/1907.05791},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05791},
  archivePrefix = {arXiv},
  eprint    = {1907.05791},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-05791.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Saunders19domain,
    title = "Domain Adaptive Inference for Neural Machine Translation",
    author = "Saunders, Danielle  and
      Stahlberg, Felix  and
      de Gispert, Adri{\`a}  and
      Byrne, Bill",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1022",
    doi = "10.18653/v1/P19-1022",
    pages = "222--228",
    abstract = "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",
}

@inproceedings{Zhang19curriculum,
    title = "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
    author = "Zhang, Xuan and Shapiro, Pamela and Kumar, Gaurav  and McNamee, Paul  and Carpuat, Marine  and Duh, Kevin",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1189",
    doi = "10.18653/v1/N19-1189",
    pages = "1903--1915",
    abstract = "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
}

@inproceedings{Pham19generic,
	Address = {Hong-Kong, CN},
	Author = {Pham, Minh Quang AND Crego, Josep-Maria AND Senellart, Jean AND Yvon, Fran\c{c}ois},
	Booktitle = {Proceedings of the 16th {International Workshop on Spoken Language Translation}},
	Series = {IWSLT},
	Keywords = {Machine Translation; Domain Adaptation},
	Pages = {9p},
	Title = {{Generic and Specialized Word Embeddings for Multi-Domain Machine Translation}},
	Url = {https://zenodo.org/record/3524959/files/IWSLT2019_paper_10.pdf},
	Year = {2019}}

@inproceedings{Kumar19reinforcement,
    title = "Reinforcement Learning based Curriculum Optimization for Neural Machine Translation",
    author = "Kumar, Gaurav and Foster, George and Cherry, Colin and Krikun, Maxim",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1208",
    doi = "10.18653/v1/N19-1208",
    pages = "2054--2061",
    abstract = "We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as noise, we seek an optimal curriculum, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes data weighting, filtering, and fine-tuning schemes. Rather than relying on prior knowledge to design a curriculum, we use reinforcement learning to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula.",
}

@article{radford19language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@ARTICLE{Gu20domain,
       author = {{Gu}, Yu and {Tinn}, Robert and {Cheng}, Hao and {Lucas}, Michael and
         {Usuyama}, Naoto and {Liu}, Xiaodong and {Naumann}, Tristan and
         {Gao}, Jianfeng and {Poon}, Hoifung},
        title = "{Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2020,
        month = jul,
          eid = {arXiv:2007.15779},
        pages = {arXiv:2007.15779},
archivePrefix = {arXiv},
       eprint = {2007.15779},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200715779G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Banon20Paracrawl,
    title = "{P}ara{C}rawl: Web-Scale Acquisition of Parallel Corpora",
    author = "Ba{\~n}{\'o}n, Marta  and
      Chen, Pinzhen  and
      Haddow, Barry  and
      Heafield, Kenneth  and
      Hoang, Hieu  and
      Espl{\`a}-Gomis, Miquel  and
      Forcada, Mikel L.  and
      Kamran, Amir  and
      Kirefu, Faheem  and
      Koehn, Philipp  and
      Ortiz Rojas, Sergio  and
      Pla Sempere, Leopoldo  and
      Ram{\'\i}rez-S{\'a}nchez, Gema  and
      Sarr{\'\i}as, Elsa  and
      Strelec, Marek  and
      Thompson, Brian  and
      Waites, William  and
      Wiggins, Dion  and
      Zaragoza, Jaume",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.417",
    doi = "10.18653/v1/2020.acl-main.417",
    pages = "4555--4567",
    abstract = "We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.",
}

@inproceedings{Oren19distributionally,
    title = "Distributionally Robust Language Modeling",
    author = "Oren, Yonatan and Sagawa, Shiori and Hashimoto, Tatsunori and Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1432",
    doi = "10.18653/v1/D19-1432",
    pages = "4227--4237",
    abstract = "Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",
}

@inproceedings{Muller20domain,
    title = "Domain Robustness in Neural Machine Translation",
    author = {M{\"u}ller, Mathias  and
      Rios, Annette  and
      Sennrich, Rico},
    booktitle = "Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = oct,
    year = "2020",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2020.amta-research.14",
    pages = "151--164",
}

@inproceedings{artetxe20cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@inproceedings{aji20neural,
    title = "In Neural Machine Translation, What Does Transfer Learning Transfer?",
    author = "Aji, Alham Fikri  and
      Bogoychev, Nikolay  and
      Heafield, Kenneth  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.688",
    pages = "7701--7710",
    abstract = "Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.",
}
@inproceedings{xu20boosting,
    title = "Boosting Neural Machine Translation with Similar Translations",
    author = "Xu, Jitao  and
      Crego, Josep  and
      Senellart, Jean",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.144",
    doi = "10.18653/v1/2020.acl-main.144",
    pages = "1580--1590",
    abstract = "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with {``}copy{''} information while translations based on embedding similarities tend to extend the translation {``}context{''}. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",
}
@inproceedings{Li20shallow,
    title = "Shallow-to-Deep Training for Neural Machine Translation",
    author = "Li, Bei  and
      Wang, Ziyang  and
      Liu, Hui  and
      Jiang, Yufan  and
      Du, Quan  and
      Xiao, Tong  and
      Wang, Huizhen  and
      Zhu, Jingbo",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.72",
    doi = "10.18653/v1/2020.emnlp-main.72",
    pages = "995--1005",
    abstract = "Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT{'}16 English-German and WMT{'}14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at \url{https://github.com/libeineu/SDT-Training}.",
}

@inproceedings{Aharoni20unsupervised,
    title = "Unsupervised Domain Clusters in Pretrained Language Models",
    author = "Aharoni, Roee  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.692",
    doi = "10.18653/v1/2020.acl-main.692",
    pages = "7747--7763",
    abstract = "The notion of {``}in-domain data{''} in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision {--} suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.",
}

@misc{feng20languageagnostic,
    title={Language-agnostic {BERT} Sentence Embedding},
    author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},
    year={2020},
    eprint={2007.01852},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{Johnson19billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  year={2019},
  url="https://arxiv.org/pdf/1702.08734.pdf",
  publisher={IEEE}
}

@inproceedings{xu19lexical,
        Title = {Lexical Micro-adaptation for Neural Machine Translation},
        Address = {Honk Kong, China},
        Author = {Xu, Jitao and Crego, Josep and Senellart, Jean},
        Booktitle = {International Workshop on Spoken Language Translation},
        url = "https://zenodo.org/record/3524977/files/IWSLT2019_paper_9.pdf?download=1",
	month = {nov},
        Year = {2019},
}

@misc{chang20pretraining,
    title={Pre-training Tasks for Embedding-based Large-scale Retrieval},
    author={Wei-Cheng Chang and Felix X. Yu and Yin-Wen Chang and Yiming Yang and Sanjiv Kumar},
    year={2020},
    eprint={2002.03932},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{xu20boosting,
    title = "Boosting Neural Machine Translation with Similar Translations",
    author = "Xu, Jitao  and
      Crego, Josep  and
      Senellart, Jean",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.144",
    doi = "10.18653/v1/2020.acl-main.144",
    pages = "1580--1590",
    abstract = "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with {``}copy{''} information while translations based on embedding similarities tend to extend the translation {``}context{''}. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",
}

@inproceedings{Brown20language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}
@inproceedings{Sharaf20metalearning,
    title = "Meta-Learning for Few-Shot {NMT} Adaptation",
    author = "Sharaf, Amr and Hassan, Hany  and Daum{\'e} III, Hal",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.ngt-1.5",
    doi = "10.18653/v1/2020.ngt-1.5",
    pages = "43--53",
    abstract = "We present META-MT, a meta-learning approach to adapt Neural Machine Translation (NMT) systems in a few-shot setting. META-MT provides a new approach to make NMT models easily adaptable to many target do- mains with the minimal amount of in-domain data. We frame the adaptation of NMT systems as a meta-learning problem, where we learn to adapt to new unseen domains based on simulated offline meta-training domain adaptation tasks. We evaluate the proposed meta-learning strategy on ten domains with general large scale NMT systems. We show that META-MT significantly outperforms classical domain adaptation when very few in- domain examples are available. Our experiments shows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU points after seeing only 4, 000 translated words (300 parallel sentences).",
}
@inproceedings{Philip20monolingual,
    title = "Monolingual Adapters for Zero-Shot Neural Machine Translation",
    author = "Philip, Jerin  and
      Berard, Alexandre  and
      Gall{\'e}, Matthias  and
      Besacier, Laurent",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.361",
    doi = "10.18653/v1/2020.emnlp-main.361",
    pages = "4465--4470",
    abstract = "We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.",
}

@inproceedings{Pham20Study,
    title = "A Study of Residual Adapters for Multi-Domain Neural Machine Translation",
    author = "Pham, Minh Quang  and
      Crego, Josep Maria  and
      Yvon, Fran{\c{c}}ois  and
      Senellart, Jean",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wmt-1.72",
    pages = "617--628",
    abstract = "Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.",
}

@inproceedings{Aharoni20Unsupervised,
    title = "Unsupervised Domain Clusters in Pretrained Language Models",
    author = "Aharoni, Roee  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.692",
    doi = "10.18653/v1/2020.acl-main.692",
    pages = "7747--7763",
    abstract = "The notion of {``}in-domain data{''} in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision {--} suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.",
}

@inproceedings{Jiang20Multi,
    title = "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing",
    author = "Jiang, Haoming  and
      Liang, Chen  and
      Wang, Chong  and
      Zhao, Tuo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.165",
    doi = "10.18653/v1/2020.acl-main.165",
    pages = "1823--1834",
    abstract = "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
}

@inproceedings{Pham20Priming,
    title = "Priming Neural Machine Translation",
    author = "Pham, Minh Quang  and
      Xu, Jitao  and
      Crego, Josep  and
      Yvon, Fran{\c{c}}ois  and
      Senellart, Jean",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wmt-1.63",
    pages = "516--527",
    abstract = "Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.",
}

@InProceedings{Wang20optimizing,
  title = 	 {Optimizing Data Usage via Differentiable Rewards},
  author =       {Wang, Xinyi and Pham, Hieu and Michel, Paul and Anastasopoulos, Antonios and Carbonell, Jaime and Neubig, Graham},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9983--9995},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wang20p/wang20p.pdf},
  url = 	 {
http://proceedings.mlr.press/v119/wang20p.html
},
  abstract = 	 {To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that “adapts” to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.}
}

@inproceedings{Wang20balancing,
    title = "Balancing Training for Multilingual Neural Machine Translation",
    author = "Wang, Xinyi and Tsvetkov, Yulia and Neubig, Graham",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.754",
    doi = "10.18653/v1/2020.acl-main.754",
    pages = "8526--8537",
    abstract = "When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.",
}

@inproceedings{Wang20learning-multi,
    title = "Learning a Multi-Domain Curriculum for Neural Machine Translation",
    author = "Wang, Wei and Tian, Ye and Ngiam, Jiquan and Yang, Yinfei and Caswell, Isaac and Parekh, Zarana",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.689",
    doi = "10.18653/v1/2020.acl-main.689",
    pages = "7711--7723",
    abstract = "Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.",
}

@inproceedings{Zhou20uncertainty,
    title = "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
    author = "Zhou, Yikai and Yang, Baosong and Wong, Derek F. and Wan, Yu and Chao, Lidia S.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.620",
    doi = "10.18653/v1/2020.acl-main.620",
    pages = "6934--6944",
    abstract = "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",
}

@InProceedings{Zhang20Pegasus,
  title =        {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  author =       {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle =    {Proceedings of the 37th International Conference on Machine Learning},
  pages =        {11328--11339},
  year =         {2020},
  editor =       {Hal Daumé III and Aarti Singh},
  volume =       {119},
  series =       {Proceedings of Machine Learning Research},
  month =        {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf},
  url =          {
http://proceedings.mlr.press/v119/zhang20ae.html
},
  abstract =     {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.}
}

@article{Saunders21Asurvey,
  author    = {Danielle Saunders},
  title     = {Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2104.06951},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.06951},
  archivePrefix = {arXiv},
  eprint    = {2104.06951},
}

@PhdThesis{Saunders21Domain,
  author = 	 {Danielle Saunders},
  title = 	 {Domain Adaptation for Neural Machine Translation},
  school = 	 {Department of Engineering, University of Cambridge},
  year = 	 2021}

@article{Pham21revisiting,
      author = {Minh Quang Pham and Josep Crego and François Yvon},
      title = {Revisiting Multi-Domain Machine Translation},
      journal = {Transactions of the Association for Computational Linguistics},
      volume = {9},
      number = {0},
      year = {2021},
      keywords = {},
      abstract = {When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work, that fall under the general umbrella of transfer learning. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.},
     issn = {2307-387X},	pages = {17--35},
     url = {https://transacl.org/index.php/tacl/article/view/2327}
}

@article{Stergiadis21Multi,
  title={Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging},
  author={Emmanouil Stergiadis and Satendra Kumar and Fedor Kovalev and Pavel Levin},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.10160}
}

@article{Kumar21learning,
  title={Learning Policies for Multilingual Training of Neural Machine Translation Systems},
  author={Kumar, Gaurav and Koehn, Philipp and Khudanpur, Sanjeev},
  journal   = {CoRR},
  volume    = {abs/2103.06964},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.06964},
  archivePrefix = {arXiv},
  eprint    = {2103.06964},
  }













