@article{Cauchy1847method,
author="Cauchy, A.",
title="Methode generale pour la resolution des systemes d'equations simultanees",
journal="C.R. Acad. Sci. Paris",
ISSN="",
publisher="",
year="1847",
month="",
volume="25",
number="",
pages="536-538",
URL="https://ci.nii.ac.jp/naid/10026863174/en/",
DOI="",
}

@article{Kullback51On,
author = {S. Kullback and R. A. Leibler},
title = {{On Information and Sufficiency}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {79 -- 86},
year = {1951},
doi = {10.1214/aoms/1177729694},
URL = {https://doi.org/10.1214/aoms/1177729694}
}


@article{Herbert51stochastic,
author = {Herbert Robbins and Sutton Monro},
title = {{A Stochastic Approximation Method}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {400 -- 407},
year = {1951},
doi = {10.1214/aoms/1177729586},
URL = {https://doi.org/10.1214/aoms/1177729586}
}
@article{Kiefer52stochastic,
author = {J. Kiefer and J. Wolfowitz},
title = {{Stochastic Estimation of the Maximum of a Regression Function}},
volume = {23},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {462 -- 466},
year = {1952},
doi = {10.1214/aoms/1177729392},
URL = {https://doi.org/10.1214/aoms/1177729392}
}

@article{Salton73On,
  title={On the Specification of Term Values in Automatic Indexing},
  author={G. Salton and C. S. Yang},
  journal={Journal of Documentation},
  year={1973},
  volume={29},
  pages={351-372}
}

@inproceedings{Baum87Supervised,
 author = {Baum, Eric and Wilczek, Frank},
 booktitle = {Neural Information Processing Systems},
 editor = {D. Anderson},
 pages = {},
 publisher = {American Institute of Physics},
 title = {Supervised Learning of Probability Distributions by Neural Networks},
 url = {https://proceedings.neurips.cc/paper/1987/file/eccbc87e4b5ce2fe28308fd9f2a7baf3-Paper.pdf},
 year = {1988}
}

@inbook{Rumelhart88learning,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title = {Learning Representations by Back-Propagating Errors},
year = {1988},
isbn = {0262010976},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Neurocomputing: Foundations of Research},
pages = {696–699},
numpages = {4}
}
@article{Michael89catastrophic,
title = "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
author = "Michael McCloskey and Cohen, {Neal J.}",
year = "1989",
doi = "10.1016/S0079-7421(08)60536-8",
language = "English (US)",
volume = "24",
pages = "109--165",
journal = "Psychology of Learning and Motivation - Advances in Research and Theory",
issn = "0079-7421",
publisher = "Academic Press Inc.",
number = "C",
}

@article{Wolpert92stacked,
title = "Stacked generalization",
journal = "Neural Networks",
volume = "5",
number = "2",
pages = "241 - 259",
year = "1992",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80023-1",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005800231",
author = "David H. Wolpert",
keywords = "Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction",
abstract = "This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory."
}

@inproceedings{Yarowsky93onesense,
 author = {Yarowsky, David},
 title = {One Sense Per Collocation},
 booktitle = {Proceedings of the Workshop on Human Language Technology},
 series = {HLT '93},
 year = {1993},
 isbn = {1-55860-324-7},
 location = {Princeton, New Jersey},
 pages = {266--271},
 numpages = {6},
 url = {https://doi.org/10.3115/1075671.1075731},
 doi = {10.3115/1075671.1075731},
 acmid = {1075731},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA}
} 
@article{Gage94anew,
author = {Gage, Philip},
title = {A New Algorithm for Data Compression},
year = {1994},
issue_date = {Feb. 1994},
publisher = {R &amp; D Publications, Inc.},
address = {USA},
volume = {12},
number = {2},
issn = {0898-9788},
journal = {C Users J.},
month = feb,
pages = {23–38},
numpages = {16}
}
@article{Caruana97multitask,
 author = {Caruana, Rich},
 title = {Multitask Learning},
 journal = {Mach. Learn.},
 issue_date = {July 1997},
 volume = {28},
 number = {1},
 month = jul,
 year = {1997},
 issn = {0885-6125},
 pages = {41--75},
 numpages = {35},
 url = {https://doi.org/10.1023/A:1007379606734},
 doi = {10.1023/A:1007379606734},
 acmid = {262872},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {backpropagation, generalization, inductive transfer, k-nearest neighbor, kernel regression, multitask learning, parallel transfer, supervised learning}
} 

@article{Hochreiter97long,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@inproceedings{Och98improving,
    title = "Improving Statistical Natural Language Translation with Categories and Rules",
    author = "Och, Franz Josef  and
      Weber, Hans",
    booktitle = "{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics",
    year = "1998",
    url = "https://www.aclweb.org/anthology/C98-2157",
}

@book{Thrun98learning,
 editor = {Thrun, Sebastian and Pratt, Lorien},
 title = {Learning to Learn},
 year = {1998},
 isbn = {0-7923-8047-9},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
} 


@article{Vilalta01perspective,
author = {Vilalta, Ricardo and Drissi, Youssef},
year = {2001},
month = {09},
pages = {},
title = {A Perspective View And Survey Of Meta-Learning},
volume = {18},
journal = {Artificial Intelligence Review},
doi = {10.1023/A:1019956318069}
}

@article{Lee01Genres,
  title={Genres, Registers, Text Types, Domains and Styles: Clarifying the Concepts and Navigating a Path through the BNC Jungle},
  author={D. Y. Lee},
  journal={Language Learning \altand Technology},
  year={2001},
  volume={5},
  pages={37-72}
}

@article{Jaeger02tutorial,
author = {Jaeger, Herbert},
year = {2002},
month = {01},
pages = {},
title = {Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the echo state network approach},
volume = {5},
journal = {GMD-Forschungszentrum Informationstechnik, 2002.}
}


@inproceedings{Papineni02bleu,
	Address = {Stroudsburg, PA, USA},
	Author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	Booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
	Location = {Philadelphia, Pennsylvania},
	Numpages = {8},
	Pages = {311--318},
	Series = {ACL '02},
	Title = {{BLEU:} a method for automatic evaluation of machine translation},
	Year = {2002}}

@article{Bengio03aneural,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1137–1155},
numpages = {19}
}

@InProceedings{Utiyama03reliable,
  author = 	"Utiyama, Masao
		and Isahara, Hitoshi",
  title = 	"Reliable Measures for Aligning Japanese-English News Articles and Sentences",
  booktitle = 	"Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
  year = 	"2003",
  url = 	"http://aclweb.org/anthology/P03-1010"
}

@article{Villani03topics,
author = {Villani, C},
year = {2003},
month = {01},
pages = {},
title = {Topics in Optimal Transportation Theory},
volume = {58},
doi = {10.1090/gsm/058}
}

@InProceedings{Utiyama03measure,
  author = 	"Utiyama, Masao
		and Isahara, Hitoshi",
  title = 	"Reliable Measures for Aligning {Japanese-English} News Articles and Sentences",
  booktitle = 	"Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
  address =    "Sapporo, Japan",
  year = 	"2003",
  url = 	"http://aclweb.org/anthology/P03-1010"
}

@inproceedings{Koehn04pharaoh,
author = {Koehn, Philipp},
year = {2004},
month = {09},
pages = {115-124},
title = {Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models},
isbn = {978-3-540-23300-8},
doi = {10.1007/978-3-540-30194-3_13}
}

@InProceedings{Burago04metric,
author="Burago, Yuri
and Shoenthal, David",
editor="Bingham, Kenrick
and Kurylev, Yaroslav V.
and Somersalo, Erkki",
title="Metric Geometry",
booktitle="New Analytic and Geometric Methods in Inverse Problems",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="3--50",
abstract="Much of one's mathematical experience with regard to metric spaces begins at the level of the metric. Instead of starting with a metric, in many cases we must begin with the length of paths as the primary notion. From this, we will derive a distance function. More precisely, we can introduce a new distance which is measured along the shortest path between two points in a space (as opposed to simply measuring the Euclidean distance between the two points). One says that a distance function on a metric space is an intrinsic metric if the distance between two points can be realized by paths connecting the points (mathematically, it must be equal to the infimum of lengths of paths between the points---a shortest path may not exist). If the length of paths is to be our primary notion, we must ask for a rigorous definition, from where it may arise, and what the properties are of such structures (which we will call length structures).",
isbn="978-3-662-08966-8"
}

@inproceedings{Koehn05europarl,
	Address = {Phuket, Thailand},
	Author = {Philipp Koehn},
	Booktitle = {2nd Workshop on EBMT of MT-Summit X},
	Pages = {79--86},
	Title = {Europarl: A Parallel Corpus for {Statistical Machine Translation}},
	Year = 2005}

@InProceedings{Steinberger06acquis,
  author = {Ralf Steinberger and Bruno Pouliquen and Anna Widiger and Camelia Ignat and Tomaž Erjavec and Dan Tufis and Dániel Varga },
  title = {The {JRC-Acquis}: A multilingual aligned parallel corpus with 20+ languages},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation},
  series = {LREC'06},
  year = {2006},
  month = {may},
  date = {24-26},
  address = {Genoa, Italy},
  publisher = {European Language Resources Association (ELRA)},
 }
@inproceedings{Blitzer06Domain,
    title = "Domain Adaptation with Structural Correspondence Learning",
    author = "Blitzer, John  and
      McDonald, Ryan  and
      Pereira, Fernando",
    booktitle = "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2006",
    address = "Sydney, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W06-1615",
    pages = "120--128",
}
@incollection{Ben07analysis,
title = {Analysis of Representations for Domain Adaptation},
author = {Ben-David, Shai and John Blitzer and Crammer, Koby and Fernando Pereira},
booktitle = {Advances in Neural Information Processing Systems 19},
editor = {B. Sch\"{o}lkopf and J. C. Platt and T. Hoffman},
pages = {137--144},
year = {2007},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/2983-analysis-of-representations-for-domain-adaptation.pdf}
}

@InProceedings{Daume07frustratingly,
  author = 	"Daume III, Hal",
  title = 	"Frustratingly Easy Domain Adaptation",
  booktitle = 	"Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
  year = 	"2007",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"256--263",
  location = 	"Prague, Czech Republic",
  url = 	"http://aclweb.org/anthology/P07-1033"
}

@inproceedings{Foster07mixture,
	Address = {Prague, Czech Republic},
	Author = {Foster, George and Kuhn, Roland},
	Booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
	Pages = {128--135},
	Title = {Mixture-Model Adaptation for {SMT}},
	Url = {http://www.aclweb.org/anthology/W/W07/W07-0717},
	Year = {2007},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/W/W07/W07-0717}}

@inproceedings{Collobert08aunified,
author = {Collobert, Ronan and Weston, Jason},
title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390177},
doi = {10.1145/1390156.1390177},
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {160–167},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@inproceedings{Dredze08online,
	Address = {Honolulu, Hawaii},
	Author = {Dredze, Mark and Crammer, Koby},
	Booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
	Month = oct,
	Pages = {689--697},
	series = {EMNLP},
	Title = {Online Methods for Multi-Domain Learning and Adaptation},
	Url = {https://www.aclweb.org/anthology/D08-1072},
	Year = {2008},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D08-1072}}

@incollection{Mansour09domain,
title = {Domain Adaptation with Multiple Sources},
author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1041--1048},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf}
}

@InCollection{Tiedemann09news,
  author =	  {J\"org Tiedemann},
  title =	  {News from {OPUS} - {A} Collection of Multilingual
                  Parallel Corpora with Tools and Interfaces},
  booktitle =	  {Recent Advances in Natural Language Processing},
  publisher =	  {John Benjamins, Amsterdam/Philadelphia},
  year =          2009,
  pages =         {237--248},
  editor =        {N. Nicolov and K. Bontcheva and G. Angelova and
                  R. Mitkov},
  volume =	  {V},
  address =	  {Borovets, Bulgaria},
  isbn =          {978 90 272 4825 1},
  pdf =           {http://stp.lingfil.uu.se/~joerg/published/ranlp-V.pdf},
  topic  =        {Parallel corpora}
}

@inproceedings{Finkel09hierarchical,
	Address = {Boulder, Colorado},
	Author = {Finkel, Jenny Rose and Manning, Christopher D.},
	Booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics},
	Month = jun,
	Pages = {602--610},
	Title = {Hierarchical {B}ayesian Domain Adaptation},
	Url = {https://www.aclweb.org/anthology/N09-1068},
	Year = {2009},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/N09-1068}}

@InProceedings{Finkel09hierarchical,
  author = 	"Finkel, Jenny Rose
		and Manning, Christopher D.",
  title = 	"Hierarchical Bayesian Domain Adaptation",
  booktitle = 	"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
  year = 	"2009",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"602--610",
  location = 	"Boulder, Colorado",
  url = 	"http://aclweb.org/anthology/N09-1068"
}

@inproceedings{Yishay09multiple,
title	= {Multiple Source Adaptation and the Renyi Divergence},
author	= {Yishay Mansour and Mehryar Mohri and Afshin Rostamizadeh},
year	= {2009},
URL	= {http://www.cs.nyu.edu/~mohri/postscript/renyi.pdf},
booktitle	= {Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence  (UAI 2009)},
address	= {Montr\'eal, Canada}
}
@conference{Yishay09domain,
title = "Domain adaptation: Learning bounds and algorithms",
abstract = "This paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data. Building on previous work by Ben-David et al. (2007), we introduce a novel distance between distributions, discrepancy distance, that is tailored to adaptation problems with arbitrary loss functions. We give Rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions. Using this distance, we derive new generalization bounds for domain adaptation for a wide family of loss functions. We also present a series of novel adaptation bounds for large classes of regularization-based algorithms, including support vector machines and kernel ridge regression based on the empirical discrepancy. This motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give several algorithms. We report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation.",
author = "Yishay Mansour and Mehryar Mohri and Afshin Rostamizadeh",
year = "2009",
month = dec,
day = "1",
language = "English (US)",
note = "22nd Conference on Learning Theory, COLT 2009 ; Conference date: 18-06-2009 Through 21-06-2009",
}

@inproceedings{Daume09bayes,
 author = {Daum{\'e},III, Hal},
 title = {Bayesian Multitask Learning with Latent Hierarchies},
 booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
 series = {UAI '09},
 year = {2009},
 isbn = {978-0-9749039-5-8},
 location = {Montreal, Quebec, Canada},
 pages = {135--142},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=1795114.1795131},
 acmid = {1795131},
 publisher = {AUAI Press},
 address = {Arlington, Virginia, United States}
} 

@book{Mccandless2010Lucene,
 author = {McCandless, Michael and Hatcher, Erik and Gospodnetic, Otis},
 title = {Lucene in Action, Second Edition: Covers Apache Lucene 3.0},
 year = {2010},
 isbn = {1933988177, 9781933988177},
 publisher = {Manning Publications Co.},
 address = {Greenwich, CT, USA}
} 

@article{Ben10A,
title	= {A theory of learning from different domains},
author	= {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jennifer Vaughan},
year	= {2010},
URL	= {http://www.springerlink.com/content/q6qk230685577n52/},
journal	= {Machine Learning},
pages	= {151--175},
volume	= {79}
}

@inproceedings{Nair10rectified,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@InProceedings{Moore10intelligent,
  author = 	"Moore, Robert C.
		and Lewis, William",
  title = 	"Intelligent Selection of Language Model Training Data",
  booktitle = 	"Proceedings of the ACL 2010 Conference Short Papers",
  year = 	"2010",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"220--224",
  location = 	"Uppsala, Sweden",
  url = 	"http://aclweb.org/anthology/P10-2041"
}

@inproceedings{Paul10overview,
  title={Overview of the {IWSLT} 2010 evaluation campaign},
  author={Paul, Michael and Federico, Marcello and St{\"u}ker, Sebastian},
  booktitle={International Workshop on Spoken Language Translation (IWSLT) 2010},
  series = {IWSLT},
  address = {Paris, France},
  pages     = {3--27},
  year      = {2010},
  url = {https://www.isca-speech.org/archive/iwslt_10/papers/slta_003.pdf}
}  

@article{Shai10A,
title	= {A theory of learning from different domains},
author	= {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jennifer Vaughan},
year	= {2010},
URL	= {http://www.springerlink.com/content/q6qk230685577n52/},
journal	= {Machine Learning},
pages	= {151--175},
volume	= {79}
}

@inproceedings{Chang10necessity,
	Address = {Cambridge, MA},
	Author = {Chang, Ming-Wei and Connor, Michael and Roth, Dan},
	Booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	Month = oct,
	Pages = {767--777},
	Title = {The Necessity of Combining Adaptation Methods},
	Url = {https://www.aclweb.org/anthology/D10-1075},
	Year = {2010},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D10-1075}}

@INPROCEEDINGS{Bottou10large,
    author = {Léon Bottou},
    title = {Large-scale machine learning with stochastic gradient descent},
    booktitle = {in COMPSTAT},
    year = {2010}
}


@InProceedings{Glorot10understanding, 
title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Glorot, Xavier and Bengio, Yoshua}, booktitle = {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages = {249--256}, year = {2010}, editor = {Teh, Yee Whye and Titterington, Mike}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = { http://proceedings.mlr.press/v9/glorot10a.html }, abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} }

@inproceedings{Foster10discriminative,
    title = "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation",
    author = "Foster, George  and
      Goutte, Cyril  and
      Kuhn, Roland",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D10-1044",
    pages = "451--459",
}

@inproceedings{Daume11domain,
    title = "Domain Adaptation for Machine Translation by Mining Unseen Words",
    author = "Daum{\'e} III, Hal  and
      Jagarlamudi, Jagadeesh",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P11-2071",
    pages = "407--412",
}
	
@InProceedings{Axelrod11domain,
  author = 	"Axelrod, Amittai
		and He, Xiaodong
		and Gao, Jianfeng",
  title = 	"Domain Adaptation via Pseudo In-Domain Data Selection",
  booktitle = 	"Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2011",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"355--362",
  location = 	"Edinburgh, Scotland, UK.",
  url = 	"http://aclweb.org/anthology/D11-1033"
}

@article{Collobert11natural,
  author  = {Ronan Collobert and Jason Weston and L{{\'e}}on Bottou and Michael Karlen and Koray Kavukcuoglu and Pavel Kuksa},
  title   = {Natural Language Processing (Almost) from Scratch},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {76},
  pages   = {2493-2537},
  url     = {http://jmlr.org/papers/v12/collobert11a.html}
}

@InProceedings{Lambert11investigation,
  author = 	"Lambert, Patrik
		and Schwenk, Holger
		and Servan, Christophe
		and Abdul-Rauf, Sadaf",
  title = 	"Investigations on Translation Model Adaptation Using Monolingual Data",
  booktitle = 	"Proceedings of the Sixth Workshop on Statistical Machine Translation",
  year = 	"2011",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"284--293",
  location = 	"Edinburgh, Scotland",
  url = 	"http://aclweb.org/anthology/W11-2132"
}

@article{Memoli11gromov,
author = {Mémoli, Facundo},
year = {2011},
month = {08},
pages = {417-487},
title = {Gromov–Wasserstein Distances and the Metric Approach to Object Matching},
volume = {11},
journal = {Foundations of Computational Mathematics},
doi = {10.1007/s10208-011-9093-5}
}
@InProceedings{Mansour12simple,
author= {Mansour, Saab and Ney, Hermann},
title= {A Simple and Effective Weighted Phrase Extraction for Machine Translation Adaptation},
booktitle= {International Workshop on Spoken Language Translation},
year= 2012,
pages= {193-200},
address= {Hong Kong},
month= dec,
booktitlelink= {http://hltc.cs.ust.hk/iwslt/},
pdf = {https://www-i6.informatik.rwth-aachen.de/publications/downloader.php?id=832&row=pdf}
}

@inproceedings{Cettolo12wit,
        Address = {Trento, Italy},
        Author = {Mauro Cettolo and Christian Girardi and Marcello Federico},
        Booktitle = {Proceedings of the 16$^{th}$ Conference of the European Association for Machine Translation (EAMT)},
        Date = {28-30},
        Month = {May},
        Pages = {261--268},
        Title = {WIT$^3$: Web Inventory of Transcribed and Translated Talks},
        Year = {2012}}

@InProceedings{Tiedemann12parallel,
  author = {J\"org Tiedemann},
  title = {Parallel Data, Tools and Interfaces in {OPUS}},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation},
  series = {LREC'12},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
 }
 
@inproceedings{Schwenk12continuous,
    title = "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation",
    author = "Schwenk, Holger",
    booktitle = "Proceedings of {COLING} 2012: Posters",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C12-2104",
    pages = "1071--1080",
}

@inproceedings{Le12continuous,
    title = "Continuous Space Translation Models with Neural Networks",
    author = "Le, Hai Son  and
      Allauzen, Alexandre  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N12-1005",
    pages = "39--48",
}

@inproceedings{Mike12japanese,
title	= {Japanese and Korean Voice Search},
author	= {Mike Schuster and Kaisuke Nakajima},
year	= {2012},
booktitle	= {International Conference on Acoustics, Speech and Signal Processing},
pages	= {5149--5152}
}

@inproceedings{Joshi12multidomain,
        Abstract = {We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multi-domain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced label setting, although in practice many multi-domain settings have domain-specific label biases. When multi-domain learning is applied to these settings, (2) are multi-domain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.},
        Author = {Mahesh Joshi and Mark Dredze and William W Cohen and Carolyn P Rose},
        Booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
        Pages = {1302-1312},
        Title = {Multi-Domain Learning: When Do Domains Matter?},
        Year = {2012}
} 

@inproceedings{Pascanu13onthe,
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
title = {On the Difficulty of Training Recurrent Neural Networks},
year = {2013},
publisher = {JMLR.org},
abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–1310–III–1318},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@inproceedings{Cuturi13sinkhorn,
 author = {Cuturi, Marco},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {2292--2300},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999868},
 acmid = {2999868},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@inproceedings{Mikolov13distributed,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Distributed Representations of Words and Phrases and their Compositionality},
 url = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
 volume = {26},
 year = {2013}
}

@inproceedings{Mikolov13efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and G. Corrado and J. Dean},
  booktitle={ICLR},
  year={2013}
}


@InProceedings{Duh13adaptation,
  author = 	"Duh, Kevin
		and Neubig, Graham
		and Sudoh, Katsuhito
		and Tsukada, Hajime",
  title = 	"Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation",
  booktitle = 	"Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"678--683",
  location = 	"Sofia, Bulgaria",
  url = 	"http://aclweb.org/anthology/P13-2119"
}

@InProceedings{Kalchbrenner13recurrent,
  author = 	"Kalchbrenner, Nal
		and Blunsom, Phil",
  title = 	"Recurrent Continuous Translation Models",
  booktitle = 	"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1700--1709",
  location = 	"Seattle, Washington, USA",
  url = 	"http://aclweb.org/anthology/D13-1176"
}

@inproceedings{Sennrich13multidomain,
	Address = {Sofia, Bulgaria},
	Author = {Sennrich, Rico and Schwenk, Holger and Aransa, Walid},
	Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Date-Added = {2019-05-14 22:42:31 +0200},
	Date-Modified = {2019-05-14 22:42:31 +0200},
	Month = aug,
	Pages = {832--840},
	Publisher = {Association for Computational Linguistics},
	Title = {A Multi-Domain Translation Model Framework for Statistical Machine Translation},
	Url = {https://www.aclweb.org/anthology/P13-1082},
	Year = {2013},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P13-1082}}

@inproceedings{Rico13multi,
  author    = {Rico Sennrich and
               Holger Schwenk and
               Walid Aransa},
  title     = {A Multi-Domain Translation Model Framework for Statistical Machine
               Translation},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2013, 4-9 August 2013, Sofia, Bulgaria, Volume
               1: Long Papers},
  pages     = {832--840},
  year      = {2013},
  crossref  = {DBLP:conf/acl/2013-1},
  url       = {https://www.aclweb.org/anthology/P13-1082/},
  timestamp = {Mon, 19 Aug 2019 18:10:05 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/SennrichSA13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Duh13selection,
  author = 	"Duh, Kevin
		and Neubig, Graham
		and Sudoh, Katsuhito
		and Tsukada, Hajime",
  title = 	"Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation",
  booktitle = 	"Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"678--683",
  address = 	"Sofia, Bulgaria",
  url = 	"http://aclweb.org/anthology/P13-2119"
}

@article{Ann13measuring,
  author    = {Ann Irvine and
               John Morgan and
               Marine Carpuat and
               Hal Daum{\'{e}} III and
               Dragos Stefan Munteanu},
  title     = {Measuring Machine Translation Errors in New Domains},
  journal   = {{TACL}},
  volume    = {1},
  pages     = {429--440},
  year      = {2013},
  url       = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/111},
  timestamp = {Thu, 28 May 2015 17:23:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tacl/IrvineMCDM13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@phdthesis{Rico13domain,
           title = {Domain adaptation for translation models in statistical machine translation},
          school = {University of Zurich},
          author = {Rico Sennrich},
            year = {2013},
        language = {english},
             url = {https://doi.org/10.5167/uzh-88574},
        abstract = {We investigate methods to adapt translation models in SMT to a specific target domain. We discuss two major problems, unknown words because of data sparseness in the (in-domain) training data, and ambiguities arising from out-of-domain parallel texts with different domain-specific translations. We propose novel solutions to both problems.
The main contributions of this thesis are as follows:
* We present a novel translation model architecture that supports domain adaptation at decoding time from a vector of component models. The combination is implemented through instance weighting, and all statistics necessary for the computation of translation probabilities are stored in the models.
* We present an architecture to combine multiple MT systems, using techniques and ideas from domain adaptation. The hypotheses by external MT systems are treated as out-of-domain knowledge, and combined with in-domain data through instance weighting.
* We introduce a sentence alignment algorithm that is able to robustly align even noisy parallel texts. We found that higher-quality sentence alignment of the in-domain parallel text has a significant effect on translation quality in our target domain.
* We propose new translation model features that express how flexible, or general, translation units are, in order to prevent translations that only occur in the context of multiword expressions from being overgeneralised.}
}
@inproceedings{Cho14properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@InProceedings{Wang14neural,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation ",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"189--195",
  location = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1023",
  url = 	"http://aclweb.org/anthology/D14-1023"
}
@inproceedings{Hasler14dynamic,
    title = "Dynamic Topic Adaptation for Phrase-based {MT}",
    author = "Hasler, Eva  and
      Blunsom, Phil  and
      Koehn, Philipp  and
      Haddow, Barry",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E14-1035",
    doi = "10.3115/v1/E14-1035",
    pages = "328--337",
}
@inproceedings{Cuong14latent,
  author    = {Cuong Hoang and
               Khalil Sima'an},
  title     = {Latent Domain Translation Models in Mix-of-Domains Haystack},
  booktitle = {{COLING} 2014, 25th International Conference on Computational Linguistics,
               Proceedings of the Conference: Technical Papers, August 23-29, 2014,
               Dublin, Ireland},
  pages     = {1928--1939},
  year      = {2014},
  crossref  = {DBLP:conf/coling/2014},
  url       = {https://www.aclweb.org/anthology/C14-1182/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/coling/HoangS14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Carpuat14linear,
    title = "Linear Mixture Models for Robust Machine Translation",
    author = "Carpuat, Marine  and
      Goutte, Cyril  and
      Foster, George",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3363",
    doi = "10.3115/v1/W14-3363",
    pages = "499--509",
}

@InProceedings{Hoang14latent,
  author = 	"Hoang, Cuong
		and Sima'an, Khalil",
  title = 	"Latent Domain Translation Models in Mix-of-Domains Haystack",
  booktitle = 	"Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2014",
  publisher = 	"Dublin City University and Association for Computational Linguistics",
  pages = 	"1928--1939",
  location = 	"Dublin, Ireland",
  url = 	"http://aclweb.org/anthology/C14-1182"
}

@inproceedings{Bahdanau14neural,
	Author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proc. {ICLR}},
	Date-Added = {2015-10-05 20:27:54 +0000},
	Date-Modified = {2015-10-08 23:13:02 +0000},
	Title = {Neural machine translation by jointly learning to align and translate},
	Year = {2015}
}

@inproceedings{Sutskever14sequence,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
 title = {Sequence to Sequence Learning with Neural Networks},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {3104--3112},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
 acmid = {2969173},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
 } 
 
@book{Nesterov14introductory,
author = {Nesterov, Yurii},
title = {Introductory Lectures on Convex Optimization: A Basic Course},
year = {2014},
isbn = {1461346916},
publisher = {Springer Publishing Company, Incorporated},
edition = {1},
abstract = {It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].}
}

@InProceedings{Bojar14findings,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\v{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@inproceedings{Diederick14auto,
  author    = {Diederik P. Kingma and
               Max Welling},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  crossref  = {DBLP:conf/iclr/2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
	
@InProceedings{Wang14neural,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation ",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"189--195",
  address = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1023",
  url = 	"http://aclweb.org/anthology/D14-1023"
}
@inproceedings{Chen2014systematic,
    title = "A Systematic Comparison of Smoothing Techniques for Sentence-Level {BLEU}",
    author = "Chen, Boxing  and
      Cherry, Colin",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3346",
    doi = "10.3115/v1/W14-3346",
    pages = "362--367",
}

@article{Srivastava14Dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{Huck15mixed,
  title={Mixed-Domain vs. Multi-Domain Statistical Machine Translation},
  author={Matthias Huck and Alexandra Birch and Barry Haddow},
  year={2015}
}

@inproceedings{Luong15stanford,
  Address = {Da Nang, Vietnam},
  Author = {Luong, Minh-Thang  and Manning, Christopher D.},
  Booktitle = {International Workshop on Spoken Language Translation},
  Title = {Stanford Neural Machine Translation Systems for Spoken Language Domain},
  Year = {2015}}

@inproceedings{Durrani15using,
  title={Using Joint Models for Domain Adaptation in Statistical Machine Translation},
  author={Nadir Durrani and Hassan Sajjad and Shafiq R. Joty and Ahmed Abdelali and Stephan Vogel},
  year={2015}
}  

@article{Wang15character,
  author    = {Wang Ling and
               Isabel Trancoso and
               Chris Dyer and
               Alan W. Black},
  title     = {Character-based Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1511.04586},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.04586},
  archivePrefix = {arXiv},
  eprint    = {1511.04586},
  timestamp = {Mon, 13 Aug 2018 16:48:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LingTDB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hinton15Distilling,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
URL	= {http://arxiv.org/abs/1503.02531},
booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}

@inproceedings{Wees15whats,
    title = "What{'}s in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Weerkamp, Wouter  and
      Monz, Christof",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2092",
    doi = "10.3115/v1/P15-2092",
    pages = "560--566",
}

@inproceedings{Ioffe15batch,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{Yang15unified,
	title = {A unified perspective on multi-domain and multi-task learning},
	author = {Yongxin Yang and Timothy M. Hospedales},
	booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	year = {2015},
	url = {https://arxiv.org/abs/1412.7489},
}

@inproceedings{Bahdanau15learning,
	Author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	Title = {Neural machine translation by jointly learning to align and translate},
	Year = {2015},
	url = {https://arxiv.org/pdf/1409.0473.pdf},
}

@inproceedings{Sennrich16neural,
	Address = {Berlin, Germany},
	Author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Doi = {10.18653/v1/P16-1162},
	Month = aug,
	Pages = {1715--1725},
	Title = {Neural Machine Translation of Rare Words with Subword Units},
	Url = {https://www.aclweb.org/anthology/P16-1162},
	Year = {2016},
}

@InProceedings{Sennrich16politeness,
  author = 	"Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  address = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Zhang16topicinformed,
  author = 	"Zhang, Jian and Li, Liangyou and Way, Andy and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers",
  series =      "COLING 2016",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@inproceedings{Chen16guided,
  title={Guided Alignment Training for Topic-Aware Neural Machine Translation},
  author={Wenhu Chen and Evgeny Matusov and Shahram Khadivi and Jan-Thorsten Peter},
  address = {Austin, Texas},
  year={2016},
  Booktitle = {Proceedings of the Twelth Biennial Conference of the Association for Machine Translation in the Americas},
  Series = {AMTA 2012}
}

@inproceedings{He16deep,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
year = {2016},
month = {06},
pages = {770-778},
title = {Deep Residual Learning for Image Recognition},
doi = {10.1109/CVPR.2016.90}
}

@article{Jimmy16layer,
  title={Layer Normalization},
  author={Jimmy Ba and J. Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@InProceedings{Wang16connecting,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Connecting Phrase based Statistical Machine Translation Adaptation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"3135--3145",
  location = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1295"
}

@inproceedings{Bousmalis16domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA}
} 

@inproceedings{Costa16character,
    title = "Character-based Neural Machine Translation",
    author = "Costa-juss{\`a}, Marta R.  and
      Fonollosa, Jos{\'e} A. R.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2058",
    doi = "10.18653/v1/P16-2058",
    pages = "357--361",
}

@inproceedings{Luong16achieving,
    title = "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models",
    author = "Luong, Minh-Thang  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1100",
    doi = "10.18653/v1/P16-1100",
    pages = "1054--1063",
}

@inproceedings{Liu16stein,
 author = {Liu, Qiang and Wang, Dilin},
 title = {Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {2378--2386},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157362},
 acmid = {3157362},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@InProceedings{Sennrich16controlling,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  location = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Sennrich16improving,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Improving Neural Machine Translation Models with Monolingual Data",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"86--96",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1009",
  url = 	"http://aclweb.org/anthology/P16-1009"
}

@InProceedings{Sennrich16neural,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Neural Machine Translation of Rare Words with Subword Units",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1715--1725",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1162",
  url = 	"http://aclweb.org/anthology/P16-1162"
}

@InProceedings{Liu16deep,
  author = 	"Liu, Pengfei
		and Qiu, Xipeng
		and Huang, Xuanjing",
  title = 	"Deep Multi-Task Learning with Shared Memory for Text Classification",
  booktitle = 	"Proceedings of the 2016 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--127",
  location = 	"Austin, Texas",
  doi = 	"10.18653/v1/D16-1012",
  url = 	"http://aclweb.org/anthology/D16-1012"
}

@ARTICLE{Gulcehre16monolingual,
    author = {G{\"{u}}l{\c c}ehre, {\c C}ağlar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Lo{\"{\i}}c and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computer Science - Computation and Language},
     month = mar,
     title = {On Using Monolingual Corpora in Neural Machine Translation},
   journal = {arXiv e-prints},
    volume = {abs/1503.03535},
      year = {2015},
       url = {https://arxiv.org/abs/1503.03535}
}

@InProceedings{Zhang16topic,
  author = 	"Zhang, Jian
		and Li, Liangyou
		and Way, Andy
		and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  location = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@inproceedings{Ha16towards,
	Address = {Vancouver, Canada},
	Author = {Ha, Thanh-He and Niehues, Jan and Waibel, Alex},
	Booktitle = {Proceedings of the International Workshop on Spoken Language Translation},
	Date-Added = {2019-05-19 16:26:43 +0200},
	Date-Modified = {2019-05-19 16:28:13 +0200},
	Organization = {IWSLT},
	Title = {Toward Multilingual Neural Machine Translationwith Universal Encoder and Decoder},
	Year = {2016}}

@inproceedings{Firat16multiway,
	Author = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	Doi = {10.18653/v1/N16-1101},
	Location = {San Diego, California},
	Pages = {866--875},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism},
	Url = {http://www.aclweb.org/anthology/N16-1101},
	Year = {2016},
}
@InProceedings{Sennrich16politeness,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  address = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Wang16connecting,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Connecting Phrase based Statistical Machine Translation Adaptation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"3135--3145",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1295"
}

@ARTICLE{Gulcehre16monolingual,
    author = {G{\"{u}}l{\c c}ehre, {\c C}ağlar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Lo{\"{\i}}c and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computer Science - Computation and Language},
     month = mar,
     title = {On Using Monolingual Corpora in Neural Machine Translation},
   journal = {arXiv e-prints},
    volume = {abs/1503.03535},
      year = {2015},
       url = {https://arxiv.org/abs/1503.03535}
}

@inproceedings{Sennrich16improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@InProceedings{Zhang16topicinformed,
  author = 	"Zhang, Jian
		and Li, Liangyou
		and Way, Andy
		and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers",
  series =      "COLING 2016",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@article{Freitag16fast,
  title={Fast Domain Adaptation for Neural Machine Translation},
  author={Markus Freitag and Yaser Al-Onaizan},
  journal={CoRR},
  year={2016},
  volume={abs/1612.06897}
}
@ARTICLE{Kirk16overcoming,
       author = {{Kirkpatrick}, James and {Pascanu}, Razvan and {Rabinowitz}, Neil and
         {Veness}, Joel and {Desjardins}, Guillaume and {Rusu}, Andrei A. and
         {Milan}, Kieran and {Quan}, John and {Ramalho}, Tiago and
         {Grabska-Barwinska}, Agnieszka and {Hassabis}, Demis and
         {Clopath}, Claudia and {Kumaran}, Dharshan and {Hadsell}, Raia},
        title = "{Overcoming catastrophic forgetting in neural networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = "2016",
        month = "Dec",
          eid = {arXiv:1612.00796},
        pages = {arXiv:1612.00796},
archivePrefix = {arXiv},
       eprint = {1612.00796},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161200796K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Chung16character,
    title = "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation",
    author = "Chung, Junyoung  and
      Cho, Kyunghyun  and
      Bengio, Yoshua",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1160",
    doi = "10.18653/v1/P16-1160",
    pages = "1693--1703",
}

@inproceedings{Konstantinos16domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 address = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA}
} 

@inproceedings{Bousmalis16Domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@article{Servan16Domain,
  title={Domain specialization: a post-training domain adaptation for Neural Machine Translation},
  author={Christophe Servan and J. Crego and Jean Senellart},
  journal={ArXiv},
  year={2016},
  volume={abs/1612.06141}
}

@inproceedings{Chen16Bilingual,
  title={Bilingual Methods for Adaptive Training Data Selection for Machine Translation},
  author={Boxing Chen and Roland Kuhn and George F. Foster and Colin Cherry and Fei Huang},
  url = {https://amtaweb.org/wp-content/uploads/2016/10/AMTA2016_Research_Proceedings_v7.pdf#page=99},
  year={2016}
}

@article{Kenji17multi,
  title={Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation},
  author={Kenji Imamura and Eiichiro Sumita},
  journal={Journal of Natural Language Processing},
  volume={24},
  number={4},
  pages={597-618},
  year={2017},
  doi={10.5715/jnlp.24.597}
}

@inproceedings{Dakwle17fine,
  author    = {Dakwale, Praveen and Monz, Christof},
  title     = {Fine-Tuning for Neural Machine Translation with Limited Degradation across In- and Out-of-Domain Data},
  booktitle = {Proceedings of the 16th Machine Translation Summit (MT-Summit 2017)},
  pages = {156--169},
  year      = {2017}
}


@incollection{Vaswani17attention,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{Costa17byte,
    title = "Byte-based Neural Machine Translation",
    author = "Costa-juss{\`a}, Marta R.  and
      Escolano, Carlos  and
      Fonollosa, Jos{\'e} A. R.",
    booktitle = "Proceedings of the First Workshop on Subword and Character Level Models in {NLP}",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4123",
    doi = "10.18653/v1/W17-4123",
    pages = "154--158",
    abstract = "This paper presents experiments comparing character-based and byte-based neural machine translation systems. The main motivation of the byte-based neural machine translation system is to build multi-lingual neural machine translation systems that can share the same vocabulary. We compare the performance of both systems in several language pairs and we see that the performance in test is similar for most language pairs while the training time is slightly reduced in the case of byte-based neural machine translation.",
}

@InProceedings{Kobus17domain,
  author = 	"Kobus, Catherine
		and Crego, Josep
		and Senellart, Jean",
  title = 	"Domain Control for Neural Machine Translation",
  booktitle = 	"Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017",
  year = 	"2017",
  publisher = 	"INCOMA Ltd.",
  pages = 	"372--378",
  location = 	"Varna, Bulgaria",
  doi = 	"10.26615/978-954-452-049-6_049",
  url = 	"https://doi.org/10.26615/978-954-452-049-6_049"
}

@incollection{Mccann17learn,
title = {Learned in Translation: Contextualized Word Vectors},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6294--6305},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf}
}

@article{Lee17fully,
    title = "Fully Character-Level Neural Machine Translation without Explicit Segmentation",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Hofmann, Thomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1026",
    doi = "10.1162/tacl_a_00067",
    pages = "365--378",
    abstract = "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT{'}15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.",
}

@article{Imamura17Multi,
author = {Imamura, Kenji and Sumita, Eiichiro},
year = {2017},
month = {09},
pages = {597-618},
title = {Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation},
volume = {24},
journal = {Journal of Natural Language Processing},
doi = {10.5715/jnlp.24.597}
}


@InProceedings{Chen17cost,
  author = 	"Chen, Boxing
		and Cherry, Colin
		and Foster, George
		and Larkin, Samuel",
  title = 	"Cost Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the First Workshop on Neural Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"40--46",
  location = 	"Vancouver",
  doi = 	"10.18653/v1/W17-3205",
  url = 	"http://aclweb.org/anthology/W17-3205"
}

@InProceedings{Miceli17regularization,
  author = 	"Miceli Barone, Antonio Valerio
		and Haddow, Barry
		and Germann, Ulrich
		and Sennrich, Rico",
  title = 	"Regularization techniques for fine-tuning in neural machine translation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1489--1494",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1156",
  url = 	"http://aclweb.org/anthology/D17-1156"
}

@InProceedings{Britz17effective,
  author = 	"Britz, Denny
		and Le, Quoc
		and Pryzant, Reid",
  title = 	"Effective Domain Mixing for Neural Machine Translation",
  booktitle = 	"Proceedings of the Second Conference on Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--126",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/W17-4712",
  url = 	"http://aclweb.org/anthology/W17-4712"
}

@InProceedings{Chu17empirical,
  author = 	"Chu, Chenhui
		and Dabre, Raj
		and Kurohashi, Sadao",
  title = 	"An Empirical Comparison of Domain Adaptation Methods for Neural Machine      Translation    ",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 2: Short Papers)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"385--391",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-2061",
  url = 	"http://aclweb.org/anthology/P17-2061"
}

@InProceedings{Wang17instance,
  author = 	"Wang, Rui
		and Utiyama, Masao
		and Liu, Lemao
		and Chen, Kehai
		and Sumita, Eiichiro",
  title = 	"Instance Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1482--1488",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1155",
  url = 	"http://aclweb.org/anthology/D17-1155"
}
@inproceedings{Matteo17neural,
  author    = {Matteo Negri and
               Marco Turchi and
               Marcello Federico and
               Nicola Bertoldi and
               M. Amin Farajian},
  title     = {Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the
               Association for Computational Linguistics, {EACL} 2017, Valencia,
               Spain, April 3-7, 2017, Volume 2: Short Papers},
  pages     = {280--284},
  year      = {2017},
  crossref  = {DBLP:conf/eacl/2017-2},
  url       = {https://www.aclweb.org/anthology/E17-2045/},
  timestamp = {Tue, 17 Sep 2019 13:40:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/eacl/NegriTFBF17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Zhang17context,
 author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
 title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {December 2017},
 volume = {25},
 number = {12},
 month = dec,
 year = {2017},
 issn = {2329-9290},
 pages = {2424--2432},
 numpages = {9},
 url = {https://doi.org/10.1109/TASLP.2017.2751420},
 doi = {10.1109/TASLP.2017.2751420},
 acmid = {3180106},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@article{Hassan17multi,
  author    = {Hassan Sajjad and
               Nadir Durrani and
               Fahim Dalvi and
               Yonatan Belinkov and
               Stephan Vogel},
  title     = {Neural Machine Translation Training in a Multi-Domain Scenario},
  journal   = {CoRR},
  volume    = {abs/1708.08712},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.08712},
  archivePrefix = {arXiv},
  eprint    = {1708.08712},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-08712},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Klein17opennmt,
  author = 	"Klein, Guillaume
		and Kim, Yoon
		and Deng, Yuntian
		and Senellart, Jean
		and Rush, Alexander",
  title = 	"OpenNMT: Open-Source Toolkit for Neural Machine Translation",
  booktitle = 	"Proceedings of ACL 2017, System Demonstrations",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"67--72",
  location = 	"Vancouver, Canada",
  url = 	"http://aclweb.org/anthology/P17-4012"
}

@incollection{Rebuffi17learning,
 title = {Learning multiple visual domains with residual adapters},
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 booktitle = {Advances in Neural Information Processing Systems 30},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {506--516},
 year = {2017},
 publisher = {Curran Associates, Inc.},
 url = {http://papers.nips.cc/paper/6654-learning-multiple-visual-domains-with-  residual-adapters.pdf}
}

@InProceedings{Peng17multitask,
  author = 	"Peng, Nanyun
		and Dredze, Mark",
  title = 	"Multi-task Domain Adaptation for Sequence Tagging",
  booktitle = 	"Proceedings of the 2nd Workshop on Representation Learning for NLP",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"91--100",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/W17-2612",
  url = 	"http://aclweb.org/anthology/W17-2612"
}

@InProceedings{Chelsea17modelagnostic,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author = 	 {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}

@InProceedings{Ghering17convolutional,
  title = 	 {Convolutional Sequence to Sequence Learning},
  author = 	 {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1243--1252},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  url = 	 {http://proceedings.mlr.press/v70/gehring17a.html},
  abstract = 	 {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT’14 English-German and WMT’14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.}
}

@article{Jiatao17non,
  author    = {Jiatao Gu and
               James Bradbury and
               Caiming Xiong and
               Victor O. K. Li and
               Richard Socher},
  title     = {Non-Autoregressive Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1711.02281},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02281},
  archivePrefix = {arXiv},
  eprint    = {1711.02281},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-02281.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Johnson17google,
	Author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernand a and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	Issn = {2307-387X},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {339--351},
	Title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	Url = {https://transacl.org/ojs/index.php/tacl/article/view/1081},
	Volume = {5},
	Year = {2017},
	Bdsk-Url-1 = {https://transacl.org/ojs/index.php/tacl/article/view/1081}}

@inproceedings{Farajian17multidomain,
	Address = {Copenhagen, Denmark},
	Author = {Farajian, M. Amin and Turchi, Marco and Negri, Matteo and Federico, Marcello},
	Booktitle = {Proceedings of the Second Conference on Machine Translation},
	Doi = {10.18653/v1/W17-4713},
	Month = sep,
	Pages = {127--137},
	Title = {Multi-Domain Neural Machine Translation through Unsupervised Adaptation},
	Url = {https://www.aclweb.org/anthology/W17-4713},
	Year = {2017},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W17-4713},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/W17-4713}}

@inproceedings{Wang17sentence,
	Address = {Vancouver, Canada},
	Author = {Wang, Rui and Finch, Andrew and Utiyama, Masao and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	Date-Added = {2019-04-28 19:22:53 +0200},
	Date-Modified = {2019-04-28 19:22:53 +0200},
	Doi = {10.18653/v1/P17-2089},
	Pages = {560--566},
	Publisher = {Association for Computational Linguistics},
	Title = {Sentence Embedding for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/P17-2089},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/P17-2089},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P17-2089}}

@article{Biao17acontextaware,
 author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
 title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {December 2017},
 volume = {25},
 number = {12},
 month = dec,
 year = {2017},
 issn = {2329-9290},
 pages = {2424--2432},
 numpages = {9},
 url = {https://doi.org/10.1109/TASLP.2017.2751420},
 doi = {10.1109/TASLP.2017.2751420},
 acmid = {3180106},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA}
}

@inproceedings{Wang17instance,
	Author = {Wang, Rui and Utiyama, Masao and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2017-09-21 21:28:02 +0000},
	Date-Modified = {2017-09-21 21:28:10 +0000},
	Location = {Copenhagen, Denmark},
	Pages = {1483--1489},
	Publisher = {Association for Computational Linguistics},
	Title = {Instance Weighting for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/D17-1155},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D17-1155}}

@inproceedings{Domhan2017using,
    title = "Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning",
    author = "Domhan, Tobias  and
      Hieber, Felix",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1158",
    doi = "10.18653/v1/D17-1158",
    pages = "1500--1505",
    abstract = "The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.",
}

@inproceedings{Wees17dynamic,
    title = "Dynamic Data Selection for Neural Machine Translation",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1147",
    doi = "10.18653/v1/D17-1147",
    pages = "1400--1410",
    abstract = "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce {`}dynamic data selection{'} for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call {`}gradual fine-tuning{'}, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.",
}

@inproceedings{Wees17whats,
  title={What’s in a domain?: Towards fine-grained adaptation for machine translation},
  author={M. V. D. Wees},
  year={2017}
}

@article{Lee17fully,
    title = "Fully Character-Level Neural Machine Translation without Explicit Segmentation",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Hofmann, Thomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1026",
    doi = "10.1162/tacl_a_00067",
    pages = "365--378",
    abstract = "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT{'}15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.",
}

@InProceedings{Britz2017mixing,
  author = 	"Britz, Denny
		and Le, Quoc
		and Pryzant, Reid",
  title = 	"Effective Domain Mixing for Neural Machine Translation",
  booktitle = 	"Proceedings of the Second Conference on Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--126",
  address = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/W17-4712",
  url = 	"http://aclweb.org/anthology/W17-4712"
}

@misc{Khresmoi17test,
 title = {Khresmoi Summary Translation Test Data 2.0},
 author = {Du{\v s}ek, Ond{\v r}ej and Haji{\v c}, Jan and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Libovick{\'y}, Jind{\v r}ich and Pecina, Pavel and Tamchyna, Ale{\v s} and Ure{\v s}ov{\'a}, Zde{\v n}ka},
 url = {http://hdl.handle.net/11234/1-2122},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Creative Commons - Attribution-{NonCommercial} 4.0 International ({CC} {BY}-{NC} 4.0)},
 year = {2017} }

@inproceedings{Assylbekov17syllable,
    title = "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones",
    author = "Assylbekov, Zhenisbek  and
      Takhanov, Rustem  and
      Myrzakhmetov, Bagdat  and
      Washington, Jonathan N.",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1199",
    doi = "10.18653/v1/D17-1199",
    pages = "1866--1872",
    abstract = "Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18{\%}-33{\%} fewer parameters and is trained 1.2-2.2 times faster.",
}

@article{Ataman17linguistically,
  title={Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English},
  author={Duygu Ataman and Matteo Negri and M. Turchi and Marcello Federico},
  journal={The Prague Bulletin of Mathematical Linguistics},
  year={2017},
  volume={108},
  pages={331 - 342}
}

@inproceedings{Huck17target,
    title = "Target-side Word Segmentation Strategies for Neural Machine Translation",
    author = "Huck, Matthias  and
      Riess, Simon  and
      Fraser, Alexander",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4706",
    doi = "10.18653/v1/W17-4706",
    pages = "56--67",
}

@InProceedings{Chu17comparison,
  author = 	"Chu, Chenhui
		and Dabre, Raj
		and Kurohashi, Sadao",
  title = 	"An Empirical Comparison of Domain Adaptation Methods for Neural Machine      Translation    ",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 2: Short Papers)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"385--391",
  address = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-2061",
  url = 	"http://aclweb.org/anthology/P17-2061"
}

@inproceedings{Currey17copied,
    title = "Copied Monolingual Data Improves Low-Resource Neural Machine Translation",
    author = "Currey, Anna  and
      Miceli Barone, Antonio Valerio  and
      Heafield, Kenneth",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4715",
    doi = "10.18653/v1/W17-4715",
    pages = "148--156",
}

@InProceedings{Khayrallah17neural,
  author    = {Khayrallah, Huda and Kumar, Gaurav and Duh, Kevin and Post, Matt and Koehn, Philipp},
  title     = {Neural Lattice Search for Domain Adaptation in Machine Translation},
  booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  year      = {2017},
  publisher = {Asian Federation of Natural Language Processing},
  month     = {November},
  pages     = {20--25},
  url       = {http://www.aclweb.org/anthology/I17-2004},
}

@inproceedings{Chelsea17model,
  author    = {Chelsea Finn and
               Pieter Abbeel and
               Sergey Levine},
  editor    = {Doina Precup and
               Yee Whye Teh},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {1126--1135},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/finn17a.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/FinnAL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Machcek18morphological,
  title={Morphological and Language-Agnostic Word Segmentation for NMT},
  author={Dominik Mach{\'a}cek and Jon{\'a}s Vidra and Ondrej Bojar},
  booktitle={TSD},
  year={2018}
}

@inproceedings{Ott18scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
    abstract = "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT{'}14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT{'}14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
}

@inproceedings{Burlot18using,
    title = "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
    author = "Burlot, Franck  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6315",
    doi = "10.18653/v1/W18-6315",
    pages = "144--155",
    abstract = "Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation - a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.",
}

@inproceedings{Rebuffi18efficient,
  author    = {Sylvestre{-}Alvise Rebuffi and
               Hakan Bilen and
               Andrea Vedaldi},
  title     = {Efficient Parametrization of Multi-Domain Deep Neural Networks},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {8119--8127},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Rebuffi\_Efficient\_Parametrization\_of\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00847},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/RebuffiBV18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Niu18multitask,
  author    = {Xing Niu and Sudha Rao and Marine Carpuat},
  title     = {Multi-Task Neural Models for Translating Between Styles Within and Across Languages},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  pages     = {1008--1021},
  series    = {COLING},
  address   = {Santa Fe, New Mexico, USA},
  year      = {2018},
  editor    = {Emily M. Bender and Leon Derczynski and Pierre Isabelle},
  url       = {https://aclanthology.info/papers/C18-1086/c18-1086},
}

@inproceedings{Zeng18multidomain,
	Address = {Brussels, Belgium},
	Author = {Zeng, Jiali and Su, Jinsong and Wen, Huating and Liu, Yang and Xie, Jun and Yin, Yongjing and Zhao, Jianqiang},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {447--457},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination},
	Url = {http://aclweb.org/anthology/D18-1041},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1041}
} 

@inproceedings{Poncelas18Feature,
  title={Feature decay algorithms for neural machine translation},
  author={Alberto Poncelas and G. M. D. B. Wenniger and A. Way},
  booktitle = {Proceedings of the 21st Annual Conference of the European Association for Machine Translation},
  pages = {239-248},
  editor= {European Association for Machine Translation},
  url = {http://rua.ua.es/dspace/handle/10045/76084},
  
  year={2018}
}

@inproceedings{Taku18subword,
  author    = {Taku Kudo},
  editor    = {Iryna Gurevych and
               Yusuke Miyao},
  title     = {Subword Regularization: Improving Neural Network Translation Models
               with Multiple Subword Candidates},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20, 2018, Volume
               1: Long Papers},
  pages     = {66--75},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://www.aclweb.org/anthology/P18-1007/},
  doi       = {10.18653/v1/P18-1007},
  timestamp = {Mon, 16 Sep 2019 13:46:41 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/Kudo18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Michel18extreme,
  author = 	"Michel, Paul
		and Neubig, Graham",
  title = 	"Extreme Adaptation for Personalized Neural Machine Translation",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"312--318",
  address = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-2050"
}

@inproceedings{Cherry18revisiting,
    title = "Revisiting Character-Based Neural Machine Translation with Capacity and Compression",
    author = "Cherry, Colin  and
      Foster, George  and
      Bapna, Ankur  and
      Firat, Orhan  and
      Macherey, Wolfgang",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1461",
    doi = "10.18653/v1/D18-1461",
    pages = "4295--4305",
    abstract = "Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.",
}

@inproceedings{Koehn18findings,
    title = "Findings of the {WMT} 2018 Shared Task on Parallel Corpus Filtering",
    author = "Koehn, Philipp  and
      Khayrallah, Huda  and
      Heafield, Kenneth  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6453",
    doi = "10.18653/v1/W18-6453",
    pages = "726--739",
    abstract = "We posed the shared task of assigning sentence-level quality scores for a very noisy corpus of sentence pairs crawled from the web, with the goal of sub-selecting 1{\%} and 10{\%} of high-quality data to be used to train machine translation systems. Seventeen participants from companies, national research labs, and universities participated in this task.",
}

@inproceedings{Pham18fixing,
    title = "Fixing Translation Divergences in Parallel Corpora for Neural {MT}",
    author = "Pham, MinhQuang  and
      Crego, Josep  and
      Senellart, Jean  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1328",
    doi = "10.18653/v1/D18-1328",
    pages = "2967--2973",
    abstract = "Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered/corrected corpora actually improves MT performance.",
}

@InProceedings{Chu18asurvey,
  author = 	"Chu, Chenhui and Wang, Rui",
  title = 	"A Survey of Domain Adaptation for Neural Machine Translation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  series = "COLING 2018",
  year = 	"2018",
  pages = 	"1304--1319",
  address = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1111"
}

@article{Alex18first,
  author    = {Alex Nichol and
               Joshua Achiam and
               John Schulman},
  title     = {On First-Order Meta-Learning Algorithms},
  journal   = {CoRR},
  volume    = {abs/1803.02999},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.02999},
  archivePrefix = {arXiv},
  eprint    = {1803.02999},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-02999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Peters18deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@incollection{Hoffman18algorithms,
title = {Algorithms and Theory for Multiple-Source Adaptation},
author = {Hoffman, Judy and Mohri, Mehryar and Zhang, Ningshan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8246--8256},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation.pdf}
}
@inproceedings{Chu18survey,
    title = "A Survey of Domain Adaptation for Neural Machine Translation",
    author = "Chu, Chenhui  and
      Wang, Rui",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1111",
    pages = "1304--1319",
    abstract = "Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domain-specific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT.",
}
@inproceedings{Pei18multi,
  author    = {Zhongyi Pei and
               Zhangjie Cao and
               Mingsheng Long and
               Jianmin Wang},
  title     = {Multi-Adversarial Domain Adaptation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3934--3941},
  year      = {2018},
  crossref  = {DBLP:conf/aaai/2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17067},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/PeiCLW18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Li18one,
    title = "One Sentence One Model for Neural Machine Translation",
    author = "Li, Xiaoqing  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC}-2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Languages Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1146",
}

@inproceedings{Silva18extracting,
    title = "Extracting In-domain Training Corpora for Neural Machine Translation Using Data Selection Methods",
    author = "Silva, Catarina Cruz  and
      Liu, Chao-Hong  and
      Poncelas, Alberto  and
      Way, Andy",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6323",
    doi = "10.18653/v1/W18-6323",
    pages = "224--231",
}

@InProceedings{Zheng18multi,
  author = 	"Zeng, Jiali
		and Su, Jinsong
		and Wen, Huating
		and Liu, Yang
		and Xie, Jun
		and Yin, Yongjing
		and Zhao, Jianqiang",
  title = 	"Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"447--457",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1041"
}

@InProceedings{Zhang18sentence,
  author = 	"Zhang, Shiqi
		and Xiong, Deyi",
  title = 	"Sentence Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"3181--3190",
  location = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1269"
}

@InProceedings{Thompson18freezing,
  author = 	"Thompson, Brian
		and Khayrallah, Huda
		and Anastasopoulos, Antonios
		and McCarthy, Arya D.
		and Duh, Kevin
		and Marvin, Rebecca
		and McNamee, Paul
		and Gwinnup, Jeremy
		and Anderson, Tim
		and Koehn, Philipp",
  title = 	"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation",
  booktitle = 	"Proceedings of the Third Conference on Machine Translation: Research Papers",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"124--132",
  location = 	"Belgium, Brussels",
  url = 	"http://aclweb.org/anthology/W18-6313"
}

@InProceedings{Vilar18learning,
  author = 	"Vilar, David",
  title = 	"Learning Hidden Unit Contribution for Adapting Neural Machine Translation      Models    ",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies,      Volume 2 (Short Papers)    ",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"500--505",
  location = 	"New Orleans, Louisiana",
  doi = 	"10.18653/v1/N18-2080",
  url = 	"http://aclweb.org/anthology/N18-2080"
}

@InProceedings{Chu18survey,
  author = 	"Chu, Chenhui
		and Wang, Rui",
  title = 	"A Survey of Domain Adaptation for Neural Machine Translation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1304--1319",
  location = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1111"
}

@inproceedings{Zhongyi18multi,
  author    = {Zhongyi Pei and
               Zhangjie Cao and
               Mingsheng Long and
               Jianmin Wang},
  title     = {Multi-Adversarial Domain Adaptation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3934--3941},
  year      = {2018},  
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17067},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/PeiCLW18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Alvarez18gromov,
  author = 	"Alvarez-Melis, David
		and Jaakkola, Tommi",
  title = 	"Gromov-Wasserstein Alignment of Word Embedding Spaces",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1881--1890",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1214"
}

@inproceedings{Niu18multitask,
  author    = {Xing Niu and Sudha Rao and Marine Carpuat},
  title     = {Multi-Task Neural Models for Translating Between Styles Within and Across Languages},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  pages     = {1008--1021},
  series    = {COLING},
  address   = {Santa Fe, New Mexico, USA},
  year      = {2018},
  editor    = {Emily M. Bender and Leon Derczynski and Pierre Isabelle},
  crossref  = {DBLP:conf/coling/2018},
  url       = {https://aclanthology.info/papers/C18-1086/c18-1086},
}

@inproceedings{Post18A,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}

@InProceedings{Chu18multilingual,
  author = 	 {Chenhui Chu and Raj Dabre},
  title = 	 {Multilingual and multi-domain adaptation for neural machine translation},
  booktitle = {Proceedings of the 24st Annual Meeting of the Association for Natural Language Processing (NLP 2018)},
  year = 	 2018,
  pages = 	 {909-–912},
  address = 	 {Okayama, Japan}}

@inproceedings{Lample18unsupervised,
title={Unsupervised Machine Translation Using Monolingual Corpora Only},
author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
booktitle={International Conference on Learning Representations},
year={2018},
address = {Vancouver, Canada},
url={https://openreview.net/forum?id=rkYTTf-AZ},
}

@inproceedings{Artetxe18unsupervised,
title={Unsupervised Neural Machine Translation},
author={Mikel Artetxe and Gorka Labaka and Eneko Agirre and Kyunghyun Cho},
booktitle={International Conference on Learning Representations},
year={2018},
address = {Vancouver, Canada},
url={https://openreview.net/forum?id=Sy2ogebAW},
}

@inproceedings{Platanios18contextual,
	Address = {Brussels, Belgium},
	Author = {Platanios, Emmanouil Antonios and Sachan, Mrinmaya and Neubig, Graham and Mitchell, Tom},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {425--435},
	Publisher = {Association for Computational Linguistics},
	Title = {Contextual Parameter Generation for Universal Neural Machine Translation},
	Url = {http://aclweb.org/anthology/D18-1039},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1039}}

@inproceedings{Khayrallah2018regularized,
    title = "Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation",
    author = "Khayrallah, Huda  and
      Thompson, Brian  and
      Duh, Kevin  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2705",
    doi = "10.18653/v1/W18-2705",
    pages = "36--44",
    abstract = "Supervised domain adaptation{---}where a large generic corpus and a smaller in-domain corpus are both available for training{---}is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model{'}s output word distribution and that of the out-of-domain model to prevent the model{'}s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU.",
}

@inproceedings{Devlin19bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}

@inproceedings{Dou19unsupervised,
    title = "Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings",
    author = "Dou, Zi-Yi  and
      Hu, Junjie  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1147",
    doi = "10.18653/v1/D19-1147",
    pages = "1417--1422",
    abstract = "The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.",
}

@ARTICLE{Yan19wordbased,
       author = {{Yan}, Shen and {Dahlmann}, Leonard and {Petrushkov}, Pavel and
         {Hewavitharana}, Sanjika and {Khadivi}, Shahram},
        title = "{Word-based Domain Adaptation for Neural Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = "2019",
        month = "Jun",
          eid = {arXiv:1906.03129},
        pages = {arXiv:1906.03129},
archivePrefix = {arXiv},
       eprint = {1906.03129},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190603129Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Jiang19multidomain,
    title = "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing",
    author = "Jiang, Haoming  and
      Liang, Chen  and
      Wang, Chong  and
      Zhao, Tuo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.165",
    doi = "10.18653/v1/2020.acl-main.165",
    pages = "1823--1834",
    abstract = "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
}

@article{So19theevolved,
  author    = {David R. So and
               Chen Liang and
               Quoc V. Le},
  title     = {The Evolved Transformer},
  journal   = {CoRR},
  volume    = {abs/1901.11117},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.11117},
  archivePrefix = {arXiv},
  eprint    = {1901.11117},
  timestamp = {Mon, 04 Feb 2019 08:11:03 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-11117},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Junjie19domain,
    title = {Domain Adaptation of Neural Machine Translation by Lexicon Induction},
    author = {Junjie Hu and Mengzhou Xia and Graham Neubig and Jaime Carbonell},
    booktitle = {The 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
    address = {Florence, Italy},
    month = {July},
    url = {https://arxiv.org/abs/1906.00376},
    year = {2019}
}

@ARTICLE{Li19semisupervised,
       author = {{Li}, Yitong and {Baldwin}, Timothy and {Cohn}, Trevor},
        title = "{Semi-supervised Stochastic Multi-Domain Learning using Variational Inference}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = "2019",
        month = "Jun",
          eid = {arXiv:1906.02897},
        pages = {arXiv:1906.02897},
archivePrefix = {arXiv},
       eprint = {1906.02897},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190602897L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Wang19dynamically,
    title = "Dynamically Composing Domain-Data Selection with Clean-Data Selection by {``}Co-Curricular Learning{''} for Neural Machine Translation",
    author = "Wang, Wei  and
      Caswell, Isaac  and
      Chelba, Ciprian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1123",
    doi = "10.18653/v1/P19-1123",
    pages = "1282--1292",
    abstract = "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a {``}co-curricular learning{''} method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the {``}co-curriculum{''}. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",

}

@inproceedings{Miller19simplified,
    title = "Simplified Neural Unsupervised Domain Adaptation",
    author = "Miller, Timothy",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1039",
    doi = "10.18653/v1/N19-1039",
    pages = "414--419",
    abstract = "Unsupervised domain adaptation (UDA) is the task of training a statistical model on labeled data from a source domain to achieve better performance on data from a target domain, with access to only unlabeled data in the target domain. Existing state-of-the-art UDA approaches use neural networks to learn representations that are trained to predict the values of subset of important features called {``}pivot features{''} on combined data from the source and target domains. In this work, we show that it is possible to improve on existing neural domain adaptation algorithms by 1) jointly training the representation learner with the task learner; and 2) removing the need for heuristically-selected {``}pivot features.{''} Our results show competitive performance with a simpler model.",
}

@inproceedings{Liu19reinforced,
    title = "Reinforced Training Data Selection for Domain Adaptation",
    author = "Liu, Miaofeng  and
      Song, Yan  and
      Zou, Hongbin  and
      Zhang, Tong",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1189",
    doi = "10.18653/v1/P19-1189",
    pages = "1957--1968",
    abstract = "Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make TDS self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks.",
}
@ARTICLE{Dou19Unsupervised,
       author = {{Dou}, Zi-Yi and {Hu}, Junjie and {Anastasopoulos}, Antonios and
         {Neubig}, Graham},
        title = "{Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2019",
        month = "Aug",
          eid = {arXiv:1908.10430},
        pages = {arXiv:1908.10430},
archivePrefix = {arXiv},
       eprint = {1908.10430},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190810430D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Zhou19synchronous,
    title = "Synchronous Bidirectional Neural Machine Translation",
    author = "Zhou, Long  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1006",
    doi = "10.1162/tacl_a_00256",
    pages = "91--105",
    abstract = "Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional{--}neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese{--}English, WMT14 English{--}German, and WMT18 Russian{--}English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese{--}English and English{--}German translation tasks.",
}

@inproceedings{Bapna19simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}
@inproceedings{Aharoni19massively,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}
@misc{Wang19neural,
    title={Neural Machine Translation with Byte-Level Subwords},
    author={Changhan Wang and Kyunghyun Cho and Jiatao Gu},
    year={2019},
    eprint={1909.03341},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@InProceedings{Welleck19non, title = {Non-Monotonic Sequential Text Generation}, author = {Welleck, Sean and Brantley, Kiant{\'e} and Iii, Hal Daum{\'e} and Cho, Kyunghyun}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {6716--6726}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/welleck19a/welleck19a.pdf}, url = { http://proceedings.mlr.press/v97/welleck19a.html }, abstract = {Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.} }

@inproceedings{Wang19dynamically,
    title = "Dynamically Composing Domain-Data Selection with Clean-Data Selection by {``}Co-Curricular Learning{''} for Neural Machine Translation",
    author = "Wang, Wei  and
      Caswell, Isaac  and
      Chelba, Ciprian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1123",
    doi = "10.18653/v1/P19-1123",
    pages = "1282--1292",
    abstract = "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a {``}co-curricular learning{''} method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the {``}co-curriculum{''}. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",
}

@article{Naveen19massively,
  author    = {Naveen Arivazhagan and
               Ankur Bapna and
               Orhan Firat and
               Dmitry Lepikhin and
               Melvin Johnson and
               Maxim Krikun and
               Mia Xu Chen and
               Yuan Cao and
               George Foster and
               Colin Cherry and
               Wolfgang Macherey and
               Zhifeng Chen and
               Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings
               and Challenges},
  journal   = {CoRR},
  volume    = {abs/1907.05019},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05019},
  archivePrefix = {arXiv},
  eprint    = {1907.05019},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-05019},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Xuan19curriculum,
  author    = {Xuan Zhang and
               Pamela Shapiro and
               Gaurav Kumar and
               Paul McNamee and
               Marine Carpuat and
               Kevin Duh},
  title     = {Curriculum Learning for Domain Adaptation in Neural Machine Translation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {1903--1915},
  year      = {2019},
  crossref  = {DBLP:conf/naacl/2019-1},
  url       = {https://www.aclweb.org/anthology/N19-1189/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/naacl/ZhangSKMCD19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Brian19overcoming,
  author    = {Brian Thompson and
               Jeremy Gwinnup and
               Huda Khayrallah and
               Kevin Duh and
               Philipp Koehn},
  title     = {Overcoming Catastrophic Forgetting During Domain Adaptation of Neural
               Machine Translation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {2062--2068},
  year      = {2019},
  crossref  = {DBLP:conf/naacl/2019-1},
  url       = {https://www.aclweb.org/anthology/N19-1209/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/naacl/ThompsonGKDK19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Conneau19cross,
 author = {CONNEAU, Alexis and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Cross-lingual Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Saunders19domain,
    title = "Domain Adaptive Inference for Neural Machine Translation",
    author = "Saunders, Danielle  and
      Stahlberg, Felix  and
      de Gispert, Adri{\`a}  and
      Byrne, Bill",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1022",
    doi = "10.18653/v1/P19-1022",
    pages = "222--228",
    abstract = "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",
}

@inproceedings{Saunders19domain,
    title = "Domain Adaptive Inference for Neural Machine Translation",
    author = "Saunders, Danielle  and
      Stahlberg, Felix  and
      de Gispert, Adri{\`a}  and
      Byrne, Bill",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1022",
    doi = "10.18653/v1/P19-1022",
    pages = "222--228",
    abstract = "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",
}

@inproceedings{Pham19generic,
	Address = {Hong-Kong, CN},
	Author = {Pham, Minh Quang AND Crego, Josep-Maria AND Senellart, Jean AND Yvon, Fran\c{c}ois},
	Booktitle = {Proceedings of the 16th {International Workshop on Spoken Language Translation}},
	Series = {IWSLT},
	Keywords = {Machine Translation; Domain Adaptation},
	Pages = {9p},
	Title = {{Generic and Specialized Word Embeddings for Multi-Domain Machine Translation}},
	Url = {https://zenodo.org/record/3524959/files/IWSLT2019_paper_10.pdf},
	Year = {2019}}

@ARTICLE{Gu20domain,
       author = {{Gu}, Yu and {Tinn}, Robert and {Cheng}, Hao and {Lucas}, Michael and
         {Usuyama}, Naoto and {Liu}, Xiaodong and {Naumann}, Tristan and
         {Gao}, Jianfeng and {Poon}, Hoifung},
        title = "{Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2020,
        month = jul,
          eid = {arXiv:2007.15779},
        pages = {arXiv:2007.15779},
archivePrefix = {arXiv},
       eprint = {2007.15779},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200715779G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Banon20Paracrawl,
    title = "{P}ara{C}rawl: Web-Scale Acquisition of Parallel Corpora",
    author = "Ba{\~n}{\'o}n, Marta  and
      Chen, Pinzhen  and
      Haddow, Barry  and
      Heafield, Kenneth  and
      Hoang, Hieu  and
      Espl{\`a}-Gomis, Miquel  and
      Forcada, Mikel L.  and
      Kamran, Amir  and
      Kirefu, Faheem  and
      Koehn, Philipp  and
      Ortiz Rojas, Sergio  and
      Pla Sempere, Leopoldo  and
      Ram{\'\i}rez-S{\'a}nchez, Gema  and
      Sarr{\'\i}as, Elsa  and
      Strelec, Marek  and
      Thompson, Brian  and
      Waites, William  and
      Wiggins, Dion  and
      Zaragoza, Jaume",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.417",
    doi = "10.18653/v1/2020.acl-main.417",
    pages = "4555--4567",
    abstract = "We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.",
}

@inproceedings{Muller20domain,
    title = "Domain Robustness in Neural Machine Translation",
    author = {M{\"u}ller, Mathias  and
      Rios, Annette  and
      Sennrich, Rico},
    booktitle = "Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = oct,
    year = "2020",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2020.amta-research.14",
    pages = "151--164",
}

@inproceedings{Li20shallow,
    title = "Shallow-to-Deep Training for Neural Machine Translation",
    author = "Li, Bei  and
      Wang, Ziyang  and
      Liu, Hui  and
      Jiang, Yufan  and
      Du, Quan  and
      Xiao, Tong  and
      Wang, Huizhen  and
      Zhu, Jingbo",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.72",
    doi = "10.18653/v1/2020.emnlp-main.72",
    pages = "995--1005",
    abstract = "Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT{'}16 English-German and WMT{'}14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at \url{https://github.com/libeineu/SDT-Training}.",
}

@inproceedings{Aharoni20unsupervised,
    title = "Unsupervised Domain Clusters in Pretrained Language Models",
    author = "Aharoni, Roee  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.692",
    doi = "10.18653/v1/2020.acl-main.692",
    pages = "7747--7763",
    abstract = "The notion of {``}in-domain data{''} in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision {--} suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.",
}

@inproceedings{Brown20language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{Pham20Study,
    title = "A Study of Residual Adapters for Multi-Domain Neural Machine Translation",
    author = "Pham, Minh Quang  and
      Crego, Josep Maria  and
      Yvon, Fran{\c{c}}ois  and
      Senellart, Jean",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wmt-1.72",
    pages = "617--628",
    abstract = "Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.",
}

@inproceedings{Aharoni20Unsupervised,
    title = "Unsupervised Domain Clusters in Pretrained Language Models",
    author = "Aharoni, Roee  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.692",
    doi = "10.18653/v1/2020.acl-main.692",
    pages = "7747--7763",
    abstract = "The notion of {``}in-domain data{''} in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision {--} suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.",
}

@inproceedings{Jiang20Multi,
    title = "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing",
    author = "Jiang, Haoming  and
      Liang, Chen  and
      Wang, Chong  and
      Zhao, Tuo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.165",
    doi = "10.18653/v1/2020.acl-main.165",
    pages = "1823--1834",
    abstract = "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
}

@inproceedings{Pham20Priming,
    title = "Priming Neural Machine Translation",
    author = "Pham, Minh Quang  and
      Xu, Jitao  and
      Crego, Josep  and
      Yvon, Fran{\c{c}}ois  and
      Senellart, Jean",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wmt-1.63",
    pages = "516--527",
    abstract = "Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.",
}

@InProceedings{Zhang20Pegasus,
  title =        {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  author =       {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle =    {Proceedings of the 37th International Conference on Machine Learning},
  pages =        {11328--11339},
  year =         {2020},
  editor =       {Hal Daumé III and Aarti Singh},
  volume =       {119},
  series =       {Proceedings of Machine Learning Research},
  month =        {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf},
  url =          {
http://proceedings.mlr.press/v119/zhang20ae.html
},
  abstract =     {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.}
}

@article{Saunders21Asurvey,
  author    = {Danielle Saunders},
  title     = {Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2104.06951},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.06951},
  archivePrefix = {arXiv},
  eprint    = {2104.06951},
}

@PhdThesis{Saunders21Domain,
  author = 	 {Danielle Saunders},
  title = 	 {Domain Adaptation for Neural Machine Translation},
  school = 	 {Department of Engineering, University of Cambridge},
  year = 	 2021}

@article{Pham21Revisiting,
      author = {Minh Quang Pham and Josep Crego and François Yvon},
      title = {Revisiting Multi-Domain Machine Translation},
      journal = {Transactions of the Association for Computational Linguistics},
      volume = {9},
      number = {0},
      year = {2021},
      keywords = {},
      abstract = {When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work, that fall under the general umbrella of transfer learning. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.},
     issn = {2307-387X},	pages = {17--35},
     url = {https://transacl.org/index.php/tacl/article/view/2327}
}

@article{Stergiadis21Multi,
  title={Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging},
  author={Emmanouil Stergiadis and Satendra Kumar and Fedor Kovalev and Pavel Levin},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.10160}
}















