Neural machine translation (NMT) systems have been the state-of-the-art system in machine translation for a long time. However, such translation models require a relatively large amount of training data but struggle to handle a specific domain text. A domain may consist of text from a particular topic or text for a specific purpose. While NMT systems can be adapted for better translation quality in a target domain via its representative training corpus, this technique has adverse side-effects, including brittleness against out-of-domain examples and "catastrophic forgetting" of previous domains in training data. Moreover, one translation system has to scope with many possible domains in real applications, making the "one domain one model" impractical. A promising solution is to build a multi-domain NMT system trained from many possible domains and adapted to multiple target domains. The rationale of this direction is twofold. First, large training corpora improve the generalization of the NMT system. Secondly, the data of a domain can be valuable for adapting an NMT model to a similar domain.
This thesis first unifies domain adaptation and multi-domain adaptation in one mathematical framework. In addition, we review the literature in (multi-)domain adaptation through a structural approach by pointing out four principal cases and matching previous methods to each application case. Secondly, we propose a novel multi-criteria evaluation of multi-domain approaches. We point out the requirements for a multi-domain system and perform an exhaustive comparison of a large set of methods. We propose different model-centric approaches, including sparse word embedding and gated residual adapter, which are cheap and able to handle many domains. To balance the heterogeneity in the training data, we explore techniques relating to dynamic data sampling, which iteratively adapt the training distribution to a pre-determined testing distribution. Finally, we are interested in context augmented translation approaches, which reuse similar translation memories to improve the prediction of a sentence. These methods are suitable for adapting our NMT system to an unknown domain.
