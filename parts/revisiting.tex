\chapter{Revisiting multi-domain machine translation}
\textit{This chapter draws from the following publications: \citet{Pham21revisiting}}
\section{Motivation}
In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. We define in a more precise fashion the requirements that an effective MDMT system should meet (Section~\ref{sec:requirements}). Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behavior of multi-domain systems and to make them fully hold their promises.

Data-based Machine Translation (MT), whether statistical or neural, rests on well understood machine learning principles. Given a training sample of matched source-target sentence pairs $(\src,\trg)$ drawn from an underlying distribution $\mathcal{D}_s$, a model parameterized by $\theta$ (here, a translation function $h_{\theta}$) is trained by minimizing the empirical expectation of a loss function $\ell(h_\theta(\src), \trg)$. This approach ensures that the translation loss remains low when translating more sentences drawn from the same distribution.

Owing to the great variability of language data, this ideal situation is rarely met in practice, warranting the study of an alternative scenario, where the test distribution $\mathcal{D}_t$ differs from $\mathcal{D}_s$. In this setting, \emph{domain adaptation} (DA) methods are in order. DA has a long history in Machine Learning in general (e.g.\ \cite{Shimodaira00improving,Ben10A,Quinonero08dataset,Pan10asurvey}) and in NLP in particular (eg.\ \cite{Daume06domain,Blitzer07domain,Jiang07instance}). Various techniques thus exist to handle both the situations where a (small) training sample drawn from $\mathcal{D}_t$ is available in training, or where only samples of source-side (or target-side) sentences are available (see \cite{Foster07mixture,Bertoldi09domain,Axelrod11domain} for proposals from the statistical MT era, or \cite{Chu18asurvey} for a recent survey of DA for Neural MT).

A seemingly related problem is \emph{multi-domain} (MD) machine translation \cite{Sajjad17neural,Farajian17multidomain,Kobus17domain,Zeng18multidomain,Pham19generic} where one single system is trained and tested with data from multiple domains. MD machine translation (MDMT) corresponds to a very common situation, where all available data, no matter its origin, is used to train a robust system that performs well for any kind of new input. If the intuitions behind MDMT are quite simple, the exact specifications of MDMT systems are rarely spelled out: for instance, should MDMT perform well when the test data is distributed like the training data, when it is equally distributed across domains or when the test distribution is unknown? Should MDMT also be robust to new domains? How should it handle domain labeling errors? 

A related question concerns the relationship between supervised domain adaptation and multi-domain translation. The latter task seems more challenging \revisiondone{as it tries to optimize MT performance for a more diverse set of potential inputs, with an additional uncertainty regarding the distribution of test data.} Are there still situations where MD systems can surpass single domain adaptation, as is sometimes expected?   

Our first contribution is thus of methodological nature and consists of lists of expected properties of MDMT systems and associated measurements to evaluate them (Section~\ref{sec:challenging}). In doing so, we also shed light on new problems that arise in this context, regarding for instance the accommodation of new domains in the course of training, or the computation of automatic domain tags. Our second main contribution is experimental and consists in a thorough reanalysis of eight recent multi-domain approaches from the literature, including a variant of a model initially introduced for DA. We show in Section~\ref{sec:experiments} that existing approaches still fall short to match many of these requirements, notably with respect to the handling of a large amount of heterogeneous domains and to dynamically integrating new domains in training.
 
\section{Requirements of multi-domain MT \label{sec:requirements}}
In this section, we recap the main reasons for considering a multi-domain scenario and discuss their implications in terms of performance evaluation.

\subsection{Formalizing multi-domain translation \label{ssec:formalization}}

We conventionally define a domain $d$ as a distribution $\mathcal{D}_d(x)$ over some feature space $\mathcal{X}$ that is shared across domains \cite{Pan10asurvey}: in machine translation, $\mathcal{X}$ is the representation space for source sentences; each domain corresponds to a specific source of data, and differs from the other data sources in terms of textual genre, thematic content \cite{Chen16guided,Zhang16topicinformed}, register \cite{Sennrich16politeness}, style \cite{Niu18multitask}, etc. Translation in domain $d$ is formalized by a translation function $h_d(y|x)$ pairing sentences in a source language with sentences in a target language $y \in \mathcal{Y}$. $h_d$ is usually assumed to be deterministic (hence $y = h_d(x)$), but can differ from one domain to the other.

A typical learning scenario in MT is to have access to samples from $n_d$ domains, which means that the training distribution $\mathcal{D}^s$ is a mixture $\mathcal{D}^s(x) = \sum_d \lambda^{s}_{d} \mathcal{D}_d(x)$\revisiondone{, with $\{\lambda^{s}_d, d=1 \dots n_d\}$ the corresponding mixture weights ($\sum_d \lambda^{s}_d=1$)}. Multi-domain learning, as defined in \citet{Dredze08online} further assumes that domain tags are also available in testing; the implication being that the test distribution is also as a mixture $\mathcal{D}^t(x) = \sum_d \lambda^{t}_{d} \mathcal{D}_d(x)$ of several domains, making the problem distinct from mere domain adaption. A multi-domain learner is then expected to use these tags effectively \cite{Joshi12multidomain} when computing the combined translation function $h(x,d)$, and to perform well in all domains \cite{Finkel09hierarchical}. This setting is closely related to the multi-source adaptation problem formalized in \cite{Mansour09domain,Mansour09multiple,Hoffman18algorithms}.

This definition seems to be the most accepted view of a multi-domain MT\footnote{An exception is \citep{Farajian17multidomain}, where test translations rely on similarity scores between test and train sentences, rather than on domain labels.} and one that we also adopt here. Note that in the absence of further specification, the naive answer to the MD setting should be to estimate one translation function $\hat{h}_d(x)$ separately for each domain, then to translate using $\hat{h}(x,d) = \sum_{d'} h_{d'}(x) \indic{d' = d}$, where $\indic{x}$ is the indicator function. We now discuss the arguments that are put forward to proceed differently.

\subsection{Reasons for building MDMT systems \label{ssec:whymdmt}}

A first motivation for moving away from the one-domain / one-system solution are practical  \cite{Sennrich13multidomain,Farajian17neural}: when faced with inputs that are potentially from multiple domains, it is easier and computationally cheaper to develop one single system instead of having to optimize and maintain multiple engines. The underlying assumption here is that the number of domains of interests can be large, a limiting scenario being fully personalized machine translation \cite{Michel18extreme}.

A second line of reasoning rests on linguistic properties of the translation function and contends that domain specificities are mostly expressed lexically and will primarily affect content words or multi-word expressions; function words, on the other hand, are domain agnostic and tend to remain semantically stable across domains, motivating some cross-domain parameter sharing. An MDMT system should simultaneously learn lexical domain peculiarities, and leverage cross-domain similarities to improve the translation of generic contexts and words \cite{Zeng18multidomain,Pham19generic}. It is here expected that the MDMT scenario should be more profitable when the domain mix includes domains that are closely related and can share more information.

A third series of motivations are of statistical nature. The training data available for each domain is usually unevenly distributed, and domain-specific systems trained or adapted on small datasets are likely to have a high variance and generalize poorly. For some test domains, there may even be no data at all \cite{Farajian17neural}. Training mix-domain systems is likely to reduce this variance, at the expense of a larger statistical bias \cite{Clark12onesystem}. Under this view, MDMT would be especially beneficial for domains with little training data. This is observed for multilingual MT from English: an improvement for under-resourced languages due to positive transfer, at the cost of a decrease in performance for well-resourced languages \cite{Arivazhagan19massively}.

Combining multiple domain-specific MTs can also be justified in the sake of distributional robustness \cite{Mansour09domain,Mansour09multiple}, for instance when the test mixture differs from the train mixture, or when it includes new domains unseen in training.
An even more challenging case is when the MT would need to perform well for any test distribution, as studied for statistical MT in \cite{Huck15mixeddomain}. In all these cases, mixing domains in training and/or testing is likely to improve robustness against unexpected or adversarial test distribution \cite{Oren19distributionally}.

A distinct line of reasoning is that mixing domains can have a positive regularization effect for all domains. By introducing variability in training, it prevents DA from overfitting the available adaptation data and could help improve generalization even for well-resourced domains. A related case is made in \cite{Joshi12multidomain}, which shows that part of the benefits of MD training is due to an ensembling effect, where systems from multiple domains are simultaneously used in the prediction phase; this effect may subsist even in the absence of clear domain separations.

To recap, there are multiple arguments for adopting MDMT, some already used in DA settings, and some original. These arguments are not mutually exclusive; however each yields specific expectations with respect to the performance of this approach, and should also yield appropriate evaluation procedure. If the motivation is primarily computational, then a drop in MT quality with respect to multiple individual domains might be acceptable if compensated by the computational savings. If it is to improve statistical estimation, then the hope will be that MDMT will improve, at least for some under-resourced domains, over individually trained systems. If finally, it is to make the system more robust to unexpected or adversarial test distributions, then this is the setting that should be used to evaluate MDMT. The next section discusses ways in which these requirements of MDMT systems could be challenged. 

\section{Challenging multi-domain systems \label{sec:challenging}}
In this section, we propose seven operational requirements that can be expected from an effective multi-domain system, and discuss ways to evaluate whether these requirements are actually met. All these evaluations will rest on comparison of translation performance, and do not depend on the choice of a particular metric. To make our results comparable with the literature, we will only use the BLEU score \cite{Papineni02bleu} in Section~\ref{sec:experiments}, noting it may not be the best yardstick to assess subtle improvements of lexical choices that are often associated with domain adapted systems \cite{Irvine13measuring}. Other important figures of merit for MDMT systems are the computational training cost and the total number of parameters.

\subsection{Multi-domain systems should be effective \label{ssec:effective}}
A first expectation is that MDMT systems should perform well in the face of mixed-domain test data. We thus derive the following requirements.

\paragraph{[P1-LAB]} A MDMT should perform better than the baseline which disregards domain labels, or reassigns them in a random fashion \cite{Joshi12multidomain}. Evaluating this requirement is a matter of a mere comparison, assuming the test distribution of domains is known: if all domains are equally important, performance averages can be reported; if they are not, weighted averages should be used instead.

\paragraph{[P2-TUN]} Additionally, one can expect that MDMT will improve over fine-tuning \cite{Luong15stanford,Freitag16fast}, at least in domains where data is scarce, or in situations where several domains are close. To evaluate this, we perform two measurements, using a real as well as an artificial scenario. In the real scenario, we simply compare the performance of MDMT and fine-tuning for domains of varying sizes, expecting a larger gain for smaller domains. In the artificial scenario, we split a single domain in two parts which are considered as distinct in training. The expectation here is that a MDMT should yield a clear gain for both pseudo sub-domains, which should benefit from the supplementary amount of relevant training. In this situation, MDMT should even outperform fine-tuning on either of the pseudo sub-domain.

\subsection{Robustness to fuzzy domain separation \label{ssec:robusness}}
A second set of requirements is related to the definition of a domain. As repeatedly pointed out in the literature, parallel corpora in MT are often collected opportunistically and the view that each corpus constitutes a single domain is often a gross approximation.\footnote{Two of our own ``domains'' actually comprise several subcorpora (IT and MED), see details in Section~\ref{ssec:corpora}.} MDMT should aim to make the best of the available data and be robust to domain assignments. To challenge these requirements we propose evaluating the following requirements.

\paragraph{[P3-HET]}
The notion of a domain being a fragile one, an effective MDMT system should be able to discover not only when cross-domain sharing is useful (cf.\ requirement [P2-TUN]), but also when intra-domain heterogeneity is hurting. This requirement is tested by artificially conjoining separate domains into one during training, hoping that the loss in performance with respect to the baseline (using correct domain tags) will remain small.

\paragraph{[P4-ERR]}
MDMTs should perform best when the true domain tag is known, but deteriorate gracefully in the face of tag errors; in this situation, catastrophic drops in performance are often observed. This requirement can be assessed by translating test texts with erroneous domain tags and reporting the subsequent loss in performance.

\paragraph{[P5-UNK]}
A related situation occurs when the domain of a test document is unknown. Several situations need be considered: for domains seen in training, using automatically predicted domain labels should not be much worse than using the correct one. For test documents from unknown domains (zero-shot transfer), a good MD system should ideally outperform the default baseline that merges all available data.

\paragraph{[P6-DYN]}
Another requirement, more of an operational nature, is that an MDMT system should smoothly evolve to handle a growing number of domains, without having to retrain the full system each time new data is available. This is a requirement [P6-DYN] that we challenge by dynamically changing the number of training and test domains.

\subsection{Scaling to a large number of domains \label{ssec:scaling}}

\paragraph{[P7-NUM]} As mentioned above, MDMT systems have often been motivated by computational arguments. This argument is all the more sensible as the number of domains increases, making the optimization of many individual systems both ineffective and undesirable. For lack of having access to corpora containing very large sets (eg.\ in the order of 100-1000) domains, we experiment with automatically learned domains.\fyFuture{considering a varying number of clusters.}

\section{Experimental settings \label{sec:experiments}}

\subsection{Data and metrics \label{ssec:corpora}}

We experiment with translation from English into French and use texts initially originating from 6~domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 (\domain{med})\footnote{\url{https://ufal.mff.cuni.cz/ufal_medical_corpus}. \revisiondone{We only use the in-domain (medical) subcorpora: PATR, EMEA, CESTA, ECDC.}}, the European Central Bank corpus (\domain{bank}) \cite{Tiedemann12parallel}; The JRC-Acquis Communautaire corpus (\domain{law}) \cite{Steinberger06acquis}, documentations for KDE, Ubuntu, GNOME and PHP from Opus collection \cite{Tiedemann09news}, collectively merged in a \domain{it}-domain, Ted Talks (\domain{talk}) \cite{Cettolo12wit}, and the Koran (\domain{rel}). Complementary experiments also use v12 of the News Commentary corpus (\domain{news}). Most corpora are available from the Opus web site.\footnote{\url{http://opus.nlpl.eu}} These corpora were deduplicated and tokenized with in-house tools; statistics are in Table~\ref{tab:Corpora}. To reduce the number of types and build open-vocabulary systems, we use Byte-Pair Encoding \cite{Sennrich16neural} with 30,000 merge operations on a corpus containing all sentences in both languages.

We randomly select in each corpus a development and a test set of 1,000 lines and keep the rest for training.\footnote{The code for reproducing our train, dev and test datasets is available at \url{https://github.com/qmpham/experiments}.} Validation sets are used to chose the best model according to the average BLEU score \cite{Papineni02bleu}.\footnote{We use truecasing and the \texttt{multibleu} script.} Significance testing is performed using bootstrap resampling \cite{Koehn04statistical}, implemented in compare-mt\footnote{\url{https://github.com/neulab/compare-mt}} \cite{Neubig19compare-mt}. We report significant differences at the level of $p=0.05$.

\begin{table*}[htbp]
  \centering
  \begin{tabular}{|l|ccccccc|} %*{4}{|r|}}
    \cline{2-8} 
    %\multicolumn{4}{|l|}{Vocab size - En: 30,165, Fr: 30,398}\\
    \multicolumn{1}{c|}{} & \multicolumn{1}{c}{\domain{med}} & \multicolumn{1}{c}{\domain{law}} & \multicolumn{1}{c}{\domain{bank}} & \multicolumn{1}{c}{\domain{it}} & \multicolumn{1}{c}{\domain{talk}} & \multicolumn{1}{c}{\domain{rel}} & \multicolumn{1}{c|}{\domain{news}} \\
    \hline 
    \# lines & 2609 (0.68) & 501 (0.13) & 190 (0.05) & 270 (0.07) & 160 (0.04) & 130 (0.03) & 260 (0) \\
    \# \revisiondone{tokens}  &  133 / 154  &  17.1 / 19.6 &  6.3 / 7.3 &  3.6 / 4.6 &  3.6 / 4.0 &  3.2 / 3.4 & 7.8 / 9.2   \\
    \# \revisiondone{types}  & 771 / 720 & 52.7 / 63.1 & 92.3 / 94.7 & 75.8 / 91.4 & 61.5 / 73.3 & 22.4 / 10.5 & - \\
    \# \revisiondone{uniq} & 700 / 640 & 20.2 / 23.7 & 42.9 / 40.1 & 44.7 / 55.7 & 20.7 / 25.6 & 7.1 / 2.1 & - \\
    \hline
  \end{tabular}
  \caption{Corpora statistics: number of parallel lines ($\times 10^3$) and proportion in the basic domain mixture (which does not include the \domain{news} domain), number of tokens in English and French ($\times 10^6$), number of types in English and French ($\times 10^3$), number of types that only appear in a given domain ($\times 10^3$). \domain{med} is the largest domain, containing almost 70\% of the sentences, while \domain{rel} is the smallest, with only 3\% of the data.
  }
\label{tab:Corpora}
\end{table*}

We measure the distance between domains using the $\mathcal{H}$-Divergence \cite{Ben10A}, which relates domain similarity to the test error of a domain discriminator: the larger the error, the closer the domains.
Our discriminator is a SVM independently trained for each pair of domains, with sentence representations derived via mean pooling from the source side representation of the generic Transformer model. We used the scikit-learn\footnote{\url{https://scikit-learn.org}} implementation with default values. Results in Table~\ref{tab:domaindist} show that all domains are well separated from all others, with \domain{rel} being the furthest apart, while \domain{talk} is slightly more central.

\begin{table}\centering
  \begin{tabular}{|l*{5}{|r}|} 
  \cline{2-6}
  \multicolumn{1}{c|}{} & \domain{law} & \domain{bank} & \domain{talk} & \domain{IT} & \domain{rel} \\ \hline
    \domain{med} &1.93 &1.97 &1.9 &1.93 &1.97 \\
    \domain{law}   && 1.94 & 1.97 &1.93 & 1.99 \\
    \domain{bank} &&&1.98 &1.94 &1.99 \\
    \domain{talk}   &&&&1.92 &1.93 \\
     \domain{IT}     &&&&& 1.99 \\ \hline
  \end{tabular}
  \caption{The $\mathcal{H}$-divergence between domains}
  \label{tab:domaindist}
\end{table}

\subsection{Baselines \label{ssec:baselines}}

Our baselines are standard for multi-domain systems.\footnote{We however omit domain-specific systems trained only with the corresponding subset of the data, which are always inferior to the mix-domain strategy \cite{Britz17effective}.} Using Transformers \cite{Vaswani17attention} implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \cite{Klein17opennmt}, we build the following systems:

\begin{itemize}
\item a generic model trained on a concatenation of all corpora (\texttt{Mixed}). We develop two versions\footnote{In fact three: to enable a fair comparison with WDCMT, a RNN-based variant is also trained and evaluated. \revisiondone{This system appears as \system{Mixed-Nat-RNN} in Table~\ref{tab:performance}}.} of this system, one where the domain unbalance reflects the distribution of our training data \revisiondone{given in Table~\ref{tab:Corpora}} (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}). The former is the best option when the train mixture $\mathcal{D}^s$ is also expected in testing; the latter should be used when the test distribution is uniform across domains. Accordingly, we report two aggregate scores: a weighted average reflecting the training distribution, and an unweighted average, meaning that test domains are equally important.
\item fine-tuned models \cite{Luong15stanford,Freitag16fast}, based on the \system{Mixed-Nat} system, further trained on each domain for at most 20~000 iterations, with early stopping when the dev BLEU stops increasing. The full fine-tuning (\system{FT-Full}) procedure may update all the parameters of the initial generic model, resulting in six systems adapted for one domain, with no parameter sharing across domains.
\end{itemize}

All models use embeddings and the hidden layers sizes of dimension~512. Transformers contain with 8 attention heads in each of the 6+6 layers; the inner feedforward layer contains 2048 cells. The adapter-based systems (see below) additionally use an adaptation block in each layer, composed of a 2-layer perceptron, with an inner $\operatorname{ReLU}$ activation function operating on normalized entries of dimension~1024. 
Training uses batches of~12,288 tokens, Adam with parameters $\beta_1=0.9$, $\beta_2= 0.98$, Noam decay ($warmup\_steps=4000$), and a dropout rate of $0.1$ in all layers.

\subsection{Multi-domain systems \label{ssec:systems}}

Our comparison of multi-domain systems includes our own reimplementations of recent proposals from the literature:\footnote{\revisiondone{Further implementation details are in Appendix~A.}}
\begin{itemize}
\item a system using domain control as in \cite{Kobus17domain}: domain information is introduced either as an additional token for each source sentence (\system{DC-Tag}), or as a supplementary feature for each word (\system{DC-Feat}).
\item a system using lexicalized domain representations \cite{Pham19generic}: word embeddings are composed of a generic and a domain specific part (\system{LDR});
\item the three proposals of \cite{Britz17effective}. \system{TTM} is a feature-based approach where the domain tag is introduced as an extra word \textsl{on the target side}. Training uses reference tags and inference is usually performed with predicted tags, just like for regular target words. \system{DM} is a multi-task learner where a domain classifier is trained on top the MT encoder, so as to make it aware of domain differences; \system{ADM} is the adversarial version of \system{DM}, pushing the encoder towards learning domain-independent source representations. These methods thus only use domain tags in training.
\item the multi-domain model of \cite{Zeng18multidomain} (\system{WDCMT}), where a domain-agnostic and a domain-specialized representation of the input are simultaneously processed; supervised classification and adversarial training are used to compute these representations. \revisiondone{Again, inference does not use domain tags.}\footnote{For this system, we use the available RNN-based system from the authors (\url{https://github.com/DeepLearnXMU/WDCNMT}) which does not directly compare to the other, Transformer-based, systems; the improved version of \cite{Su19exploring} seems to produce comparable, albeit slightly improved, results.}
\item \revisiondone{two multi-domain versions of the approach of \cite{Bapna19simple}, denoted \system{FT-Res} and \system{MDL-Res}, where a domain-specific adaptation module is added to all the Transformer layers; within each layer, residual connections enable to short-cut this adapter. The former variant corresponds to the original proposal of \citet{Bapna19simple} \revisiondone{(see also \cite{Sharaf20metalearning})}. It fine-tunes the adapter modules of a \system{Mixed-Nat} system independently for each domain, keeping all the other parameters frozen. The latter uses the same architecture, but a different training procedure and learns all parameters jointly from scratch with a mix-domain corpus.}
  
\end{itemize}
This list includes systems that slightly depart from our definition of MDMT: standard implementations of \system{TTM} and \system{WDCMT} rely on infered, rather than on gold, domain tags - which must somewhat affect their predictions; \system{DM} and \system{ADM} make no use of domain tags at all. We however did not consider the proposal of \cite{Farajian17multidomain} which performs on-the-fly tuning for each test sentence and diverges more strongly from our notion of MDMT.

\section{Results and discussion \label{sec:results}}

\subsection{Performance of MDMT systems \label{ssec:rawperformance}}

\begin{table*}
  \centering
  \begin{tabular}{|p{4cm}|*{8}{r|}} \hline
%     &&&&&& \\
    Model / Domain & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline % & \multicolumn{1}{c|}{\domain{news}} 
    \system{Mixed-Nat}  \hfill{\footnotesize[65m]} & 37.3 & 54.6 & 50.1 & 33.5 & 43.2 & 77.5  & 41.1  & 49.4 \\% & 23.5\\
    \system{Mixed-Bal}   \hfill{\footnotesize[65m]} &  35.3 & 54.1 & 52.5 & 31.9 & 44.9 & 89.5 & 40.3  & 51.4 \\ %& \\
    \system{FT-Full}       \hfill{\footnotesize[6$\times$65m]} & 37.7 & \SB{59.2} & \SB{54.5} & 34.0 & \SB{46.8} & \SB{90.8}   & \SB{42.7} & \SB{53.8} \\ \hline
 %   Full-finetuned on extended in-domain corpora (news) & && 33.96&&& & &\\nn
    \system{DC-Tag} \hfill{\footnotesize[+4k]}        & 38.1 & 55.3 & 49.9   & 33.2 & 43.5 & \SB{80.5} &41.6 & 50.1    \\%    & 21.8 \\
    \system{DC-Feat} \hfill{\footnotesize[+140k]}    & 37.7  & 54.9 & 49.5   & 32.9 & 43.6 & \SB{79.9} &41.4 & 49.9   \\% & \SW{21.7} \\
    \system{LDR}       \hfill{\footnotesize[+1.4m]}    & 37.0   & 54.7 & 49.9 & 33.9 & 43.6 & \SB{79.9} &40.9 & 49.8          \\% & 22.1 \\ 
    \system{TTM}      \hfill{\footnotesize[+4k]}        & 37.3 & 54.9 & 49.5 & 32.9 & 43.6 & \SB{79.9} &41.0 & 49.7     \\% &  23.4 \\
    \system{DM}        \hfill{\footnotesize[+0]}         & \SW{35.6} & \SW{49.5}  & \SW{45.6}& \SW{29.9} & \SW{37.1} & \SW{62.4} & 38.1 & 43.4 \\ % & 22.6\\
    \system{ADM}      \hfill{\footnotesize[+0]}         & 36.4 & \SW{53.5}  & \SW{48.3} & \SW{32.0} & \SW{41.5} & \SW{73.4} & 38.9 & 47.5 \\% & 23.3 \\
    \revisiondone{\system{FT-Res}}   \hfill{\footnotesize[+12.4m]}  & 37.3 & \SB{57.9} & \SB{53.9} & 33.8 & \SB{46.7} & \SB{90.2}  & \SB{42.3} & \SB{53.3} \\ % & 20.5\\ \hline
    \system{MDL-Res} \hfill{\footnotesize[+12.4m]}    & 37.9 & \SB{56.0}  & \SB{51.2}   & 33.5   &  44.4  & \SB{88.3} & 42.0 & \SB{51.9} \\%  & \SW{21.2} \\
%     \hfill MDL Res (gen)    & 37.7 & 51.0 & 34.0 & 30.4 & 34.2 & 15.2 & 36.4 & 33.7\\
%    \system{MDL Gated} & 37.7 & 56.5 & 53.2 & 34.1 & 44.6 & 90.7 & 42.3 & 53.3&\\
     \hline \hline
    \system{Mixed-Nat-RNN} \hfill{\footnotesize[51m]}  & 36.8 & 53.8 & 47.2 & 30.0 & 35.7 & 60.2  & 39.2  & 44.0 \\
    \hline
    \system{WDCMT}  \hfill{\footnotesize[73m]} & 36.0 & 53.3 & \SB{48.8} & 31.1 & \SB{38.8} & \SW{58.5} & 39.0 & 44.4 \\ % & 20.4 \\
    \hline
  \end{tabular}
  \caption{Translation performance of MDMT systems \revisiondone{based on the same Transformer (top) or RNN (bottom) architecture. The former contains 65m parameters, the latter has 51m. For each system, we report the number of additional domain specific parameters,}  BLEU scores for each domain, domain-weighted (w\domain{avg}) and unweighted (\domain{avg}) averages. \revisiondone{For weighted-averages, we take the domain proportions from Table~\ref{tab:Corpora}}. Boldface denotes significant gains with respect to \system{Mix-Nat} (or \system{Mix-Nat-RNN}, for WDCMT), underline denotes significant losses.}
  \label{tab:performance}
\end{table*}

In this section, we discuss the basic performance of MDMT systems trained and tested on $6$~domains. Results are in Table~\ref{tab:performance}. As expected, balancing data in the generic setting makes a great difference (the unweighted average is 2~BLEU points better, notably owing to the much better results for \domain{rel}). As explained above, this setting should be the baseline when the test distribution is assumed to be balanced across domains. As all other systems are trained with an unbalanced data distribution, we use the weighted average to perform global comparisons.

Fine-tuning each domain separately yields a better baseline, outperforming \system{Mixed-Nat} for all domains, with significant gains for domains that are distant from \domain{med}: \domain{rel}, \domain{it}, \domain{bank}, \domain{law}.

All MDMTs (except \system{DM} and \system{ADM}) slightly improve over \system{Mixed-Nat}(for most domains), but these gains are rarely significant. \revisiondone{Among systems using an extra domain feature, \system{DC-Tag} has a small edge over \system{DC-Feat} and also requires less parameters; it also outperforms \system{TTM}, which however uses predicted rather than gold domain tags. \system{TTM} is also the best choice among the systems that do not use domain tags in inference.} \revisiondone{The best contenders overall are  \system{FT-Res} and \system{MDL-Res}, which significantly improve over \system{Mixed-Nat} for a majority of domains, and are the only ones to clearly fulfill [P1-LAB];} \system{WDCMT} also improves on three domains, but regresses on one. The use of a dedicated adaptation module thus seems better than feature-based strategies, \revisiondone{but yields a large increase of the number of parameters.} The effect of the adaptation layer is especially significant for small domains (\domain{bank}, \domain{it} and \domain{rel}).

All systems fail to outperform fine-tuning, sometimes by a wide margin, especially for an ``isolated'' domain like \domain{rel}. This might be due to the fact that domains are well separated (cf.\ Section~\ref{ssec:corpora}) and are hardly helping each other. In this situation, MDMT systems should dedicate a sufficient number of parameters to each domain, so as to close the gap with fine-tuning.

\subsection{Redefining domains \label{ssec:redomains}}

Table~\ref{tab:redomains} summarizes the results of four experiments where we artificially redefine the boundaries of domains, with the aim to challenge requirements [P2-TUN], [P3-HET], and [P4-ERR]. In first three, we randomly \emph{split} one corpus in two parts and proceed as if this corresponded to two actual domains. A MD system should detect that these two pseudo-domains are mutually beneficial and should hardly be affected by this change with respect to the baseline scenario (no split). In this situation, we expect MDMT to even surpass fine-tuning separately on each of these dummy domains, as MDMT exploits all data, while fine-tuning focuses only on a subpart. In testing, we decode the test set twice, once with each pseudo-domain tag. This makes no difference for \system{TTM}, \system{DM}, \system{ADM} and \system{WDCMT}, which do not use domain tags in testing.
In the \textsl{merge} experiment, we merge two corpora in training, in order to assess the robustness with respect to heterogenous domains [P3-HET]. We then translate the two corresponding tests with the same (merged) system.

\begin{table*}
  \centering% \small
  \begin{tabular}{|p{1.8cm}|*{10}{r|}} \hline
    \hfill Set-up & \multicolumn{2}{c|}{Split} &  \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Merge} & \multicolumn{2}{c|}{Wrong} \\ % \hline
     Model \hfill & \multicolumn{2}{c|}{\domain{med} \footnotesize{(0.5 / 0.5)}} &  \multicolumn{2}{c|}{\domain{med} {\footnotesize (0.25 / 0.75)}} & \multicolumn{2}{c|}{\domain{law} {\footnotesize (0.5 / 0.5)}} & \multicolumn{2}{c|}{\domain{bank}+\domain{law}} &  \multicolumn{1}{c|}{\domain{rnd}} &  \multicolumn{1}{c|}{\domain{new}}\\ \hline
    & \domain{med}$_1$ & \domain{med}$_2$ & \domain{med}$_1$ & \domain{med}$_2$ &  \domain{law}$_1$ & \domain{law}$_2$ & \domain{bank} & \domain{law}   & \domain{all} & \domain{News} \\
    \system{FT-Full}      & -0.1 & -0.6 & \SW{-1.5} & -0.2& \SW{-2.3} & \SW{-5.1} &\SW{-1.6} & \SW{-1.4}& \SW{-19.6} & \SW{-3.3}\\% [32.5]
    \system{DC-Tag}     & -0.2 & -0.3& +\SB{0.1}  & +0.2& -0.4 & -0.4 & -0.5 & -0.4 & \SW{-13.4} & \SW{-1.7}\\% [35.9]
    \system{DC-Feat}    & -0.5 & 0.0 & +\SB{0.3}   & +0.3 & +0.3 & +0.3 & +0.3 & +0.1 & \SW{-14.2} &\SW{-1.8}\\ % [34.9] 
    \system{LDR}           & +0.1 & +0.1 & +0.4 & +0.4 & 0.0 &  0.0 &  0.0 & +0.1& \SW{-12.0} & \SW{-1.4}\\ % [37.0]
    \system{TTM} (*)        & -0.2 &  -0.2 & -0.2 & -0.2 & -0.3 &-0.3 &  0.0 & -0.3 & 0.0 & -0.1\\
    \system{DM} (*)           & -0.3   & -0.3  & +0.4 & +0.4 & +0.3 & +0.3 & +0.9 & +0.1 & 0.0 &-0.9\\
    \system{ADM} (*)        & +0.6   & +0.6 & +0.4 & +0.4 & +0.4 & +0.4 &  +0.1 & -0.4 & 0.0&-0.2\\
    \revisiondone{\system{FT-Res}}   & -0.1   & -0.4 & -0.3 &-0.3 & \SW{-2.2} & \SW{-2.9} & \SW{-2.4} & -\SW{3.2} & \SW{-13.3} & \SW{-3.0}\\ % [31.8]
    \system{MDL-Res}   & -0.2   & -0.1 & +\SB{0.2} &+0.0 & -0.9 & -0.9 & +0.7 & -0.3 & \SW{-18.6} & \SW{-1.3}\\ % [31.8]
    \system{WDCMT} (*)     & -0.0    & -0.0  & +0.2 & +0.2  & +0.8 & +0.8  & -0.4 & -0.8 & 0.0 & +0.2 \\
    \hline
  \end{tabular}
  \caption{Translation performance with variable domain definitions. In the Split/Merge experiments, we report BLEU differences for the related test set(s). Underline denotes significant loss when domains are changed wrt.\ the baseline situation; bold for a significant improvement over \system{FT-Full}; (*) tags systems ignoring test domains.
  }
  \label{tab:redomains}
\end{table*}

Our findings can be summarized as follows. For the split experiments, we see small variations that can be positive or negative compared to the baseline situation, but these are hardly significant. All systems show some robustness with respect to fuzzy domain boundaries; this is mostly notable for \system{ADM}, suggesting that when domain are close, ignoring domain differences is effective. In contrary, \system{FT-Full} incurs clear losses across the board, especially for the small data condition \cite{Miceli17regularization}. Even in this very favourable case however, very few MDMT systems are able to significantly outperform \system{FT-Full} and this is only observed for the smaller part of the \domain{med} domain. The merge condition is hardly different, with again large losses for \system{FT-full} \revisiondone{and \system{FT-Res}}, and small variations for all systems. We even observe some rare improvements with respect to the situation where we use actual domains. 

\subsubsection{Handling wrong or unknown domains \label{sssec:unknowns}}

In the last two columns of Table~\ref{tab:redomains}, we report the drop in performance when the domain information is not correct. In the first (\domain{rnd}), we use test data from the domains seen in training, presented with a random domain tag. In this situation, the loss with respect to using the correct tag is generally large (more than 10 BLEU points), showing an overall failure to meet requirement [P4-ERR], except for systems that ignore domain tags in testing. 

In the second (\domain{new}), we assess [P5-UNK] by translating sentences from a domain unseen in training (\domain{news}). For each sentence, we automatically predict the domain tag and use it for decoding.\footnote{\revisiondone{Domain tags are assigned as follows: we train a language model for each domain and assign tag on a per-sentence basis based on the language model log-probability (assuming uniform domain priors). The domain classifier has an average prediction error of 16.4\% for in-domain data.}}
In this configuration, again, systems using domain tags during inference perform poorly, significantly worse than the \system{Mixed-Nat} baseline (Bleu=23.5).

\subsubsection{Handling growing numbers of domains}

Another set of experiments evaluate the ability to dynamically handle supplementary domains (requirement [P6-DYN]) as follows. Starting with the existing MD systems of Section~\ref{ssec:rawperformance}, we introduce an extra domain (\domain{News}) and resume training with this new mixture of data\footnote{The design of a proper balance between domains in training is critical for achieving optimal performance: as our goal is to evaluate all systems in the same conditions, we consider a basic mixing policy based on the new training distribution. This is detrimental to the small domains, for which the ``negative transfer'' effect is stronger than for larger domains.} for 50,000 additional iterations. We contrast this approach with training all systems from scratch and report differences in performance in Figure~\ref{fig:warmrestart} (see also Table~\ref{tab:warmrestart} in appendix~B).\footnote{WDCMT results are excluded from this table, as resuming training proved difficult to implement.}
We expect that MDMT systems should not be too significantly impacted by the addition of a new domain and reach about the same performance as when training with this domain from scratch. From a practical viewpoint, dynamically integrating new domains is straightforward for \system{DC-Tag}, \system{DC-Feat} or \system{TTM}, for which new domains merely add new labels. It is less easy for \system{DM}, \system{ADM} and \system{WDCMT}, which include a built-in domain classifier whose outputs have to be pre-specified, or for \system{LDR}, \system{FT-Res} and \system{MDL-Res} for which the number of possible domains is built in the architecture and has to be anticipated from the start. This makes a difference between domain-bounded systems, for which the number of domains is limited and truly open-domain systems.

\begin{figure*}[h!]
    \begin{center}
        \input{graphics/table5.pgf}
    \end{center}
    \caption[Ability to handle a new domain]{Ability to handle a new domain. \revisiondone{We report BLEU scores for a complete training session with 7 domains, as well as differences (in blue) with training with 6 domains (from Table~\ref{tab:performance}); and (in red) differences with continual training}.}
    \label{fig:warmrestart}
\end{figure*}


We can first compare the results of coldstart training with six or seven domains in Table~\ref{tab:warmrestart}: a first observation is that the extra training data \revisiondone{is hardly helping} for most domains, except for \domain{News}, where we see a large gain, and for \domain{talk}. The picture is the same when one looks at MDMTs, where only the weakest systems (\system{DM}, \system{ADM}) seem to benefit from more (out-of-domain) data.
Comparing now the coldstart with the warmstart scenario, we see that the former is always significantly better for \domain{news}, as expected, and that resuming training also negatively impacts the performance for other domains. This happens notably for \system{DC-Tag}, \system{TTM} and \system{ADM}. In this setting \system{MDL-Res} and \system{DM} show the smaller average loss, with the former achieving the best balance of training cost and average BLEU score.

\subsection{Automatic domains \label{ssec:autodomains}}
In this section, we experiment with automatic domains, obtained by clustering sentences into $k=30$ classes using the k-means algorithm based on generic sentence representations obtained via mean pooling (cf.\ Section~\ref{ssec:corpora}). This allows us to evaluate requirement [P7-scale], training and testing our systems as if these domains were fully separated. Many of these clusters are mere splits of the large \domain{med}, while a fewer number of classes are mixtures of two (or more) existing domains (full details are in Appendix~C). We are thus in a position to reiterate, at a larger scale, the measurements of Section~\ref{ssec:redomains} and test whether multi-domain systems can effectively take advantage from the cross-domain similarities and to eventually perform better than fine-tuning. \revisiondone{The results in Table~\ref{tab:avg_automatic_domains} also suggest that MDMT can surpass fine-tuning for the smaller clusters; for the large clusters, this is no longer true. The complete table (in Appendix~C) shows that this effect is more visible for small subsets of the medical domain.}

\begin{table*}[t]
\centering \footnotesize
\revisiondone{
  \begin{tabular}{|p{1.3cm}|*{11}{c|}} \hline
   Model/ & Train &\system{Mixed}&\system{FT}&\system{FT}&\system{MDL}&\system{DC}&\system{DC}&\multirow{2}{*}{\system{TTM}}&\multirow{2}{*}{\system{ADM}}&\multirow{2}{*}{\system{DM}}&\multirow{2}{*}{\system{LDR}}  \\ 
   Clusters & size & \system{Nat} & \system{Full} & \system{Res} &\system{Res} & \system{Feat}& \system{Tag}& & & & \\ \hline
10 small&29.3k&68.3&70.0&70.7&\bf 71.2&70.6&53.1&67.3&69.8&67.0&70.2\\
10 mid&104.7k&44.8&\bf 48.0&46.0&45.7&44.8&44.3&44.5&43.7&41.6&44.5\\
10 large&251.1k&50.4&\bf 52.9&52.0&51.3&49.6&43.2&49.1&48.5&44.3&49.5\\
Avg&128.4k&54.5&\bf 57.0&56.2&56.1&55.0&46.9&53.6&54.0&51.0&54.7\\ \hline
  \end{tabular}
   \caption{BLEU scores computed by merging the 10 smaller, medium, and larger cluster test sets. Best score for each group is in boldface. For the small clusters, full-fine tuning is outperformed by several MDMT systems - see details in Appendix~C.}
   \label{tab:avg_automatic_domains}}
\end{table*}
Finally, Table~\ref{tab:subdomains} reports the effect of using automatic domain for each of the 6 test sets: each sentence was first assigned to an automatic class, translated with the corresponding multi-domain system with 30 classes; aggregate numbers were then computed, and contrasted with the 6-domain scenario. Results are clear and confirm previous observations: even though some clusters are very close, the net effect is a loss in performance for almost all systems and conditions. In this setting, the best MDMT in our pool (\system{MDL-Res}) is no longer able to surpass the \system{Mix-Nat} baseline.

\begin{table*}[t]
  \centering
  \begin{tabular}{|p{3cm}|*{8}{r|}} \hline
%     &&&&&& \\
    Domain / Model  & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline % & \multicolumn{1}{c|}{\domain{news}} 
  %    \system{Mixed-Nat}  & 37.3 & 54.6 & 50.1 & 33.5 & 43.2 & 77.5  & 41.1  & 49.4 \\% & 23.5\\
  %     \system{FT-Full}       & 37.7 & \SB{59.2} & \SB{54.5} & 34.0 & \SB{46.8} & \SB{90.8}   & 42.8 & 53.8\\
  %   Full-finetuned on extended in-domain corpora (news) & && 33.96&&& & &\\nn
    \system{DC-Tag}       & 38.5 & \SW{54.0} & 49.0   & 33.6 & \SW{42.2} & \SW{76.7} & 41.6 & 49.0 \\%    & 21.8 \\
    \system{DC-Feat}      & 37.3  & 54.2 & 49.3   & 33.6 & \SW{41.9} & \SW{75.8} & 40.8 & \SW{48.7}  \\% & \SW{21.7} \\
    \system{LDR}             & 37.4   & 54.1 & \SW{48.7} & \SW{32.5} & \SW{41.4} & \SW{75.9} & 39.1 & \SW{48.3}         \\% & 22.1 \\ 
    \system{TTM}            & 37.4 & \SW{53.7} & 48.9 & 32.8 & 41.3 & \SW{75.8} & 40.7 & \SW{48.3}   \\% &  23.4 \\
    \system{DM}             & 35.4 & 49.3  & 45.2 & 29.7 & 37.1 & \SW{60.0} & 37.8 & 42.8 \\ % & 22.6\\
    \system{ADM}           & 36.1 & 53.5  & 48.0 & 32.0 & 41.1 & 72.1 & 39.5 & 47.1\\% & 23.3 \\
    \revisiondone{\system{FT-Res}}     & 37.5 & \SW{55.7}  & \SW{51.1}   & 33.1   &  \SW{44.1}  & \SW{86.7} & 41.6 & \SW{51.4}\\%  & \SW{21.2} \\
    \system{MDL-Res}     & 37.3 & 55.5  & \SW{50.2}   & \SW{32.2}   &  \SW{42.1}  & \SW{86.7} & 41.2 & \SW{50.7}\\%  & \SW{21.2} \\
    \system{WDCMT}       & 35.6 & 53.1 & 48.4 & 30.5 & \SW{37.7} & \SW{56.0} & 38.5 & 43.6 \\ % & 20.4 \\ 
    \hline
  \end{tabular}
  \caption{Translation performance with automatic domains, computed with the original test sets. Significance tests are for comparisons with the 6-domain scenario \revisiondone{(Table~\ref{tab:performance}).}}
  \label{tab:subdomains}
\end{table*}
\section{Conclusion and outlook \label{sec:conclusion}}

In this study, we have carefully reconsidered the idea of multi-domain machine translation, which seems to be taken for granted in many recent studies. We have spelled out the various motivations for building such systems and the associated expectations in terms of system performance. We have then designed a series of requirements that MDMT systems should meet, and proposed a series of associated test procedures. In our experiments with a representative sample of MDMTs, we have found that most requirements were hardly met for our experimental conditions. If MDMT systems are able to outperform the mixed-domain baseline, at least for some domains, they all fall short to match the performance of fine-tuning on each individual domain, which remains the best choice in multi-source single domain adaptation.

As expected however, MDMTs are less brittle than fine-tuning when domain frontiers are uncertain, and can, to a certain extend, dynamically accommodate additional domains, this being especially easy for feature-based approaches. Our experiments finally suggest that all methods show decreasing performance when the number of domains or the diversity of the domain mixture increases. % (for a fixed data size).

Two other main conclusions can be drawn from this study: first, it seems that more work is needed to make MDMT systems make the best out of the variety of the available data, both to effectively share what needs to be shared while at the same time separating what needs to be kept separated. We notably see two areas worthy of further explorations: the development of parameter sharing strategies when the number of domains is large; the design of training strategies that can effectively handle a change of the training mixture, including an increase in the number of domains. Both problems are of practical relevance in industrial settings. Second, and maybe more importantly, there is a general need to adopt better evaluation methodologies for evaluating MDMT systems, which require systems developers to clearly spell out the testing conditions and the associated expected distribution of testing instances, and to report more than comparisons with simple baselines on a fixed and known handful of domains.


