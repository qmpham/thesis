\chapter{Revisiting supervised multi-domain machine translation} \label{chap:revisiting}
\section{Introduction}
In this chapter, we analyze a wide range of popular model-centric methods which aim to solve supervised (multi-)domain adaptation (see definition in Section~\ref{sec:case1}). First we formulate the motivations for developing multi-domain machine translation (MDMT\nomenclature[mdmt]{MDMT}{Multi-domain machine translation}) systems and the associated expectations with respect to performance. We define five essential requirements that an effective MDMT system should meet (Section~\ref{sec:requirements-chap4}). We reevaluate a wide range of popular MDMT systems and show that most of these expectations are hardly met. We suggest that further work is needed to analyze the current behavior of multi-domain systems better and to make them fully hold their promises.

Data-based Machine Translation (MT), whether statistical or neural, rests on well-understood machine learning principles. Given a train sample of matched source-target sentence pairs $(\src,\trg)$ drawn from an underlying distribution $\mathcal{D}_e$ of the input space $\Omega_e$ and a labeling function $g: \Omega_e \rightarrow \Omega_f$ where $\Omega_e$ and $\Omega_f$ are the set of sentences of the language $e$ and language $f$ respectively. An NMT model parameterized by $\theta$ (here, a translation function $h_{\theta}$) is trained by minimizing the empirical expectation of a loss function $\ell(h_\theta(\src),\trg)$. This approach ensures that the translation loss remains low when translating more sentences drawn from the same distribution. Owing to the great variability of language data, this ideal situation is rarely met in practice, warranting the study of an alternative scenario, where the test distribution $\mathcal{D}_e^T$ differs from train distribution $\mathcal{D}_e^S$ and the test labeling function $g^T$ differs from the train labeling function $g^S$. In this setting, \emph{domain adaptation} (DA) methods are in order.

\emph{Multi-domain} (MD) machine translation \citep{Sajjad17neural,Farajian17multidomain,Kobus17domain,Zeng18multidomain,Pham19generic} 
generalizes the conventional setting of domain mismatch by considering a mixture of multiple domains (i.e., multiple underlying distributions and multiple labeling functions). MDMT focuses on training one single system using a mixture of multiple train domains and evaluating it with data from multiple test domains. MDMT corresponds to a very common situation, where all available data, no matter its origin, is used to train a robust system that performs well for any kind of new input. If the intuitions behind MDMT are quite simple, the exact specifications of MDMT systems are rarely spelled out: for instance, how should MDMT handle the heterogeneity of the domains' size? Should MDMT be robust to the intra-domain heterogeneity? Should MDMT also be robust to new domains? How should it exploit the proximity between domains? How should it handle domain labeling errors during the inference? How should it handle the growing number of domains? 

Multi-domain seems more challenging than domain adaptation as it tries to optimize MT performance for a more diverse set of potential inputs, with an additional uncertainty regarding the distribution of test data. However, are there still situations where MDMT systems can surpass single domain adaptation, as is sometimes expected? 

The first contribution is thus of methodological nature and consists of lists of expected properties of MDMT systems and associated measurements to evaluate them (Section~\ref{sec:challenging-chap4}). In doing so, we also shed light on new problems that arise in this context, regarding, for instance, the integration of new domains in the course of training or the computation of automatic domain tags. The second main contribution is experimental and consists in a thorough reanalysis of eight recent multi-domain model-centric approaches from the literature. We show in Section~\ref{sec:results-chap4} that existing approaches still fail to match many of these requirements, notably with respect to the handling of a large number of heterogeneous domains and to dynamically integrating new domains in training.

\textit{This chapter draws from the following publication: \citet{Pham21revisiting}.}
 
\section{Requirements of multi-domain MT \label{sec:requirements-chap4}}
In this section, we recap the main reasons for considering a multi-domain scenario and discuss their implications in terms of performance evaluation.

\subsection{Formalizing multi-domain translation \label{ssec:formalization-chap4}}
As introduced in Section ~\ref{ssec:formulizaion}, a domain $d$ of a language pair $(e,f)$ is defined by an underlying distribution $\mathcal{D}_e^d$ over the space $\Omega_e$ of sentences in language $e$ and a translation function $g^d: \Omega_e \rightarrow \Omega_f$, in which $\Omega_f$ is the space of sentences in language $f$. A typical learning scenario in MT is to have access to samples from $n_d$ domains, which means that the train distribution on the source language $\mathcal{D}_e^S$ is a mixture $\mathcal{D}_e^S(x) = \sum_d \lambda^{s}(d) \mathcal{D}_e^{d}(x)$\revisiondone{, with $\{\lambda^{s}(d), d=1 \dots n_d\}$ the corresponding mixture weights ($\sum_d \lambda^{s}(d)=1$)}. Multi-domain learning, as defined in \citet{Dredze08online} further assumes that domain tags are also available in testing; the implication being that the test distribution is also as a mixture $\mathcal{D}_e^T(x) = \sum_d \lambda^{t}(d) \mathcal{D}_e^d(x)$ of several domains, making the problem distinct from mere domain adaption. A multi-domain learner is then expected to use these tags effectively \citep{Joshi12multidomain} when computing the combined translation function $g^d(x)$, and to perform well in all domains \citep{Finkel09hierarchical}. This setting is closely related to the multi-source adaptation problem formalized in \citet{Mansour09domain,Mansour09multiple,Hoffman18algorithms}.

This definition seems to be the most accepted view of a multi-domain machine translation\footnote{An exception is \citep{Farajian17multidomain}, where test translations rely on similarity scores between test and train sentences, rather than on domain labels.} and one that we also adopt here. Note that in the absence of further specification, the naive answer to the MDMT setting should be to estimate one translation function $\hat{g}^d(x)$ separately for each domain, then to translate using $\hat{g}(x,d) = \sum_{d'} g^{d'}(x) \indic{d' = d}$, where $\indic{x}$ is the indicator function. We now discuss the arguments that are put forward to proceed differently.

\subsection{Reasons for building MDMT systems \label{ssec:whymdmt-chap4}}

The first motivation for moving away from the "one domain / one model" solution is to be practical in an environment diversified in terms of topics, genres, and styles \citep{Sennrich13multidomain,Farajian17neural}. When faced with inputs that are potentially from multiple domains, it is easier and computationally cheaper to develop one system instead of optimizing and maintaining numerous engines. Multilingual machine translation also shares the same spirit \citep{johnson17googles}. The underlying assumption here is that the number of domains of interest can be large, for example, fully personalized machine translation systems \citep{Michel18extreme}.

The second reason rests on the linguistic properties of the translation function. The domain specificities are mainly expressed lexically and will primarily affect content words or multi-word expressions. On the other hand, the function words are domain agnostic and tend to remain semantically stable across domains, motivating some cross-domain parameter sharing. An MDMT system should simultaneously learn lexical domain peculiarities and leverage cross-domain similarities to improve the translation of generic contexts and words \citep{Zeng18multidomain,Pham19generic,Jiang20Multi}. We expect the MDMT scenario to be more profitable when the domain mixture includes similar domains that share more information.

The third motivation is of statistical nature. The train data available for each domain is usually unevenly distributed; domains such as \domain{bank} in English-German contain only a few thousand examples \ref{ssec:corpora-chap6}. Moreover, for some test domains, there may even be no train data at all \citep{Farajian17neural}. Tuning an NMT model to a small dataset without regularizations usually gives us statistically less reliable estimates of the domain's parameters resulting in a high variance in the prediction. Training mix-domain systems likely reduces this variance at the expense of the statistical bias \citep{Clark12onesystem}. Under this view, MDMT would be especially beneficial for domains with little train data. In the case of multilingual MT from English, a significant improvement for under-resourced languages was reported due to positive transfer, at the cost of a decrease in performance for well-resourced languages \citep{Arivazhagan19massively}.

Ensembling multiple domain-specific MT models can also be justified for the sake of distributional robustness \citep{Mansour09domain,Mansour09multiple,Sennrich12perplexity,Sennrich12mixture,Carpuat14linear,Freitag16fast, Sajjad17neural,Saunders19domain}, for instance, when the test mixture differs from the train mixture or when it includes new domains unseen in training. An even more challenging case is when an MT system needs to perform well for any test distribution, as studied for SMT models in \citet{Huck15mixeddomain} or language models in \citet{Oren19distributionally}. In all these cases, mixing domains in training will likely improve robustness against unexpected or adversarial test distribution.

Another motivation is that mixing domains can have a positive regularization effect for all domains. Introducing variability in training prevents DA from overfitting the available adaptation data and could help improve generalization even for well-resourced domains. \citet{Joshi12multidomain} explored a related case, which shows that part of the benefits of MD training is due to an ensembling effect, where systems from multiple domains are simultaneously used in the prediction phase; this effect may persist even in the absence of clear domain separations.

In summary, there are multiple reasons for adopting MDMT, some already used in DA settings, and some original. These arguments are not mutually exclusive; however, each yields specific expectations for the performance of the MDMT models and should also require an appropriate evaluation procedure. If the motivation is primarily computational, then a drop in MT quality for multiple individual domains might be acceptable for the computational savings. If the objective is to improve statistical estimation, we expect that MDMT improves, at least for some under-resourced domains, over individually trained systems. Finally, if the goal is to make the system more robust to adversarial test distributions, one should use this setting to evaluate MDMT. The following section discusses ways that could challenge these requirements of MDMT systems. 

\section{Challenging multi-domain systems \label{sec:challenging-chap4}}
In this section, we propose seven operational requirements that can be expected from an effective multi-domain system, and discuss ways to evaluate whether these requirements are actually met. All these evaluations will rest on comparison of translation performance, and do not depend on the choice of a particular metric. To make our results comparable with the literature, we will only use the BLEU score \citep{Papineni02bleu} in Section~\ref{sec:experiments-chap4}, noting it may not be the best yardstick to assess subtle improvements of lexical choices that are often associated with domain adapted systems \citep{Irvine13measuring}. Other important figures of merit for MDMT systems are the computational training cost and the total number of parameters.

\subsection{Multi-domain systems should be effective \label{ssec:effective-chap4}}
A first expectation is that MDMT systems should perform well in the face of mixed-domain test data. We thus derive the following requirements.

\paragraph{[P1-LAB]} A MDMT should perform better than the baseline which disregards domain labels. Evaluating this requirement is a matter of a mere comparison, assuming the test distribution of domains is known: if all domains are equally important, performance averages can be reported; if they are not, weighted averages should be used instead.

\paragraph{[P2-TUN]} Additionally, one can expect that MDMT will improve over fine-tuning \citep{Luong15stanford,Freitag16fast}, at least in domains where data is scarce, or in situations where several domains are close. To evaluate this, we perform two measurements, using a real as well as an artificial scenario. In the real scenario, we simply compare the performance of MDMT and fine-tuning for domains of varying sizes and expect an important improvement for smaller domains in MDMT compared to fine-tuning. In the artificial scenario, we split a single domain in two parts which are considered distinct in training. The expectation here is that a MDMT should yield a clear gain for both pseudo sub-domains, which should benefit from the supplementary amount of relevant train data. In this situation, MDMT should even outperform fine-tuning on either of the pseudo sub-domain. The property we assess here is the robustness to the cross-domain heterogeneity.

\subsection{Robustness to fuzzy domain separations \label{ssec:robusness-chap4}}
A second set of requirements is related to the definition of a domain. As repeatedly pointed out in the literature, parallel corpora in MT are often collected opportunistically and the view that each corpus constitutes a single domain is often a gross approximation.\footnote{Two of our own ``domains'' actually comprise several subcorpora (IT and MED), see details in Section~\ref{ssec:corpora-chap4}.} MDMT should aim to make the best of the available data and be robust to domain assignments. To challenge these requirements we evaluate the following requirements.

\paragraph{[P3-HET]}
The notion of a domain being a fragile one, an effective MDMT system should be able to discover not only when cross-domain sharing is useful (cf.\ requirement [P2-TUN]), but also when intra-domain heterogeneity is hurting. This requirement is tested by artificially conjoining two closely related domains into one during training, hoping that the loss in performance with respect to the original setting (using correct domain tags) will remain small. The property we assess here is the robustness to the intra-domain heterogeneity.

\paragraph{[P4-ERR]}
MDMTs should perform best when the true domain tag is known, but deteriorate gracefully in the face of tag errors; in this situation, catastrophic drops in performance are often observed. This requirement can be assessed by translating test texts with erroneous domain tags and reporting the subsequent loss in performance. The property we assess here is the robustness to the unseen domains.

\paragraph{[P5-UNK]}
A related situation occurs when the domain of a test document is unknown. Several situations need be considered: for domains seen in training, using automatically predicted domain labels should not be much worse than using the correct one. For test documents from unknown domains (zero-shot transfer), a good MD system should ideally outperform the default baseline that merges all available data. The property we assess here is the robustness to the erroneous domain tags.

\paragraph{[P6-DYN]}
Another requirement, more of an operational nature, is that an MDMT system should smoothly evolve to handle a growing number of domains, without having to retrain the full system each time new data is available. This is a requirement [P6-DYN] that we challenge by dynamically changing the number of train and test domains. The property we assess here is the robustness to the growing number of domains.

\subsection{Scaling to a large number of domains \label{ssec:scaling-chap4}}

\paragraph{[P7-NUM]} As mentioned above, MDMT systems have often been motivated by computational arguments. This argument is all the more sensible as the number of domains increases, making the optimization of many individual systems both ineffective and undesirable. For lack of having access to corpora containing very large sets (eg.\ in the order of 100-1000) domains, we experiment with automatically learned domains.\fyFuture{considering a varying number of clusters.}

\section{Experimental settings \label{sec:experiments-chap4}}

\subsection{Data and metrics \label{ssec:corpora-chap4}}

We experiment with translation from English into French and use texts initially originating from 6~domains, corresponding to the following data sources: the UFAL Medical corpus V1.0 (\domain{med})\footnote{\url{https://ufal.mff.cuni.cz/ufal_medical_corpus}. \revisiondone{We only use the in-domain (medical) subcorpora: PATR, EMEA, CESTA, ECDC.}}, the European Central Bank corpus (\domain{bank}) \citep{Tiedemann12parallel}; The JRC-Acquis Communautaire corpus (\domain{law}) \citep{Steinberger06acquis}, documentations for KDE, Ubuntu, GNOME and PHP from Opus collection \citep{Tiedemann09news}, collectively merged in a \domain{it}-domain, Ted Talks (\domain{talk}) \citep{Cettolo12wit}, and the Koran (\domain{rel}). Complementary experiments also use v12 of the News Commentary corpus (\domain{news}). Most corpora are available from the Opus web site.\footnote{\url{http://opus.nlpl.eu}} These corpora were deduplicated and tokenized with in-house tools; statistics are in Table~\ref{tab:Corpora-chap4}. To reduce the number of types and build open-vocabulary systems, we use Byte-Pair Encoding \citep{Sennrich16neural} with 30,000 merge operations on a corpus containing all sentences in both languages.

We randomly select in each corpus a development and a test set of 1,000 lines and keep the rest for training.\footnote{The code for reproducing our train, dev and test datasets is available at \url{https://github.com/qmpham/experiments}.} Validation sets are used to chose the best model according to the average BLEU score \citep{Papineni02bleu}.\footnote{We use detokenized, truecasing and the \texttt{multibleu} script.} Statistical significance is estimated using bootstrap resampling \citep{Koehn04statistical}, implemented in compare-mt\footnote{\url{https://github.com/neulab/compare-mt}} \citep{Neubig19compare-mt}. We report significant differences at the level of $p=0.05$.

\begin{table*}[h!]
  \centering
  \begin{tabular}{|l|ccccccc|} %*{4}{|r|}}
    \cline{2-8} 
    %\multicolumn{4}{|l|}{Vocab size - En: 30,165, Fr: 30,398}\\
    \multicolumn{1}{c|}{} & \multicolumn{1}{c}{\domain{med}} & \multicolumn{1}{c}{\domain{law}} & \multicolumn{1}{c}{\domain{bank}} & \multicolumn{1}{c}{\domain{it}} & \multicolumn{1}{c}{\domain{talk}} & \multicolumn{1}{c}{\domain{rel}} & \multicolumn{1}{c|}{\domain{news}} \\
    \hline 
    \# lines & 2609 (0.68) & 501 (0.13) & 190 (0.05) & 270 (0.07) & 160 (0.04) & 130 (0.03) & 260 (0) \\
    \# \revisiondone{tokens}  &  133 / 154  &  17.1 / 19.6 &  6.3 / 7.3 &  3.6 / 4.6 &  3.6 / 4.0 &  3.2 / 3.4 & 7.8 / 9.2   \\
    \# \revisiondone{types}  & 771 / 720 & 52.7 / 63.1 & 92.3 / 94.7 & 75.8 / 91.4 & 61.5 / 73.3 & 22.4 / 10.5 & 77.8 / 80.4 \\
    \# \revisiondone{uniq} & 700 / 640 & 20.2 / 23.7 & 42.9 / 40.1 & 44.7 / 55.7 & 20.7 / 25.6 & 7.1 / 2.1 & 31.5 / 23.1 \\
    \hline
  \end{tabular}
  \caption{Corpora statistics: number of parallel lines ($\times 10^3$) and proportion in the basic domain mixture (which does not include the \domain{news} domain), number of tokens in English and French ($\times 10^6$), number of types in English and French ($\times 10^3$), number of types that only appear in a given domain ($\times 10^3$). \domain{med} is the largest domain, containing almost 70\% of the sentences, while \domain{rel} is the smallest, with only 3\% of the data.
  }
\label{tab:Corpora-chap4}
\end{table*}

We measure the distance between domains using the $\mathcal{H}$-Divergence \citep{Ben10A}, which relates domain similarity to the test error of a domain discriminator: the larger the error, the closer the domains.
Our discriminator is a SVM independently trained for each pair of domains, with sentence representations derived via mean pooling from the source side representation of the generic Transformer model. We used the scikit-learn\footnote{\url{https://scikit-learn.org}} implementation with default values. Results in Table~\ref{tab:domaindist-chap4} show that all domains are well separated from all others, with \domain{rel} being the furthest apart, while \domain{talk} is slightly more central.

\begin{table}\centering
  \begin{tabular}{|l*{5}{|r}|} 
  \cline{2-6}
  \multicolumn{1}{c|}{} & \domain{law} & \domain{bank} & \domain{talk} & \domain{IT} & \domain{rel} \\ \hline
    \domain{med} &1.93 &1.97 &1.9 &1.93 &1.97 \\
    \domain{law}   && 1.94 & 1.97 &1.93 & 1.99 \\
    \domain{bank} &&&1.98 &1.94 &1.99 \\
    \domain{talk}   &&&&1.92 &1.93 \\
     \domain{IT}     &&&&& 1.99 \\ \hline
  \end{tabular}
  \caption{The $\mathcal{H}$-divergence between domains}
  \label{tab:domaindist-chap4}
\end{table}

\subsection{Baselines \label{ssec:baselines-chap4}}

Our baselines are standard for multi-domain systems.\footnote{We however omit domain-specific systems trained only with the corresponding subset of the data, which are always inferior to the mix-domain strategy \citep{Britz17effective}.} Using Transformers implemented in OpenNMT-tf\footnote{\url{https://github.com/OpenNMT/OpenNMT-tf}} \citep{Klein17opennmt}, we build the following systems:

\begin{itemize}
\item a generic model trained on a concatenation of all corpora (\texttt{Mixed}). We develop two versions\footnote{In fact three: to enable a fair comparison with WDCMT, a RNN-based variant is also trained and evaluated. \revisiondone{This system appears as \system{Mixed-Nat-RNN} in Table~\ref{tab:performance-chap4}}.} of this system, one where the domain heterogeneity reflects the distribution of our train data \revisiondone{given in Table~\ref{tab:Corpora-chap4}} (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}). The former is the best option when the train mixture $\mathcal{D}^s$ is also expected in testing; the latter should be used when the test distribution is uniform across domains. Accordingly, we report two aggregate scores: a weighted average reflecting the train distribution, and an unweighted average, meaning that test domains are equally important;
\item fine-tuned models \citep{Luong15stanford,Freitag16fast}, based on the \system{Mixed-Nat} system. Further description of the implementation can be found in Appendix ~\ref{appendix:a}. The full fine-tuning (\system{FT-Full}) procedure may update all the parameters of the initial generic model, resulting in six systems adapted for one domain, with no parameter sharing across domains.
\end{itemize}

All models use embeddings and the hidden layers sizes of dimension~512. Transformers contain with 8 attention heads in each of the 6+6 layers; the inner feedforward layer contains 2048 cells. The adapter-based systems (see below) additionally use an adaptation block in each layer, composed of a 2-layer perceptron, with an inner $\operatorname{ReLU}$ activation function operating on normalized entries of dimension~1024. 
Training uses batches of~12,288 tokens, Adam with parameters $\beta_1=0.9$, $\beta_2= 0.98$, Noam decay ($warmup\_steps=4000$), and a dropout rate of $0.1$ in all layers.

\subsection{Multi-domain systems \label{ssec:systems-chap4}}

Our comparison of multi-domain systems includes our own reimplementations of recent proposals from the literature:\footnote{\revisiondone{Further implementation details are in Appendix ~\ref{appendix:a}.}}
\begin{itemize}
\item a system using domain control as in \citet{Kobus17domain}: domain information is introduced either as an additional token for each source sentence (\system{DC-Tag}) or as a supplementary feature for each word (\system{DC-Feat}).
\item a system using lexicalized domain representations presented in Chapter~\ref{chap:ldr}: sparse word embeddings are composed of domain-agnostic units and domain-specific units which will be nullified when translating in other domains (\system{LDR});
\item the three proposals of \citet{Britz17effective}. \system{TTM} is a feature-based approach where the domain tag is introduced as an extra word \textsl{on the target side}. The training uses reference tags, and inference is usually performed with predicted tags, just like for regular target words. \system{DM} is a multi-task learner where a domain classifier is trained on top of the MT encoder to make it aware of domain differences. \system{ADM} is the adversarial version of \system{DM}, pushing the encoder towards learning domain-independent source representations. These methods thus only use domain tags in training.
\item the multi-domain model of \citet{Zeng18multidomain} (\system{WDCMT}), where a domain-agnostic and a domain-specialized representation of the input are simultaneously processed; supervised classification and adversarial training are used to compute these representations. \revisiondone{Again, inference does not use domain tags.}\footnote{For this system, we use the available RNN-based system from the authors (\url{https://github.com/DeepLearnXMU/WDCNMT}) which does not directly compare to the other, Transformer-based, systems; the improved version of \citet{Su19exploring} seems to produce comparable, albeit slightly improved, results.}
\item two multi-domain versions of the approach of \citet{Bapna19simple}, denoted \system{FT-Res} and \system{MT-Res}, which will be compared extensively in Chapter~\ref{chap:res}. The former variant corresponds to the original proposal of \citet{Bapna19simple} \revisiondone{(see also \citet{Sharaf20metalearning})}. It fine-tunes the adapter modules of a \system{Mixed-Nat} system independently for each domain, keeping all the other parameters frozen. The latter uses the same architecture but trains all parameters jointly from scratch with a mix-domain corpus as in multi-task training \citep{Caruana97multitask}. The loss function of \system{MT-Res} is the same as in Equation ~\refeq{eq:loss-chap5}.
\end{itemize}
This list includes systems that slightly depart from our definition of MDMT. Standard implementations of \system{TTM} and \system{WDCMT} rely on the domain inference, rather than on gold, domain tags - which must somewhat affect their predictions. \system{DM} and \system{ADM} make no use of domain tags at all. 
\section{Results and discussion \label{sec:results-chap4}}

\subsection{Computational costs}
\label{ssec:cost-chap4}
The strategy "one domain / one model" allocates 65m parameters for one domain. The second most expensive method is to use residual adapters, including \system{FT-Res} and \system{MT-Res}, which use 12.4 additional parameters per domain. \system{LDR} follows residual adapters using $4 \times |\Sigma_{e,f}| \times 3$ ($\sim$384k) additional parameters per domain ($\Sigma_{e,f}$ is the joint vocabulary of the source and target languages). Domain tags such as \system{TTM}, \system{DC-Tag} spend only 512 additional parameters, which account for one additional embedding, for one domain. Domain embeddings \system{DC-Feat} add only 4 parameters per domain to the MDMT system. \system{WDCMT}'s special gates consist of few thousand additional parameters which are however shared for every domain. Other methods such as \system{DM}, \system{ADM} do not use any additional parameters.

While \system{FT-Res} finetunes residual adapters during at most 50k iterations, the other methods consume the same number of training iterations as generic models. The training and inference latency is proportional to the number of parameters in each MDMT model. Indeed, \system{MT-Res} has largest latency while other MDMT systems perform as fast as generic models.
\subsection{Performance of MDMT systems \label{ssec:rawperformance-chap4}}

\begin{table*}
  \centering
  \begin{tabular}{|p{4cm}|*{8}{r|}} \hline
%     &&&&&& \\
    Model / Domain & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline % & \multicolumn{1}{c|}{\domain{news}} 
    \system{Mixed-Nat}  \hfill{\footnotesize[65m]} & 37.3 & 54.6 & 50.1 & 33.5 & 43.2 & 77.5  & 41.1  & 49.4 \\% & 23.5\\
    \system{Mixed-Bal}   \hfill{\footnotesize[65m]} &  35.3 & 54.1 & 52.5 & 31.9 & 44.9 & 89.5 & 40.3  & 51.4 \\ %& \\
    \system{FT-Full}       \hfill{\footnotesize[6$\times$65m]} & 37.7 & \SB{59.2} & \SB{54.5} & 34.0 & \SB{46.8} & \SB{90.8}   & \SB{42.7} & \SB{53.8} \\ \hline
 %   Full-finetuned on extended in-domain corpora (news) & && 33.96&&& & &\\nn
    \system{DC-Tag} \hfill{\footnotesize[+6$\times$512]}        & 38.1 & 55.3 & 49.9   & 33.2 & 43.5 & \SB{80.5} &41.6 & 50.1    \\%    & 21.8 \\
    \system{DC-Feat} \hfill{\footnotesize[+6$\times$4]}    & 37.7  & 54.9 & 49.5   & 32.9 & 43.6 & \SB{79.9} &41.4 & 49.9   \\% & \SW{21.7} \\
    \system{LDR}       \hfill{\footnotesize[+6$\times$384k]}    & 37.0   & 54.7 & 49.9 & 33.9 & 43.6 & \SB{79.9} &40.9 & 49.8          \\% & 22.1 \\ 
    \system{TTM}      \hfill{\footnotesize[+6$\times$1024]}        & 37.3 & 54.9 & 49.5 & 32.9 & 43.6 & \SB{79.9} &41.0 & 49.7     \\% &  23.4 \\
    \system{DM}        \hfill{\footnotesize[+0]}         & \SW{35.6} & \SW{49.5}  & \SW{45.6}& \SW{29.9} & \SW{37.1} & \SW{62.4} & 38.1 & 43.4 \\ % & 22.6\\
    \system{ADM}      \hfill{\footnotesize[+0]}         & 36.4 & \SW{53.5}  & \SW{48.3} & \SW{32.0} & \SW{41.5} & \SW{73.4} & 38.9 & 47.5 \\% & 23.3 \\
    \revisiondone{\system{FT-Res}}   \hfill{\footnotesize[+6$\times$12.4m]}  & 37.3 & \SB{57.9} & \SB{53.9} & 33.8 & \SB{46.7} & \SB{90.2}  & \SB{42.3} & \SB{53.3} \\ % & 20.5\\ \hline
    \system{MT-Res} \hfill{\footnotesize[+6$\times$12.4m]}    & 37.9 & \SB{56.0}  & \SB{51.2}   & 33.5   &  44.4  & \SB{88.3} & 42.0 & \SB{51.9} \\%  & \SW{21.2} \\
     \hline \hline
    \system{Mixed-Nat-RNN} \hfill{\footnotesize[51m]}  & 36.8 & 53.8 & 47.2 & 30.0 & 35.7 & 60.2  & 39.2  & 44.0 \\
    \hline
    \system{WDCMT}  \hfill{\footnotesize[73m]} & 36.0 & 53.3 & \SB{48.8} & 31.1 & \SB{38.8} & \SW{58.5} & 39.0 & 44.4 \\ % & 20.4 \\
    \hline
  \end{tabular}
  \caption{Translation performance of MDMT systems based on the same Transformer (top) or RNN (bottom) architecture. The former contains 65m parameters, the latter has 51m. For each system, we report the number of additional domain specific parameters, BLEU scores for each domain, domain-weighted (w\domain{avg}) and unweighted (\domain{avg}) averages. For weighted-averages, we take the domain proportions from Table~\ref{tab:Corpora-chap4}. Boldface denotes significant gains with respect to \system{Mix-Nat} (or \system{Mix-Nat-RNN}, for WDCMT), underline denotes significant losses.}
  \label{tab:performance-chap4}
\end{table*}

In this section, we discuss the basic performance of MDMT systems trained and tested on $6$~domains. Results are in Table~\ref{tab:performance-chap4}. There is a large difference between \system{Mixed-Nat} and \system{Mixed-Bal}. The former system trained with balancing data in the generic setting is 2~BLEU points better in the uniform average, notably owing to the much better results for \domain{rel}. As explained above, this setting should be the baseline when the test distribution is assumed to be balanced across domains. We use the weighted average to perform global comparisons as all other MDMT systems are trained with unbalanced data distribution.

Fine-tuning each domain separately yields a better baseline, outperforming \system{Mixed-Nat} for all domains, with significant gains for domains that are distant from \domain{med}: \domain{rel}, \domain{it}, \domain{bank}, \domain{law}.

All MDMT systems (except \system{DM} and \system{ADM}) slightly improve over \system{Mixed-Nat}(for most domains), but these gains are rarely significant. \revisiondone{Among systems using an extra domain feature, \system{DC-Tag} has a small edge over \system{DC-Feat} and also requires less parameters; it also outperforms \system{TTM}, which however uses predicted rather than gold domain tags in the inference. \system{TTM} is also the best choice among the systems that do not use domain tags in inference.} \revisiondone{The best MDMT candidates overall are  \system{FT-Res} and \system{MT-Res}, which significantly improve over \system{Mixed-Nat} for a majority of domains, and are the only ones to clearly fulfill [P1-LAB];} \system{WDCMT} also improves on three domains, but regresses on one. The use of a dedicated adaptation module thus seems better than feature-based strategies, \revisiondone{but yields a large increase of the number of parameters.} The effect of the adaptation layer is especially significant for small domains (\domain{bank}, \domain{it} and \domain{rel}).

All systems fail to outperform fine-tuning, sometimes by a wide margin, especially for an ``isolated'' domain like \domain{rel}. This might be due to the fact that domains are well separated (cf.\ Section~\ref{ssec:corpora-chap4}) and are hardly helping each other. In other words, the statistical bias is not well compensated by the variance reduction. In this situation, MDMT systems should dedicate a sufficient number of parameters to each domain, so as to close the gap with fine-tuning.

\subsection{Redefining domains \label{ssec:redomains-chap4}}

Table~\ref{tab:redomains-chap4} summarizes the results of four experiments where we artificially redefine the boundaries of domains, to challenge requirements [P2-TUN], [P3-HET], and [P4-ERR]. In the first three, we randomly \emph{split} one corpus in two parts and proceed as if this corresponded to two actual domains. An MD system should detect that these two pseudo-domains are mutually beneficial and should hardly be affected by this change with respect to the baseline scenario (no split). In this situation, we expect MDMT to even surpass fine-tuning separately on each of these dummy domains, as MDMT exploits all data. In contrast, the fine-tuning method focuses only on a subpart. In testing, we decode the test set twice, once with each pseudo-domain tag. This makes no difference for \system{TTM}, \system{DM}, \system{ADM} and \system{WDCMT}, which do not use domain tags in testing. In the \textsl{Merge} experiment, we merge two corpora in training to assess the robustness to heterogeneous domains [P3-HET]. We then translate the two corresponding tests with the same (merged) system.

\begin{table*}
  \centering% \small
  \begin{tabular}{|p{1.8cm}|*{10}{r|}} \hline
    \hfill Set-up & \multicolumn{2}{c|}{Split} &  \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Split} & \multicolumn{2}{c|}{Merge} & \multicolumn{2}{c|}{Wrong} \\ % \hline
     Model \hfill & \multicolumn{2}{c|}{\domain{med} \footnotesize{(0.5 / 0.5)}} &  \multicolumn{2}{c|}{\domain{med} {\footnotesize (0.25 / 0.75)}} & \multicolumn{2}{c|}{\domain{law} {\footnotesize (0.5 / 0.5)}} & \multicolumn{2}{c|}{\domain{bank}+\domain{law}} &  \multicolumn{1}{c|}{\domain{rnd}} &  \multicolumn{1}{c|}{\domain{new}}\\ \hline
    & \domain{med}$_1$ & \domain{med}$_2$ & \domain{med}$_1$ & \domain{med}$_2$ &  \domain{law}$_1$ & \domain{law}$_2$ & \domain{bank} & \domain{law}   & \domain{all} & \domain{News} \\
    \system{FT-Full}      & -0.1 & -0.6 & \SW{-1.5} & -0.2& \SW{-2.3} & \SW{-5.1} &\SW{-1.6} & \SW{-1.4}& \SW{-19.6} & \SW{-3.3}\\% [32.5]
    \system{DC-Tag}     & -0.2 & -0.3& +\SB{0.1}  & +0.2& -0.4 & -0.4 & -0.5 & -0.4 & \SW{-13.4} & \SW{-1.7}\\% [35.9]
    \system{DC-Feat}    & -0.5 & 0.0 & +\SB{0.3}   & +0.3 & +0.3 & +0.3 & +0.3 & +0.1 & \SW{-14.2} &\SW{-1.8}\\ % [34.9] 
    \system{LDR}           & +0.1 & +0.1 & +0.4 & +0.4 & 0.0 &  0.0 &  0.0 & +0.1& \SW{-12.0} & \SW{-1.4}\\ % [37.0]
    \system{TTM} (*)        & -0.2 &  -0.2 & -0.2 & -0.2 & -0.3 &-0.3 &  0.0 & -0.3 & 0.0 & -0.1\\
    \system{DM} (*)           & -0.3   & -0.3  & +0.4 & +0.4 & +0.3 & +0.3 & +0.9 & +0.1 & 0.0 &-0.9\\
    \system{ADM} (*)        & +0.6   & +0.6 & +0.4 & +0.4 & +0.4 & +0.4 &  +0.1 & -0.4 & 0.0&-0.2\\
    \revisiondone{\system{FT-Res}}   & -0.1   & -0.4 & -0.3 &-0.3 & \SW{-2.2} & \SW{-2.9} & \SW{-2.4} & -\SW{3.2} & \SW{-13.3} & \SW{-3.0}\\ % [31.8]
    \system{MT-Res}   & -0.2   & -0.1 & +\SB{0.2} &+0.0 & -0.9 & -0.9 & +0.7 & -0.3 & \SW{-18.6} & \SW{-1.3}\\ % [31.8]
    \system{WDCMT} (*)     & -0.0    & -0.0  & +0.2 & +0.2  & +0.8 & +0.8  & -0.4 & -0.8 & 0.0 & +0.2 \\
    \hline
  \end{tabular}
  \caption{Translation performance with variable domain definitions. In the \textsl{Split}/\textsl{Merge} experiments, we report BLEU differences for the related test set(s). In the test \domain{new}, we report BLEU differences between each MDMT system and the generic system \system{Mixed-Nat}. In the test \domain{rnd}, we report BLEU differences between using random domain tag and using true domain tag. Underline denotes significant loss when domains are changed wrt.\ the baseline situation; bold for a significant improvement over \system{FT-Full}; (*) MDMT systems ignoring test domains.
  }
  \label{tab:redomains-chap4}
\end{table*}

Our findings can be summarized as follows. For the \textsl{Split} experiments, we see small variations that can be positive or negative compared to the baseline situation, but these are hardly significant. All systems show some robustness with respect to fuzzy domain boundaries; this is mostly notable for \system{ADM}, suggesting that when domains are close, ignoring domain differences is effective. In contrary, \system{FT-Full} incurs clear losses across the board, especially for the small data condition \citep{Miceli17regularization}. Even in this very favorable case however, very few MDMT systems are able to significantly outperform \system{FT-Full} and this is only observed for the smaller part of the \domain{med} domain. The \textsl{Merge} condition is hardly different, with again large losses for \system{FT-full} and \system{FT-Res}, and small variations for all systems. We even observe some rare improvements with respect to the situation where we use actual domains. 

\subsubsection{Handling wrong or unknown domains \label{sssec:unknowns-chap4}}

In the last two columns of Table~\ref{tab:redomains-chap4}, we report the drop in performance when the domain information is not correct. In the first (\domain{rnd}), we use test data from the domains seen in training, presented with a random domain tag. In this situation, the loss with respect to using the correct tag is generally large (more than 10 BLEU points), showing an overall failure to meet requirement [P4-ERR], except for systems that ignore domain tags in testing. 

In the second (\domain{new}), we assess [P5-UNK] by translating sentences from a domain unseen in training (\domain{news}). For each sentence, we automatically predict the domain tag and use it for decoding.\footnote{\revisiondone{Domain tags are assigned as follows: we train a language model for each domain and assign a tag to a sentence basis based on the language model log-probability (assuming uniform domain priors). This domain classifier has an average prediction error of 16.4\% for in-domain data.}}
In this configuration, again, systems using domain tags during inference perform poorly, significantly worse than the \system{Mixed-Nat} baseline (Bleu=23.5).

\subsubsection{Handling growing numbers of domains}
\label{ssec:continual-chap4}
Another set of experiments evaluate the ability to dynamically handle supplementary domains (requirement [P6-DYN]) as follows. Starting with the existing MD systems of Section~\ref{ssec:rawperformance-chap4}, we introduce an extra domain (\domain{News}) and resume training with this new mixture of data\footnote{The design of a proper balance between domains in training is critical for achieving optimal performance: as our goal is to evaluate all systems in the same conditions, we consider a basic mixing policy based on the new train distribution.} for 50,000 additional iterations. We contrast this approach with training all systems from scratch and report differences in performance in Figure~\ref{fig:warmrestart-chap4} (see also Table~\ref{tab:warmrestart} in Appendix~\ref{appendix:b}).\footnote{WDCMT results are excluded from this table, as resuming training proved difficult to implement.}

We expect that MDMT systems should not be too significantly impacted by adding a new domain and reach about the same performance as when training with this domain from scratch. From a practical viewpoint, dynamically integrating new domains is straightforward for \system{DC-Tag} or \system{TTM}, for which new domains merely add new labels. \system{DC-Feat}, \system{MT-Res} and \system{FT-Res} can also easily add new domain embeddings and residual adapters respectively. It is less trivial for \system{DM}, \system{ADM} and \system{WDCMT}, which include a built-in domain classifier whose outputs have to be pre-specified or for \system{LDR} where domain-specific units have to be predefined. This makes a difference between domain-bounded systems, for which the number of domains is limited, and truly domain-open systems.

\begin{figure*}[h!]
    \begin{center}
        \input{graphics/table7.pgf}
    \end{center}
    \caption[Ability to handle a new domain]{Ability to handle a new domain. We report BLEU scores for a complete training session with 7 domains, as well as differences (in blue) with training with 6 domains (from Table~\ref{tab:performance-chap4}); and (in red) differences with continual training}.
    \label{fig:warmrestart-chap4}
\end{figure*}

First, we compare the results of coldstart training with six and seven domains in Table~\ref{tab:warmrestart}. We observe that the extra train data \revisiondone{is hardly helping} for most domains, except for \domain{News}, where we see a significant gain, and for \domain{talk}. The picture is the same when one looks at MDMTs, where only the weakest systems (\system{DM}, \system{ADM}) seem to benefit from more (out-of-domain) data.

Secondly, we compare the coldstart with the warmstart scenario. We see that the former is always significantly better for \domain{news}, as expected, and that resuming training also negatively impacts the performance for other domains. This happens notably for \system{DC-Tag}, \system{TTM} and \system{ADM}. In this setting, \system{MT-Res} and \system{DM} show small average losses, with the former achieving the best balance of training cost and average BLEU score.

\subsection{Automatic domains \label{ssec:autodomains-chap4}}
In this section, we want to evaluate the performance of MDMT systems in a large-scale setting that consists of a high number of domains. Without access to an actual large condition, we propose an artificial setting with automatic domains, obtained by clustering sentences of the mixed corpus into $k=30$ classes using the k-means algorithm based on generic sentence representations obtained via mean pooling (cf.\ Section~\ref{ssec:corpora-chap4}). This allows us to evaluate the requirement [P7-scale]. We train and test our MDMT systems as if these clusters were distinct domains. Many of these clusters are mere splits of the large \domain{med}, while a fewer number of classes are mixtures of two (or more) existing domains (full details are in Appendix~\ref{appendix:c}). We are thus in a position to reiterate, at a larger scale, the measurements of Section~\ref{ssec:redomains-chap4} and test whether multi-domain systems can effectively take advantage of the cross-domain similarities and perform better than fine-tuning eventually. \revisiondone{The results in Table~\ref{tab:avg_automatic_domains-chap4} also suggest that MDMT can surpass fine-tuning for the smaller clusters; for the large clusters, this is no longer true. The complete table (in Appendix~\ref{appendix:c}) shows that this effect is more visible for small subsets of the medical domain.}

\begin{table*}[t]
\centering \footnotesize
\revisiondone{
  \begin{tabular}{|p{1.3cm}|*{11}{c|}} \hline
   Model/ & Train &\system{Mixed}&\system{FT}&\system{FT}&\system{MT}&\system{DC}&\system{DC}&\multirow{2}{*}{\system{TTM}}&\multirow{2}{*}{\system{ADM}}&\multirow{2}{*}{\system{DM}}&\multirow{2}{*}{\system{LDR}}  \\ 
   Clusters & size & \system{Nat} & \system{Full} & \system{Res} &\system{Res} & \system{Feat}& \system{Tag}& & & & \\ \hline
10 small&29.3k&68.3&70.0&70.7&\bf 71.2&70.6&53.1&67.3&69.8&67.0&70.2\\
10 mid&104.7k&44.8&\bf 48.0&46.0&45.7&44.8&44.3&44.5&43.7&41.6&44.5\\
10 large&251.1k&50.4&\bf 52.9&52.0&51.3&49.6&43.2&49.1&48.5&44.3&49.5\\
Avg&128.4k&54.5&\bf 57.0&56.2&56.1&55.0&46.9&53.6&54.0&51.0&54.7\\ \hline
  \end{tabular}
   \caption{BLEU scores computed by merging the 10 smaller, medium, and larger cluster test sets. Best score for each group is in boldface. For the small clusters, full-fine tuning is outperformed by several MDMT systems - see details in Appendix~\ref{appendix:c}.}
   \label{tab:avg_automatic_domains-chap4}}
\end{table*}
Finally, Table~\ref{tab:subdomains-chap4} reports the effect of using automatic class index instead of true domain. For each of the 6 test sets, each sentence was first assigned to an automatic class, then translated with the corresponding multi-domain system with 30 classes. We compare the performance of MDMT systems on 30-automatic-class scenario with ones in the 6-domain scenario. Results are clear and confirm previous observations: even though some clusters are very close, the net effect is a loss in performance for almost all systems and conditions.

\begin{table*}[t]
  \centering
  \begin{tabular}{|p{3cm}|*{8}{r|}} \hline
%     &&&&&& \\
    Domain / Model  & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline % & \multicolumn{1}{c|}{\domain{news}} 
  %    \system{Mixed-Nat}  & 37.3 & 54.6 & 50.1 & 33.5 & 43.2 & 77.5  & 41.1  & 49.4 \\% & 23.5\\
  %     \system{FT-Full}       & 37.7 & \SB{59.2} & \SB{54.5} & 34.0 & \SB{46.8} & \SB{90.8}   & 42.8 & 53.8\\
  %   Full-finetuned on extended in-domain corpora (news) & && 33.96&&& & &\\nn
    \system{DC-Tag}       & 38.5 & \SW{54.0} & 49.0   & 33.6 & \SW{42.2} & \SW{76.7} & 41.6 & 49.0 \\%    & 21.8 \\
    \system{DC-Feat}      & 37.3  & 54.2 & 49.3   & 33.6 & \SW{41.9} & \SW{75.8} & 40.8 & \SW{48.7}  \\% & \SW{21.7} \\
    \system{LDR}             & 37.4   & 54.1 & \SW{48.7} & \SW{32.5} & \SW{41.4} & \SW{75.9} & 39.1 & \SW{48.3}         \\% & 22.1 \\ 
    \system{TTM}            & 37.4 & \SW{53.7} & 48.9 & 32.8 & 41.3 & \SW{75.8} & 40.7 & \SW{48.3}   \\% &  23.4 \\
    \system{DM}             & 35.4 & 49.3  & 45.2 & 29.7 & 37.1 & \SW{60.0} & 37.8 & 42.8 \\ % & 22.6\\
    \system{ADM}           & 36.1 & 53.5  & 48.0 & 32.0 & 41.1 & 72.1 & 39.5 & 47.1\\% & 23.3 \\
    \revisiondone{\system{FT-Res}}     & 37.5 & \SW{55.7}  & \SW{51.1}   & 33.1   &  \SW{44.1}  & \SW{86.7} & 41.6 & \SW{51.4}\\%  & \SW{21.2} \\
    \system{MT-Res}     & 37.3 & 55.5  & \SW{50.2}   & \SW{32.2}   &  \SW{42.1}  & \SW{86.7} & 41.2 & \SW{50.7}\\%  & \SW{21.2} \\
    \system{WDCMT}       & 35.6 & 53.1 & 48.4 & 30.5 & \SW{37.7} & \SW{56.0} & 38.5 & 43.6 \\ % & 20.4 \\ 
    \hline
  \end{tabular}
  \caption{Translation performance with automatic domains, computed with the original test sets. Significance tests are for comparisons with the 6-domain scenario (Table~\ref{tab:performance-chap4}).}
  \label{tab:subdomains-chap4}
\end{table*}
\section{Conclusions and outlook \label{sec:conclusion-chap4}}

In this study, we have carefully reconsidered the idea of multi-domain machine translation, which seems to be taken for granted in many recent studies. We spelled out the various motivations for building such systems and the associated expectations in terms of system performance. We designed a series of requirements that MDMT systems should meet and proposed a series of associated test procedures. In our experiments with a wide range of MDMT methods, we have found that most requirements were hardly met for our experimental conditions. Effectively, when MDMT systems outperform the mixed-domain baseline, at least for some domains, they all fall short of matching the performance of fine-tuning on each domain, which remains the best choice in single domain adaptation.

However, MDMTs are less brittle than fine-tuning when domain frontiers are uncertain and can, to a certain extend, dynamically accommodate additional domains, this being especially easy for feature-based approaches. Our experiments finally suggest that all methods show decreasing performance when the number of domains or the diversity of the domain mixture increases.

Two other main conclusions can be drawn from this study. First, it seems that more work is needed to make MDMT systems make the best out of the variety of the available data, both to effectively share what needs to be shared while at the same time separating what needs to be kept separated. Second, and maybe more importantly, there is a general need to adopt better evaluation methodologies for evaluating MDMT systems. Systems developers should spell out the test conditions and the associated distribution of test instances and use as many domains as possible since a great variety of data is available nowadays.



