\chapter{A - Description of multi-domain systems \label{ssec:implementation-details} (Chapter \ref{chap:revisiting})}
\label{appendix:a}
We use the following setups for MDMT systems.
\begin{itemize}
\item \system{Mixed-Nat}, \system{FT-full}, \system{TTM}, \system{DC-Tag} use a medium Transformer model of \cite{Vaswani17attention} with the following settings: embeddings size and hidden layers size are set to~$512$. Multi-head attention comprises 8 heads in each of the 6 layers; the inner feedforward layer contains~$2048$ cells. 
Training use a batch size of~$12,288$ tokens; optimization uses Adam with parameters $\beta_1=0.9$, $\beta_2= 0.98$ and Noam decay ($warmup\_steps=4000$), and a dropout rate of $0.1$ for all layers.
\item \system{FT-Res} and \system{MT-res} use the same medium Transformer and add residual layers with a bottleneck dimension of size $1024$.
\item \system{ADM}, \system{DM} use medium Tranformer model and a domain classifier composing of 3 dense layers of size $512 \times 2048$, $2048 \times 2048$ and $2048 \times domain\_num$. The two first layers of the classifier use the $\operatorname{ReLU}()$ as activation function, the last layer uses $\tanh()$ as activation function.
\item \system{DC-Feat} uses medium Transformer model and domain embeddings of size~$4$. Given a sentence of domain~$i$ in a training batch, the embedding of domain $i$ is concatenated to the embedding of each token in the sentence.
\item \system{LDR} uses medium Transformer model and for each token we introduce a \system{LDR} feature of size $4 \times domain\_num$. Given a sentence of domain $i\in[1,..,K]$ in the training batch, for each token of the sentence, the \system{LDR} units of the indexes outside of the range $[4(i-1),..,4i-1]$ are masked to $0$, and the masked LDR feature will be concatenated to the embedding of the token of size $508$. Details are in Section ~\ref{sssec:ldr6domain-chap5}.
\item \system{Mixed-Nat-RNN} uses one bidirectional LSTM layer in the encoder and one LSTM layer in the decoder. The size of hidden layers is $1024$, the size of word embeddings is $512$. 
\item \system{WDCNMT} uses one bidirectional GRU layer in the encoder and one GRU-conditional layer in the decoder. The size of hidden layers is $1024$, the size of word embeddings is $512$.
\end{itemize}
\paragraph{Training} For each domain, we create train/dev/test sets by randomly splitting each corpus. We maintain the size of validation sets and of test sets equal to 1,000 lines for every domain.
The learning rate is set
as in \cite{Vaswani17attention}. For the fine-tuning procedures used for \system{FT-full} and \system{FT-Res}, we continue training using the same learning rate schedule, continuing the incrementation of the number of steps. All other MDMT systems reported in Tables~\ref{tab:performance-chap4} and \ref{tab:redomains-chap4} use a combined validation set comprising 6,000 lines, obtained by merging the six development sets. For the results in Table~\ref{tab:warmrestart} we also append the validation set of \domain{news} to the multi-domain validation set. In any case, training stops if either training reaches the maximum number of iterations (50,000) or the score on the validation set does not increase for three consecutive evaluations. We average five checkpoints to get the final model.

\chapter{B - Experiments with continual learning \label{ssec:full-continual}(Chapter \ref{chap:revisiting})}
\label{appendix:b}
\begin{table*}[h!]
  \centering
  \begin{tabular}{|p{1.6cm}|*{9}{r|}} \hline
    \footnotesize
   \hfill Domain  Model \hfill & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{\domain{news}} & \multicolumn{1}{c|}{w\domain{avg}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline 
    \footnotesize\system{Mixed-Nat}  & 37.1  & 54.1  & 49.6	& 34.1  & 42.1	& 77.0 & 28.9 & 40.8	& 49.0 \\[-2pt]
                   & \sbcl{+0.2}{--}  &\sbcl{+0.5}{---} &\sbcl{+0.5}{--} &\sbcl{-0.6}{--} &\sbcl{+1.1}{--} &\sbcl{+0.5}{--} &\sbcl{-5.4}{--} &\sbcl{+0.3}{--} & \sbcl{+0.4}{--} \\
%    \system{Mixed-Nat}  & 37.3  & 54.6   & 50.1   & 33.5  & 43.2  & 77.5  & 23.5 & 41.1 & 49.4 \\
%    \system{FT-Full}       & 37.7  & 59.2   & 54.5   & 34.0  & 46.8  & 90.8  & **** & 42.8 & 53.8 \\
    \hline%\hline
    \footnotesize \system{DC-Tag}      &  37.7 & 54.5   & 49.9    &  34.8 &  43.9  & 78.8 & 29.5  & 41.4 & 49.9\\[-2pt]
%                                   & 38.0  & 54.4   & 49.2    & \SW{33.6}  &  \SW{42.7}  &\SW{75.3}  & \SW{28.1}  & & 48.9\\
                   & \sbcl{+0.3}{+0.3}  & \sbcl{\SB{+0.8}}{-0.1}   & \sbcl{-0.04}{-0.6}  & \sbcl{\SW{-1.6}}{\SW{-1.1}}  &  \sbcl{-0.4}{\SW{-1.3}}  & \sbcl{\SB{+1.7}}{\SW{-3.5}}  & \sbcl{\SW{-7.7}}{\SW{-1.4}}  & \sbcl{+0.2}{-0.1} & \sbcl{+0.1}{\SW{-1.1}}\\
                   
    \footnotesize \system{DC-Feat}     & 37.4 & 54.9   & 50.0    & 34.7  &  43.9  & 79.6 & 28.9  & 41.2 & 50.1\\[-2pt]
%                   & 37.2 & 54.7   & 49.9    & 34.2  &  43.0  & 79.9 & \SW{28.1}  & &\\
                   &  \sbcl{+0.3}{-0.2} & \sbcl{-0.1}{-0.1}  & \sbcl{-0.3}{-0.1} & \sbcl{\SW{-1.3}}{-0.6} & \sbcl{-0.1}{-0.9} & \scriptsize \sbcl{+0.4}{+0.3} & \sbcl{\SW{-7.3}}{\SW{-0.8}} & \sbcl{+0.1}{-0.2} & \sbcl{-0.2}{-0.3}\\
    
    \footnotesize \system{LDR}    & 37.0   & 54.6  & 49.6    & 34.3  &  43.0  &77.0  & 28.7 & 40.8 & 49.2 \\[-2pt]
                    & \sbcl{0.0}{-0.6} &  \sbcl{+0.1}{+0.5} & \sbcl{+0.2}{-0.4} &  \sbcl{-0.4}{-0.6} &  \sbcl{+0.5}{+0.5} & \sbcl{\SB{+2.9}}{+\SB{3.8}} & \sbcl{\SW{-6.6}}{\SW{-0.9}} & \sbcl{+0.6}{+0.5} &  \sbcl{+0.1}{-0.4} \\

    \footnotesize \system{TTM}           &  37.3 & 54.4   & 49.6    & 33.8  &  42.9  &78.2  & 29.1 & 41.0 & 49.4\\[-2pt]

                    & \sbcl{0.0}{-0.3}  & \sbcl{+0.4}{-0.3}  & \sbcl{-0.1}{-0.5}  & \sbcl{-0.9}{\SW{-1.1}}  & \sbcl{+0.6}{\SW{-1.0}}  & \sbcl{\SB{+1.8}}{\SW{-4.0}} & \sbcl{\SW{-5.7}}{\SW{-1.4}} & \sbcl{0.0}{-0.5} & \sbcl{+0.3}{\SW{-1.2}}\\
    
    \footnotesize \system{DM}            &36.0 &51.3&46.8&31.8&39.8&65.7&27.0 & 38.9 & 45.2\\[-2pt]
                   & \sbcl{-0.4}{+0.6} & \sbcl{\SW{-1.8}}{+0.4} & \sbcl{\SW{-1.2}}{+0.6} & \sbcl{\SW{-1.8}}{-0.1} & \sbcl{\SW{-2.6}}{+0.5} & \sbcl{\SW{-3.3}}{0.0} & \sbcl{\SW{-4.4}}{\SW{-1.2}} & \sbcl{-0.8}{+0.5} & \sbcl{\SW{-1.8}}{+0.3}\\    
    
    \footnotesize \system{ADM}          &36.6&54.2&49.1&32.9&42.1&75.7&28.7 & 40.2 & 48.4 \\[-2pt]
                   & \sbcl{-0.2}{+0.3} & \sbcl{-0.7}{-0.8} & \sbcl{-0.8}{-0.8} & \sbcl{-0.9}{-0.2} & \sbcl{-0.5}{-0.4} & \sbcl{\SW{-2.3}}{\SW{-5.0}} & \sbcl{\SW{-5.4}}{\SW{-1.9}} & \sbcl{-0.5}{-0.2} & \sbcl{\SW{-0.9}}{\SW{-1.1}}\\
    
    \footnotesize \system{FT-Res}  & 37.0 & 57.6 & 53.8 & 34.5 &	46.1 & 91.1 & 29.6 &	42.2  & 53.3 \\[-2pt]
                               & \sbcl{+0.3}{+0.3} & \sbcl{+0.4}{+0.4} & \sbcl{+0.1}{+0.1} & \sbcl{-0.7}{-0.7} & \sbcl{+0.5}{+0.5} & \sbcl{-0.9}{-0.9} & \sbcl{\SW{-9.0}}{-0.6} & \sbcl{-0.1}{-0.1} & \sbcl{+0.2}{+0.2} \\
    
    \footnotesize \system{MT-Res}    &37.7   & 55.6   & 51.1   & 34.4  & 44.5  & 87.5  & 29.1 & 41.9 & 51.8 \\[-2pt]
                        &  \sbcl{+0.2}{-0.2} & \sbcl{+0.4}{+0.5} & \sbcl{+0.1}{0.0} & \sbcl{-0.9}{-0.4}  & \sbcl{-0.1}{-0.2} &  \sbcl{+0.9}{-0.2} & \sbcl{\SW{-8.0}}{\SW{-0.8}} & \sbcl{+0.1}{-0.2} & \sbcl{+0.1}{-0.1} \\
     \hline
  \end{tabular}
  \caption{Ability to handle a new domain. We report BLEU scores for a complete training session with 7 domains, as well as differences with (left) training with 6 domains (from Table~\ref{tab:performance-chap4}); (right) continuous training mode. Averages only take into account six domains (\domain{News} excluded). Underline denotes a significant loss, bold a significant gain.}
  \label{tab:warmrestart}
\end{table*}

\chapter{C - Experiments with automatic domains \label{ssec:full-automatic}(Chapter \ref{chap:revisiting})}
\label{appendix:c}
\begin{table*}[t]
  \centering
  \footnotesize
  \begin{tabular}{|p{1.3cm}|*{11}{c|}} \hline
   Model  & size&\system{Mixed}&\system{FT}&\system{FT}&\system{MT}&\system{DC}&\system{DC}&\multirow{2}{*}{\system{TTM}}&\multirow{2}{*}{\system{ADM}}&\multirow{2}{*}{\system{DM}}&\multirow{2}{*}{\system{LDR}}  \\ 
   Cluster & train / test & \system{Nat} & \system{Full} & \system{Res} &\system{Res} & \system{Feat}& \system{Tag}& & & & \\ \hline
24 \hfill [med]&8.1k / 3 &90.4&90.4&90.4&90.4&100.0&65.6&100.0&90.4&100.0&100.0 \\
13 \hfill [-]&17.3k / 52&67.6&75.4&74.3&74.3&75.0&54.7&74.7&75.9&65.9&76.9 \\
28 \hfill [-]&25.6k / 54&71.6&68.7&68.1&70.2&71.0&42.5&72.0&71.3&65.6&72.6 \\
19 \hfill [IT]&27.2k / 88&58.5&63.0&60.9&63.9&63.7&57.2&59.4&61.1&60.5&60.3 \\
0   \hfill [-]&27.4k / 72&43.9&33.3&45.4&45.4&49.9&15.4&46.8&49.2&46.6&47.8 \\
22 \hfill [-]&27.5k / 103&91.5&93.7&93.4&93.9&92.5&72.8&92.3&93.2&91.4&93.4 \\
25 \hfill [-]&28.2k / 56&57.0&44.8&48.2&49.1&54.6&47.2&49.8&54.2&45.1&52.4 \\
16 \hfill [med]&30.4k / 18&57.2&70.4&77.4&73.5&61.8&54.2&58.4&58.1&52.5&58.3 \\
23 \hfill [med]&47.0k / 23&24.5&27.2&26.5&28.5&30.5&27.3&32.0&24.4&29.0&29.8 \\
17 \hfill [med]&54.4k / 26&39.9&40.3&41.6&38.0&37.1&36.6&35.2&35.4&31.3&33.7 \\
8  \hfill [IT]&61.4k / 214&46.9&53.1&55.8&53.6&48.9&45.1&48.8&50.9&43.0&46.7 \\
1 \hfill [-]&68.1k / 122&47.2&47.5&48.7&45.1&46.8&39.1&45.4&44.2&40.7&44.9 \\
7 \hfill [med]&91.5k / 30&41.3&35.5&41.4&39.9&41.4&36.5&37.3&37.1&40.7&41.8 \\
11 \hfill [med]&93.0k / 38&31.6&42.6&31.8&35.4&36.0&29.6&36.7&32.7&26.5&36.6 \\
29 \hfill [law]&109.2k / 242&65.9&69.2&67.6&67.7&66.0&63.8&65.1&64.7&62.4&65.9 \\
27 \hfill [med]&109.3k / 49&11.0&9.6&8.7&9.2&10.0&19.4&9.4&7.9&10.7&10.6 \\
5 \hfill [-]&109.9k / 267&46.3&47.4&46.9&45.4&44.0&42.9&43.7&44.3&40.9&45.7 \\
6 \hfill [med]&133.4k / 73&37.2&38.9&38.7&36.8&37.5&27.5&38.0&37.2&31.3&35.9 \\
26 \hfill [-]&134.8k / 428&31.8&30.8&31.8&31.2&31.9&32.6&32.2&30.5&29.6&31.2 \\
15 \hfill [bank]&136.9k / 674&46.5&51.5&47.9&48.0&46.6&46.0&45.8&45.7&42.9&46.0 \\
4 \hfill [rel]&137.4k / 1016&77.1&85.3&83.5&83.3&75.8&46.1&74.2&73.3&63.2&75.9 \\
2 \hfill [med]&182.6k / 85&70.6&75.8&71.7&69.4&68.2&67.3&67.3&68.6&65.6&68.2 \\
20 \hfill [med]&183.0k / 71&47.4&47.2&46.8&47.2&48.4&47.5&48.8&47.3&47.1&46.8 \\
21 \hfill [-]&222.8k / 868&38.7&38.8&39.0&37.2&37.5&35.9&36.9&37.1&33.4&37.0 \\
10 \hfill [med]&225.4k / 115&40.0&42.6&40.0&38.2&39.9&35.8&39.5&39.1&36.3&40.7 \\
18 \hfill [med]&245.0k / 106&57.7&60.3&58.7&58.6&58.4&56.3&57.3&56.1&54.9&55.9 \\
9 \hfill [med]&301.6k / 145&37.2&37.3&36.5&36.1&36.4&37.7&36.4&35.2&34.2&37.0 \\
3 \hfill [law]&323.5k / 680&50.1&52.0&50.8&50.1&49.1&48.3&49.0&48.2&44.4&49.1 \\
14 \hfill [med]&334.0 / 146&31.6&31.4&31.9&33.0&32.5&34.1&31.4&32.1&30.5&31.8 \\
12 \hfill [med]&356.4k / 148&36.3&36.6&35.9&35.9&35.8&37.0&36.4&35.4&34.2&36.3 \\ \hline
  \end{tabular}
  \caption{Complete results for the experiments with automatic domains. For each cluster, we report: the majority domain when one domain accounts for more than 75\% of the class; training and test sizes; and BLEU scores obtained with the various systems used in this study. Most test sets are too small to report significance tests.}
  \label{tab:automatic_domains}
\end{table*}

This experiment aims to simulate with automatic domains a scenario where the number of ``domains'' is large and where some ``domains'' are close and can effectively share information. Full results in Table~\ref{tab:automatic_domains}. Cluster size vary from approximately 8k sentences (cluster~24) up to more than 350k sentences. More than 2/3 of these clusters mostly comprise texts from one single domain, as for cluster 12 which is predominantly \domain{med}, the remaining clusters typically mix 2 domains. Fine-tuning with small domains is often outperformed by other MDMT techniques, an issue that a better regularization strategy might mitigate. Domain-control (\system{DC-Feat}) is very effective for small domains, but again less so in larger data conditions. Among the MD models, approaches using residual adapters have the best average performance.

\chapter{D - Generalized Multi-Domain Dynamic Adaptation Curriculum Algorithm (Chapter \ref{chap:mdac})}
\label{appendix:d}
The pseudo-code for the generalized Multi-Domain Adaptation Dynamic Sampling Algorithm is in Algorithm~\ref{alg:mdmt}.
\chapter{E - Experiments with automatic domains (Chapter \ref{chap:mdac})}
\label{appendix:e}
\begin{table*}[htbp]
  \centering
  \footnotesize
  \begin{tabular}{|p{0.5cm}|*{7}{c|}|*{6}{c|}} 
  \hline
\multirow{2}*{Cl. }&\multirow{2}*{size}&\multicolumn{6}{c||}{Domain content}&\multicolumn{6}{c|}{MDAC systems}\\
  \cline{3-14}
&&MED&ECB&IT&LAW&REL&TALK&MED&ECB&IT&LAW&REL&TALK \\
\hline
1&27436&0.24&0.11&0.47&0.17&0.00&0.01&0.01&0.00&0.01&0.00&0.00&0.01 \\
2&68108&0.48&0.04&0.19&0.28&0.00&0.00&0.02&0.03&0.02&0.01&0.02&0.02 \\
3&182594&1.00&0.00&0.00&0.00&0.00&0.00&0.03&0.06&0.04&0.06&0.03&0.03 \\
4&323474&0.05&0.06&0.01&0.87&0.00&0.00&0.03&\underline{0.07}&0.03&\underline{0.20}&0.03&0.04 \\
5&137451&0.03&0.00&0.00&0.00&0.86&0.10&0.03&0.04&0.04&0.06&\underline{0.17}&0.04 \\
6&109949&0.44&0.04&0.40&0.07&0.01&0.04&0.04&0.06&0.04&0.05&0.02&0.03 \\
7&133395&0.92&0.01&0.01&0.05&0.00&0.00&0.03&0.04&0.03&0.05&0.05&0.03 \\
8&91464&0.98&0.00&0.00&0.02&0.00&0.00&0.04&0.03&0.04&0.04&0.05&0.03 \\
9&61353&0.02&0.01&0.96&0.01&0.00&0.00&0.03&0.02&0.03&0.01&0.04&0.02 \\
10&301639&0.98&0.00&0.00&0.02&0.00&0.00&0.02&0.01&0.01&0.01&0.02&0.03 \\
11&225347&0.93&0.00&0.01&0.04&0.00&0.01&0.03&0.04&0.04&0.02&0.03&0.04 \\
12&92982&0.98&0.00&0.01&0.01&0.00&0.00&0.04&0.02&0.03&0.01&0.03&0.03 \\
13&356377&0.99&0.00&0.00&0.01&0.00&0.00&0.03&0.03&0.04&0.03&0.03&0.04 \\
14&17260&0.03&0.37&0.01&0.59&0.00&0.00&0.03&0.03&0.03&0.02&0.03&0.02 \\
15&333957&0.98&0.00&0.01&0.01&0.00&0.00&0.04&0.02&0.03&0.01&0.03&0.03 \\
16&136944&0.02&0.89&0.01&0.08&0.00&0.00&0.04&\underline{0.08}&0.03&0.04&0.03&0.04 \\
17&30443&0.96&0.01&0.02&0.01&0.00&0.00&0.04&0.03&0.03&0.03&0.04&0.03 \\
18&54378&0.93&0.00&0.05&0.02&0.00&0.00&0.04&0.04&0.02&0.03&0.07&0.03 \\
19&245000&0.99&0.00&0.00&0.00&0.00&0.00&0.04&0.04&0.03&0.03&0.04&0.03 \\
20&27227&0.15&0.00&0.79&0.05&0.00&0.01&0.04&0.03&0.03&0.02&0.04&0.03 \\
21&182990&0.99&0.00&0.00&0.01&0.00&0.00&0.03&0.02&0.03&0.03&0.03&0.03 \\
22&222802&0.21&0.01&0.40&0.04&0.02&0.32&0.04&0.04&\underline{0.08}&0.02&0.01&\underline{0.08} \\
23&27534&0.11&0.48&0.02&0.39&0.00&0.00&0.03&0.03&0.05&0.02&0.01&0.04 \\
24&47065&0.99&0.00&0.01&0.00&0.00&0.00&0.04&0.02&0.05&0.05&0.03&0.04 \\
25&8129&0.95&0.00&0.04&0.01&0.00&0.00&0.03&0.03&0.04&0.03&0.02&0.02 \\
26&28237&0.59&0.02&0.23&0.11&0.00&0.05&0.03&0.02&0.02&0.01&0.03&0.02 \\
27&134828&0.53&0.00&0.02&0.00&0.01&0.44&0.04&0.03&0.03&0.01&0.01&0.05 \\
28&109324&0.99&0.00&0.00&0.01&0.00&0.00&0.04&0.03&0.03&0.02&0.01&0.02 \\
29&25561&0.56&0.16&0.08&0.16&0.00&0.04&0.03&0.03&0.03&0.02&0.02&0.03 \\
30&109260&0.01&0.06&0.00&0.92&0.00&0.00&0.03&0.03&0.03&0.06&0.04&0.04 \\
\hline
\end{tabular}
\caption{Automatic clustering experiments. We report the size of each cluster. In the 6 left columns, each line gives the proportions of the domains in each cluster. In the 6 right columns, each column corresponds to a MDAC experiment; each line gives the cumulated proportion of the corresponding cluster in the training data. For instance, when targeting the domain \domain{ecb}, cluster~4 (mostly \domain{law}) is sampled with a probability of $0.07$, and cluster~16 (mostly \domain{ecb}) is sampled with probability $0.08$. For each system, we underline the most often sampled clusters.}
\label{tab:automatic_domains-appendix}
\end{table*}

This experiment aims to simulate with automatic domains a scenario where the number of ``domains'' is large and where some ``domains'' are close and can effectively share information. Full results are in Table~\ref{tab:automatic_domains-appendix}. Cluster sizes vary from approximately 8k sentences (cluster~24) up to more than 350k sentences. More than 2/3 of these clusters mostly comprise texts from one single domain, as for cluster 12, which is predominantly \domain{med}, the remaining clusters typically mix 2 domains. According to the table, for each system, the clusters most sampled by MDAC contain most of the data of the corresponding domain. This demonstrates that MDAC is able to find related data to the task automatically. However, MDAC performance is still far behind that of the supervised scenario, as we explain in the paper. 
\\
\\
\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{breakablealgorithm}
\caption{Multi-Domain Adaptation Dynamic Sampling} \label{alg:mdmt}
\begin{algorithmic}[1]
\REQUIRE {
  \begin{itemize}
    \setlength{\itemsep}{-1pt}    \setlength{\parsep}{0pt}
  \item[] 
  \item $n_d$ corpora $C^d, d \in [1, \dots,n_d]$ for $n_d$ domains equipped by an empirical distribution $D_d(x)$
  \item $n_d$  dev sets $Dev^d, d\in [1, \dots,n_d]$ for $n_d$ domains.
  \item Domain testing distribution $\vlambda^t \in \mathbb{R}^{n_d}$
  \item Batch size $B$
  \item Domain Dynamic Sampling Distribution $\lambda^l_i \in \mathbb{R}^{n_d}$ where $i$ indexes iterations.
  \item $Eval\_scores = []$
  \item $Early\_stopping$ criterion
  \item Total training iterations $iter\_num$
  \item Update rule for sampling distribution $\vlambda^l_i$
 \end{itemize}}
\REPEAT 
\STATE{// Start of iteration i}
\STATE{Randomly pick $d \in [1,\dots,n_d]$ from sampling distribution $\lambda^l_i$}
\STATE{Sample $B$ sentences from $C^d$ with empirical distribution $D_d(x)$}
\STATE{Update model by applying SGD computed from $B$ sampled sentences}
\IF{$i \equiv 0 \mod{eval\_step}$}
	\STATE{Evaluate current model with $n_d$ dev sets. $S^d_i$ is the performance at iteration $i^{th}$ on domain $d$}
	\STATE{Report weighted score using test distribution $\vlambda^t$. $$eval(i) = \displaystyle{\mathop{\sum}_d^{n_d} \lambda^t(d) S^d_i}$$}
	\STATE{$Eval\_scores.append(eval(i))$}
\ENDIF
\IF{$i \equiv 0 \mod{sampler\_updating\_step}$}
	\STATE Update $\vlambda^l_i$
\ENDIF
\IF{$Early\_stopping(Eval\_scores)$}
	\STATE{break}
\ENDIF
\STATE{$i=i+1$}
\UNTIL{$i> iter\_num$}
\end{algorithmic}
\end{breakablealgorithm}
\chapter{F - Résumé en Français}
\section*{Introduction}
Un modèle de traduction automatique neuronale (NMT) a généralement du mal à traduire des phrases dont le genre, le registre ou le thème diffèrent de ceux des phrases utilisées pour l'entraînement du modèle. Il s'agit d'une faiblesse des méthodes d'apprentissage automatique axées sur les données, dont les performances sont garanties en supposant que les distributions d'entraînement et de test soient identiques. Par conséquent, pour obtenir des performances élevées dans un domaine cible, nous devons soigneusement adapter le modèle NMT à ce domaine. Le problème de l'adaptation d'un modèle NMT à un domaine cible est appelé le \SB{problème d'adaptation au domaine}. Deux facteurs rendent ce problème complexe, notamment la pénurie de données d'entraînement du domaine cible et le problème d'oubli catastrophique des modèles neuronaux. Le manque de données d'entraînement nous motive à exploiter des données parallèles provenant d'autres domaines pour entraîner nos modèles NMT. D'ailleurs, les modèles basés sur les réseaux neuronaux ont besoin de nombreuses données pour optimiser leurs paramètres. Par conséquent, nous devons généralement adapter notre modèle NMT au domaine cible en utilisant de nombreuses données hors domaine et une petite quantité de données du domaine cible. Deuxièmement, malgré les améliorations importantes plusieurs approches visant à adapter un modèle NMT en l'affinant à l'aide des données dans le domaine cible ont des performances très faibles au test hors du domaine. Ce problème est appelé \SB{oubli catastrophique} dans la littérature de recherche sur les réseaux neuronaux. Les modèles neuronaux ont tendance à être nettement moins performants dans les tâches précédentes après avoir été formés pour effectuer leurs tâches actuelles. Dans les applications réelles, nous cherchons généralement à améliorer les performances dans le domaine cible et la robustesse des modèles neuronaux par rapport aux domaines connus précédemment.
\section*{Contributions}
Dans cette thèse, nos contributions sont les suivantes.

Premièrement, nous formalisons le problème de l'adaptation aux multi-domaines de la traduction automatique. Nous mettons en évidence quatre situations principales dans le problème de l'inadéquation des domaines. Nous fournissons une correspondance complète entre chaque cas et ses méthodes réalisables.

Ensuite, nous proposons une nouvelle évaluation multicritères pour les méthodes de traduction automatique (multi)domaines. Nous réévaluons un large ensemble de méthodes avec les paramètres expérimentaux correspondant aux critères que nous proposons.

Troisièmement, nous proposons, évaluons et analysons une méthode de TA multi-domaine bon marché, qui utilise l'intégration de mots épars avec des unités spécifiques au domaine. Cette méthode est beaucoup moins chère que les adaptateurs résiduels. Outre une amélioration dans certains contextes multi-domaines légers, la méthode peut gérer un nombre croissant de domaines. Nous étendons l'idée de la représentation éparse aux couches supérieures d'un modèle NMT. Nous démontrons que la performance de la méthode est équivalente à celle de plusieurs méthodes MDMT fortes. Nous proposons une nouvelle méthode d'analyse pour les incorporations de mots, qui identifie les tokens agnostiques et spécifiques au domaine en observant la variation des K voisins les plus proches d'un token tout en changeant son domaine.

Notre quatrième contribution est une étude approfondie de l'utilisation des adaptateurs résiduels dans l'adaptation (multi)domaine. Nous démontrons son caractère pratique et ses bonnes performances dans un contexte de TA multi-domaine composé d'un grand nombre de domaines aux tailles déséquilibrées. Nous proposons différentes méthodes de régularisation pour éviter le surajustement sur les domaines à faibles ressources. Enfin, nous proposons deux variantes plus robustes qui sont robustes par rapport aux erreurs d'étiquettes de domaine et réduisent légèrement le coût de calcul.

Ensuite, nous étudions des stratégies d'échantillonnage dynamique pour la traduction automatique multi-domaine. Nous montrons que ces méthodes améliorent l'échantillonnage des données à partir du mélange de corpus dans le domaine par rapport à la stratégie heuristique d'échantillonnage fixe. De plus, nous démontrons leur efficacité dans plusieurs contextes particuliers tels que l'adaptation uni-domaine, l'adaptation bi-domaine et l'adaptation à un domaine non vu.

Enfin, nous étudions deux paradigmes populaires pour les domaines de test inconnus qui reposent sur la recherche de texte. Ces techniques recherchent les traductions les plus similaires et incorporent ces informations supplémentaires dans la prédiction d'un modèle NMT. Nous démontrons leur efficacité ainsi que leurs faiblesses. En outre, nous proposons une variante simple qui améliore légèrement les performances des techniques précédentes et qui est capable d'exploiter les traductions de synthèse.

\section*{Conclusion et travail futur}

\afterpage{\null\newpage}