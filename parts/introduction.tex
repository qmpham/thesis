\chapter{Introduction}
\section{Motivation}
A neural machine translation (NMT\nomenclature[nmt]{NMT}{Neural machine translation}) model usually has trouble translating sentences that are less similar to the sentences used for training the model. This is a common limitation of the data-driven machine learning methods, whose performance is guaranteed by assuming that the training and testing distribution are identical. Therefore, to achieve high performance in a given domain, we must carefully tailor the NMT model to the domain. The problem of tailoring an NMT model to a target domain is referred to as the domain adaptation problem. Two factors make this problem so complex, including the scarcity of training data from the target domain and the catastrophic forgetting problem of the deep models. The lack of training data drives us to leverage parallel data from other domains to train our NMT model. The neural network-based models need many data to optimize their parameters. Therefore, we usually have to adapt our NMT model to the target domain using lots of out-of-domain data with a small amount of data from the target domain. Secondly, several approaches to adapting an NMT model by finetuning it with the in-domain data only make its performance very brittle to the out-of-domain test. This problem is referred to as catastrophic forgetting in training neural networks. The neural models tend to perform dramatically worse in previous tasks after being trained to perform their current tasks. In real applications, we usually aim to significantly improve the performance in the target domain and the robustness of NMT models against previous training domains.

The domain problem is addressed by domain adaptation approaches as long as the testing domain is known before training. However, in many applications, such as online translation as Google Translate, the text to translate can be from any domain. We consider this situation "non domain-deterministic testing" in our review chapter(\ref{chap:mdmt-review}). In this situation, we could build domain-expert models for the source domains and combine their predictions during the inference \citep{Saunders19domain} to get a domain-adapted translation on the fly. Besides the mixture of model paradigm, several methods use context to improve the translation of similar sentences. However, these methods do not guarantee domain robustness against the out-of-domain text.

Moreover, in real applications, an MT engine has to translate text from many domains whose genre and topic are highly variable. The strategy "one specialized NMT model per domain" will cost us largely when the number of domains explodes. Therefore, developing a multi-domain NMT system is essential for the MT business.

This thesis aims to provide a complete overview of the multi-domain adaptation problem in machine translation and study the approaches to adapting an NMT system to many domains with a small computation and storage cost.

\section{Contributions}
In this thesis, our contributions are as follows.

First, we provided a generalized framework of machine translation (multi-)domain adaptation. We pointed out four principal situations in the domain mismatch problem. We provided a complete match between each case and its feasible methods.

Secondly, we provided a new multi-criteria evaluation for MT (multi-)domain methods. We reevaluated a large set of methods with our proposed experimental settings corresponding to our proposed criteria.

Thirdly, we proposed a cheap MT multi-domain method, which uses sparse word embedding with domain-specified units.

Our fourth contribution is a thorough study on the use of the residual adapters in (multi-)domain adaptation. We demonstrate its convenience and strong performance in a multi-domain MT setting consisting of a large set of domains with unbalanced sizes. We proposed different regularization methods to avoid overfitting on small domains. Finally, we proposed two more robust variants against the domain label error and slightly reduced the computation cost.

Next, we improved the data sampling from the mix of in-domain corpora by replacing the heuristic temperature-based sampling strategy with dynamic parameterized sampling. We proved the performance of our novel approach against several contrasting methods in many experimental settings.

Finally, we proposed a novel method that incorporates similar translations to the context of the translation. Our approach largely improved the quality of the translation.

\section{Outline}
The structure of our thesis is as follows.

The second chapter \ref{chap:nmt-review} is a review of neural machine translation. The chapter provides basic knowledge in the text processing for neural machine translation systems, neural architectures for machine translation, the training procedure, and the inference procedure of neural machine translation models.

The third chapter \ref{chap:mdmt-review} reviews the literature of (multi-)domain adaptation in machine translation. We introduce four main sub-problems of (multi-)domain adaptation and provide an overview of the approaches for each sub-problem.

The remainder of the thesis consists of our original work. Chapter \ref{chap:revisiting} proposes a novel multi-criteria evaluation of multi-domain machine translation systems. We reevaluated a large set of model-centric approaches using a relatively large collection of domains. Chapter \ref{chap:ldr} proposes a (multi-)domain NMT system with cheap computation cost by using a sparse word embedding that nullifies a number of domain-specific units. Chapter \ref{chap:res} proposes several approaches to regularize the residual adapters \citep{Bapna19simple} in the (multi-)domain setting. In this chapter, we propose two variants of the residual adapter that allow to modularize the domain-agnostic and domain-specific encoding and to improve the performance of the adapted NMT model against out-of-domain examples. In chapter \ref{chap:mdac}, we study different dynamic sampling approaches for training the NMT model in the (multi-)domain setting. We propose a novel method that automatically iteratively adapts the sampling distribution to any pre-determined testing distribution. Chapter \ref{chap:priming} proposes a novel method for non-domain-deterministic test sets by reusing similar translation memories.

Finally, in chapter \ref{chap:conclusion}, we draw conclusions on the progress of each main problem in (multi-)domain adaptation machine translation. In addition, we propose a new direction to improve the domain-robustness of NMT systems.


























