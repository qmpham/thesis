\chapter{Introduction}
\section{Motivation}
A neural machine translation (NMT\nomenclature[nmt]{NMT}{Neural machine translation}) model usually has trouble translating sentences that are less similar to the sentences used for training the model. This is a common limitation of the data-driven machine learning methods, whose performance is guaranteed by assuming that the training and testing distribution are identical. Therefore, to achieve high performance in a given domain, we must carefully tailor the NMT model to the domain. Moreover, in real applications, an MT engine has to translate text from many domains, whose genre and topic are highly variable, making the strategy "one specialized NMT model per domain" costly in computation and storage. This thesis aims to study the approaches to adapting an NMT system to many domains with a small computation and storage cost.

The problem of tailoring an NMT model to a target domain is referred to as the domain adaptation problem. Two factors make this problem so complex, including the scarcity of training data from the target domain and the catastrophic forgetting problem of the deep models. The lack of training data effectively forces us to leverage non-related parallel data from other domains to train our NMT model as the neural network-based models need many data to optimize their parameters. Therefore, we usually have to adapt our NMT model to the target domain using lots of out-of-domain data with a small amount of data from the target domain. Secondly, several approaches to adapting an NMT model by finetuning it with the in-domain data only make the model's performance very brittle to the out-of-domain test. This problem is referred to as catastrophic forgetting in training neural networks. The neural models tend to perform dramatically worse in previous tasks after being trained to perform their current tasks. Despite the significant improvement in the performance in the target domain, the robustness of NMT models against previous training domains is always desirable.

The domain problem is addressed by domain adaptation approaches as long as the testing domain is known before training. However, in many applications, such as online translation as Google Translate, the text to translate can be from any domain. We consider this situation "non domain-deterministic" in our review chapter(\ref{chap:mdmt-review}). Non-domain-deterministic and domain robustness share a common goal is to scope with unfamiliar testing sentences. But non-domain-deterministic also aims to achieve higher translation quality in testing cases similar to the training data. According to our study on the domain-related problems in machine translation, this situation is not yet fully studied. Several methods use context to improve the translation of similar sentences but omit the domain robustness and vice versa.
\section{Contributions}
In this thesis, our contributions are as follows.

First, we provided a generalized framework of machine translation (multi-)domain adaptation. We pointed out four principal situations in the domain mismatch problem. We provided a complete match between each situation and its feasible methods.

Secondly, we provided a new multi-criteria evaluation for MT (multi-)domain methods. We reevaluated a large set of methods with our proposed experimental settings corresponding to our proposed criteria.

Thirdly, we proposed a cheap MT multi-domain method, which uses sparse word embedding with domain-specified units.

Our fourth contribution is a thorough study on the use of the residual adapters in (multi-)domain adaptation. We demonstrate its convenience and strong performance in a multi-domain MT setting consisting of a large set of domains with unbalanced sizes. We proposed different regularization methods to avoid overfitting on small domains. Finally, we proposed two more robust variants against the domain label error and slightly reduced the computation cost.

Next, we improved the data sampling from the mix of in-domain corpora by replacing the heuristic temperature-based sampling strategy with dynamic parameterized sampling. We proved the performance of our novel approach against several contrasting methods in many experimental settings.

Finally, we proposed a novel method that incorporates similar translations to the context of the translation. Our approach largely improved the quality of the translation.

\section{Outline}

