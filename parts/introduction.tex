\chapter{Introduction}
\section{Motivation}
A neural machine translation (NMT\nomenclature[nmt]{NMT}{Neural machine translation}) model usually has trouble translating sentences that differ in genre, register or theme from the sentences used for training the model. This is a common limitation of data-driven machine learning methods, whose performance is guaranteed by assuming that the training and testing distributions are identical. Therefore, to achieve high performance in a given domain, we must carefully tailor the NMT model to that domain. The problem of tailoring an NMT model to a target domain is referred to as the \SB{domain adaptation problem}. Two factors make this problem complex, including the scarcity of training data from the target domain and the catastrophic forgetting problem of the deep models. The lack of training data drives us to leverage parallel data from other domains to train our NMT models. The neural network-based models need many data to optimize their parameters. Therefore, we usually have to adapt our NMT model to the target domain using lots of out-of-domain data and a small amount of data from the target domain. Second, several approaches to adapting an NMT model by finetuning it with the in-domain data only make its performance very brittle to the out-of-domain test. This problem is referred to as \SB{catastrophic forgetting} in the neural network literature. The neural models tend to perform dramatically worse in previous tasks after being trained to perform their current tasks. In real applications, we usually aim to significantly improve the performance in the target domain and the robustness of NMT models with respect to previous training domains.

The domain problem is addressed to some extent by domain adaptation approaches as long as the testing domain is known before training. However, in many applications, such as online translation on the web, the text to translate can be from any domain. We consider this situation "non domain-deterministic testing" in our review \ref{chap:mdmt-review}. In this situation, we could build domain-expert models for the source domains and combine their predictions during the inference \citep{Saunders19domain} to get a domain-adapted translation on the fly. Besides the mixture of model paradigm, several methods use context to improve the translation of similar sentences. However, these methods do not guarantee domain robustness against the out-of-domain text.

Moreover, in real applications, an MT engine has to translate text from many domains whose genre and topic are highly variable. The strategy "one specialized NMT model per domain" will cost us largely when the number of domains explodes. Therefore, developing a multi-domain NMT system is essential for the MT business.

This thesis aims to provide a complete overview of the multi-domain adaptation problem in machine translation and study the approaches to adapting an NMT system to many domains with a small computation and storage cost.

\section{Contributions}
In this thesis, our contributions are as follows.

First, we provide a generalized framework of the machine translation (multi-)domains adaptation problem. We point out four main situations in the domain mismatch problem. We provide a complete match between each case and its feasible methods.

Second, we provide a new multi-criteria evaluation for MT (multi-)domains methods. We reevaluate a large set of methods with our proposed experimental settings corresponding to our proposed criteria.

Thirdly, we propose, evaluate and analyze a cheap MT multi-domain method, which uses sparse word embedding with domain-specified units. The method adds less than one million additional parameters per domain, which are much less than residual adapters. Beside an improvement in multi-domain performance, we propose a novel analysis method for word embeddings, which identifies domain-agnostic tokens and domain-specific tokens, by observing the variation of K nearest neighbors of one token while changing its domain.

Our fourth contribution is a thorough study on the use of the residual adapters in (multi-)domain adaptation. We demonstrate its practicality and strong performance in a multi-domain MT setting consisting of a large set of domains with unbalanced sizes. We propose different regularization methods to avoid overfitting on low-resourced domains. Finally, we propose two more robust variants that are robust with respect to the domain label errors and slightly reduce the computation cost.

Next, we improve the data sampling from the mix of in-domain corpora by replacing the heuristic temperature-based sampling strategy with dynamic parameterized sampling. We demonstrate its effectiveness against several strong baselines in many experimental settings.

Finally, we propose a novel method that incorporates similar translations to the translation context. Our approach largely improve the quality of the translation.

\section{Outline}
The structure of our thesis is as follows.

Chapter \ref{chap:nmt-review} is a review of neural machine translation. The chapter provides basic knowledge in the text processing for neural machine translation systems, neural architectures for machine translation, the training, and the inference procedures of neural machine translation models.

Chapter \ref{chap:mdmt-review} reviews the literature of (multi-)domain adaptation in machine translation. We introduce four main sub-problems of (multi-)domain adaptation and provide an overview of the approaches for each sub-problem.

The remainder of the thesis consists of our original work. Chapter \ref{chap:revisiting} proposes a novel multi-criteria evaluation for multi-domain machine translation systems. We reevaluated a large set of model-centric approaches using a relatively large collection of domains. Chapter \ref{chap:ldr} proposes a (multi-)domain NMT system with cheap computation cost by using a sparse word embedding that nullifies a number of domain-specific units. Chapter \ref{chap:res} proposes several approaches to regularize the residual adapters \citep{Bapna19simple} in the (multi-)domain setting. In this chapter, we propose two variants of the residual adapter that allow us to modularize the domain-agnostic and domain-specific representation and to improve the performance of the adapted NMT model against out-of-domain examples. In chapter \ref{chap:mdac}, we study multiple dynamic sampling approaches for training the NMT model in the (multi-)domain setting. We propose a novel method that automatically iteratively adapts the sampling distribution to any pre-determined testing distribution. Chapter \ref{chap:priming} proposes a novel method for non-domain-deterministic test sets by reusing similar examples retrieved from translation memories.

Finally, in chapter \ref{chap:conclusion}, we draw conclusions on the progress of each main problem in (multi-)domain adaptation machine translation. In addition, we propose new directions to improve the domain-robustness of NMT systems.


























