\chapter{MT (multi-)domain adaptation MT's review}
One of the objectives of this thesis is to reemphasize and reevaluate a long history literature of MT (multi-)domain adaptation, which has been actively studied for almost two decades and is still being explored to better communicate many languages in many life aspects. By "standing on the shoulders of giants", we consider our work as an extension of the work of many researchers. In this chapter, we would like to do our best to capture a thorough review of MT (multi-)domain adaptation.

Machine Translation has applications in many life aspects, such as translating legal documents, news, scientific documents, books, movie subtitles, etc. Every domain has its specific vocabulary, registers(formal or informal), and writing style (e.g., preferring active voice over passive voice). Data-driven MT models (SMT, NMT) trained with related data always perform much better in the domain of interest than ones trained with the same amount of less relevant data \citep{Rico13domain, Saunders21domain}. Therefore, tailoring MT models to the target domain is very important. Domain adaptation is to build an ultimate MT system for one target domain. For data-driven systems such as SMT and NMT, the more domain-relevant data is available, the better the MT system performs in the target domain. The research on domain adaptation focuses on better tailoring the MT system to a target domain in diverse situations involving the scarcity of in-domain data. For example, when the in-domain parallel data is very limited or even non-available, and there is a large amount of parallel data collected from other domains, which can be valuable thanks to the proximity between domains. The multi-domain adaptation problem is a generalized problem from domain adaptation as the MT system is optimized for many domains of interest. The problem is interesting because, as same as domain adaptation, the leverage of data from other domains is sometimes valuable thanks to the proximity between other domains and the target domain. Furthermore, having one ultimate MT system for many domains saves us largely the deploying time, the training time, and the storage space.
\subsection{What is a domain?}
While many approaches have been explored to deliver better domain resolution for MT systems, the characteristics of a domain are hardly discussed. The importance of how domains differ is usually underestimated and crudely accessed in almost all research works. \cite{Wees15Whats,Wees17Whats} access the variability of topic and genre across different domains and how they affect the MT system's quality. We find this work both essential and interesting in MT domain adaptation research. Effectively, we need to know which factors of a domain that matter in domain adaptation in order to better clarify the objectives of our methods and evaluate our methods.

\section{From domain adaptation MT to multi-domain adaptation MT}
Multi-domain adaptation and domain adaptation are birds of a feather. The former problem is the generalized version of the latter one. 

A typical setting in machine translation (MT) is to collect the largest possible collection of parallel data for the chosen language pair, with the intent to achieve optimal performance for the task of interest. In such situations, the training data distribution is opportunistic, while the test data distribution is chosen and fixed; a key component in training is then to mitigate the detrimental effects of a possible mismatch between these distributions. Single-source and multi-source\footnote{In this paper, multi-source DA means having multiple domains to adapt from and should not be confused with multi-source translation, where several \emph{source languages} are considered.} domain adaptation (DA) is a well-studied instance of this setting (see \citet{Chu17comparison,Saunders21asurvey} for a review), and so is multi-domain (MD) learning \citep{Chu18multilingual,Zeng18multidomain,Jiang19multidomain,Pham21revisiting}. A related situation is multilingual machine translation \citep{Firat16multiway,Ha16towards,Johnson17google,Aharoni19massively},
where the heterogeneity of training data not only corresponds to variations in the topic, genre, or register but also in language.

\section{Multi-facet of MT (multi-)domain adaptation problem}
Recently the work of \citet{Chu18survey} significantly captured the landscape of MT domain adaptation by categorizing methods in 2 main classes: data-centric and model-centric. More precisely, the data-centric category includes methods that manipulate the training distribution to better resemble to the distribution of the target domain. Model-centric methods improve the internalization of domain dependence by alternating the different properties of NMT models, including the architecture, the optimization procedure, and the inference procedure. This taxonomy is largely adopted in MT domain adaptation's research. However, we find a naivety in this classification as it misses to deliver an answer for the most ultimate question that is "which method solves which problem?". We propose to add another dimension, which is the context of MT adaptation problem, to form a Cartesian map of the MT multi-domain adaptation area. We believe that this coupling of methods and problem contexts will allow us to better navigate in this forest that has been growing for almost two decades, to put more research on unexplored adaptation problems or to find another application of an existing method.

MT (multi-)domain adaptation is indeed a rich landscape where many problems have not been given a shot. The work of \citet{Pham21revisiting} recently gave an attempt of formulating the context of (multi-)domain adaptation problems. In this work, we would like to categorize the domain adaptation context by two following questions.
\begin{itemize}
	\item Is the domain label is available in training?
	\item Is the domain label is available in testing?
\end{itemize}

The domain label is usually extracted from the metadata of the corpora. A domain label can be the name of the field from which the text is collected or just the name of the corpora. Domain labeling depends on the source and the clustering strategy of the researcher. For example, Ubuntu and Gnome can be two different domains or mixed into one domain. The separation facilitates the capturing of the subtle unique properties of the text but loses the generalization, and on the contrary, the combination helps the generalization but loses the in-domain uniqueness. We envy such (multi-)domain adaptation method that is robust to any domain labeling strategy. Domain label is important for almost all adaptation methods. However, this additional information is not always available. Effectively, enormous corpora such as Paracrawl, Commoncrawl, and Newscrawl, which are grossly collected from webs, contain many different text genres and topics but lack domain labels. Exploiting those corpora in (multi-)domain adaptation needs more effort. Unsupervised clustering can be a good direction for this problem \citep{Aharoni20unsupervised,Pham21revisiting}.















































































































































































































































































































































































