\chapter{MT (multi-)domain adaptation's review}
In this chapter, we would like to do our best to give a thorough review of the research literature on the MT (multi-)domain adaptation problem. MT (multi-)domain adaptation has a long history as the area has been actively studied for almost two decades and is still being explored to better communicate many languages in many fields. Earliest domain adaptation methods were explored for SMT systems. Recently, most of the effort has been done to better adapt NMT systems to domains for NMT has surpassed SMT to be state-of-the art architecture.

We divide this chapter into 5 sections. In the first section \ref{sec:domain}, we would like to discuss about how we usually define a domain, how translations differ between domains and the importance of domain adaptation in the real applications. In the second section \ref{sec:multi-facet} we would like to regroup 2 notions domain adaptation and multi-domain adaptation; then divide the large problem MT (multi-)domain adaptation into 4 sub-problems. We dedicate 4 following sections to those sub-problems. In each of 4 following sections, we review groups of methods according to \citet{Chu18survey}, that match the requirement of the corresponding problem.

\section{What is a domain?}
\label{sec:domain}
In the context of binary classification problem, \citet{Shai10A} defined a domain by a pair consisting of a distribution $\mathcal{D}_x$ on inputs $\mathcal{X}$ and a labeling function $f: \mathcal{X} \rightarrow [0,1]$. In machine translation context, the labeling function will be $f: \mathcal{X} \rightarrow \mathcal{Y}$ where $\mathcal{X}$ and $\mathcal{Y}$ are the set of sentences of the source language and the target language respectively. Denote 2 different domains, $\big( \mathcal{D}_S, \mathcal{F}_S \big)$ and $\big( \mathcal{D}_T, \mathcal{F}_T \big)$. Domain adaptation is required when we train a machine learning model (statistical or neural) with data generated by $\big(\mathcal{D}_S, \mathcal{F}_S \big)$ but test it with data generated by $\big(\mathcal{D}_S, \mathcal{F}_S \big)$. In principle, there is no hope that the model works in the second domain. However, we could aim to exploit the sharing knowledge between the 2 domains, for example \citet{Blitzer06Domain} used pivot features, which are features which occur frequently in the two domains and behave similarly in both. The context of machine translation is much more complicated than binary classification as the label is a structured sequence of symbols, that has to satisfy both the adaquecy according to the source sequence and the fluency according to the target language. \cite{Wees15Whats,Wees17Whats} identified the following elements of text, which influence the translation the most.
\begin{itemize}
	\item Topic: subject of the text such as medical, news, IT or religious. A topic owns its own specific vocabulary, terminologies. These items can not be transferred between distant topic such as medical and religious.
	\item Genre: purpose of the text such as education, talk, report or instruction. It identifies groups of texts that share a common form of transmission, purpose and discourse properties. A genre is characterized by textual style, structure of text, etc.
\end{itemize}

Despite many specifications, domains share the same grammar of the corresponding language. The grammar, the textual style or the structure of text are abstract to machine translation models. MT models learn to translate from examples without being explicitly explained how to do it. They are supposed to learn this knowledge to translate but until now, we are not able to tell whether they do it.

Solving the MT (multi-)domain adaptation problem is essential for deploying MT in real context. Machine Translation has applications in many sectors, such as translating legal documents, news, scientific documents, books, movie subtitles, etc. Every domain has its own specific vocabulary, registers(formal or informal), and genres (e.g., talk, instruction). Therefore, tailoring MT models to a target domain is essential to achieve good translation in that domain. In practice, MT models (SMT, NMT) trained with domain-related data always perform much better in the domain of interest than ones trained with the same amount of less relevant data \citep{Rico13domain, Saunders21Domain}. The more domain-relevant data is available, the better the MT system performs in the target domain. However, not every domain has enough data to train an MT models. The state-of-the-art architecture ANMT will need millions parallel sentence pairs to learn its parameters. Therefore, we have to work around in the situation where there is very few data or even no data at all. Domain Adaptation aims to improve performance of MT model in low-resourced domains. Besides, multi-domain adaptation aims to achieve best performance in more than one domain. However, the domains of interest in multi-domain adaptation are not limited to be low-resourced domains. The motivation of having one model adapted to many domains is to optimize the storage, the training time and the deploying time. Having one model per domain increases the storage, the time accessing to a model, thus the latency of the translation. Online translation services, such as Google Translate, Systran Translate or DL Translate, have to translate text from any possible domain while minimizing the latency of translation in order to be beneficial. In conclusion, the variety of text between fields requires domain adaptation while fast and robust translation requires multi-domain adaptation.

\section{Multi-facet of MT (multi-)domain adaptation problem}
\label{sec:multi-facet}
\subsection{From domain adaptation MT to multi-domain adaptation MT}
Even though domain adaptation and multi-domain adaptation do not have same motivation as one focuses on low-resourced domain whereas the other focuses on adapting to as many as possible domains, they can be cast under one general framework. Formally, training instances are distributed according to a mixture $\mathcal{D_S}$ such that $\mathcal{D_S}(x) = \sum_{d=1}^{n_d} \lambda^{s}(d) \mathcal{D}_d(x)$, with $\{\lambda^{s}(d), d=1 \dots n_d\}$ the mixture weights satisfying $\sum_d \lambda^{s}(d)=1$. The target domains are represented in the test distribution which is also a mixture of $\mathcal{D_T}(x) = \sum_{d=1}^{n_d} \lambda^{t}(d) \mathcal{D}_d(x)$, with $\{\lambda^{t}(d), d=1 \dots n_d\}$ the mixture weights satisfying $\sum_d \lambda^{t}(d)=1$. We assume $\mathcal{D}_d(x), d=1 \dots n_d\}$ is the support of the both source and target distribution. Domain adaptation solves the case where $\lambda^s * \lambda^t = 0$ and $\lambda^t$ is one-hot vector while multi-domain adaptation happens to solve the case where $\lambda^t$ is not one-hot vector. We illustrate this formulation in figure %\ref{fig:mdmt-lambdas}.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{graphics/mdmt-lambdas}
  \caption{Training and testing with distribution mismatch. We consider just three domains, and represent vectors of mixture weights $\vlambda^{s}$ and $\vlambda^{t}$ in the 3-dimensional simplex. Training with weights in (a) and testing with weights in (c) is supervised multi-source domain adaptation to domain~2 ($d_2$), while (b)-(c) is the unsupervised version, with no training data from $d_2$; training with weights in (a) and testing with weights in (d) is multi-domain learning, also illustrated with configurations (a)-(e) (training domain $d_1$ is not seen in test), and (b)-(d)  (test domain $d_2$ is unseen in training).}
  \label{fig:mdmt-lambdas}
\end{figure}

This framework modelizes closely the real context because, in a typical setting in machine translation, we collect the largest possible collection of parallel data for the chosen language pair with the intent to achieve optimal performance for the task of interest. In such situations, the training data distribution is opportunistic, while the test data distribution is chosen and fixed; a key component in training is then to mitigate the detrimental effects of a possible mismatch between these distributions. Training data may be a mixture of many domains such as the JRC-Acquis Communautaire corpus (\domain{law}-domain) \citep{Steinberger06acquis} or documentation for KDE, Ubuntu, GNOME and PHP from the Opus collection \citep{Tiedemann09news} (\domain{it}-domain). We also leverage a collection of data with no specific topic and genre, such as Paracrawl \citep{Banon20Paracrawl}. The testing data represent the domains of interest. In multi-domain adaptation, because there are multiple target domains, the testing is composed of multiple tests. The performance of an MT system is usually the average of its performance on these tests. However, we can access the quality of the multi-domain MT system with different priorities over target domains using weighted mean. The evaluation of multi-domain MT system should be clarified before training as early stopping criteria relies on the evaluation over validation tests.
\subsection{4 main sub-problems}
From our study on many previous works, we realize that the (multi-)domain adaptation problem is a multifaceted problem. Effectively, all the cases of (Multi-)domain adaptation can be classified into four groups by answering two following questions:
\begin{itemize}
	\item Is/Are the domain(s) of the training well defined?
	\item Is/Are the domain(s) of the testing well defined?
\end{itemize}
More precisely, first question asks whether the content of the training data is known a priori. For example, the domain of a mixture of multiple corpora of specific topics, such as JRC-Acquis Communautaire corpus (\domain{law}-domain) and KDE (\domain{it}-domain), is well defined. However, the domain of Paracrawl \citep{Banon20Paracrawl} is not well defined as its content is unknown. The second question asks whether the domain of the testing data is well-defined. If there exist a collection of text (monolingual or parallel), that defines the domain of the testing, then its domain is well defined and vice versa. 

Our first case of (multi-)domain adaptation \ref{sec:case1} is supervised multi-domain adaptation in which domain labels are both available in training and testing. Furthermore, the target domains in testing are included in the training. This case might be the easiest situation. Many approaches in multi-domain MT conducted their experiments in this setting \cite{Pham21revisiting}. 

The second case \ref{sec:case2} considers the use of the parallel data crudely collected from webs such as Paracrawl \citep{Banon20Paracrawl} or Commoncrawl \footnote{\url{https://commoncrawl.org/}}. The content of these corpora varies from many topics, but unfortunately, there is not any available domain label for sentences. Fortunately, in the second case, the target domains are known, i.e., there exist data of these domains, which can be used to adapt the model. The second case focuses on the exploitation of opportunistic text. 

The third case \ref{sec:case3} assumes a well domain-labeled training data while allowing the testing sentence from any possible domain. This case focuses on the robustness of the MT system against any possible shift distribution in testing. The third setting is very closed to real applications where the provenance of the input text is unknown. 

Finally, the last setting \ref{sec:case4} focuses on both exploiting opportunistic data in training and being robust against unknown testing distribution. We dedicate the four following sections to discuss more details of each setting.

Recently the work of \citet{Chu18survey} significantly captured the landscape of MT domain adaptation by categorizing previous methods in 2 main classes: data-centric and model-centric. The data-centric category includes methods that manipulate the training distribution to better resemble the distribution of the target domain. The model-centric methods focus on changing the architecture, modifying the training objective and improving the inference.

According to \citet{Chu18asurvey}, the data-centric focuses on 2 paradigms, including: 1) collecting parallel data related to the domain target, 2) creating synthetic data resembling the domain target. The first paradigm searches similar examples to the ones of the domain target in order to enlarge the training data of the domain target. The second paradigm aims to create pseudo examples resembling the data of the domain target. Besides the two paradigms, we propose another paradigm, which is data sampling. Data sampling paradigm consists of changing the data sampling distribution during the training course to better balance the . These methods mitigate the heterogeneity of the size of the data between domains, and also the variety of the "difficulty" of the domains. Therefore data-centric consists of 3 paradigms, including: 1) data collection 2) data synthesization and 3) data sampling.

This taxonomy is largely adopted in MT domain adaptation's research. However, we find a naivety in this classification as it misses to deliver an answer for the most ultimate question that is "which method solves which problem?". We propose adding another dimension, the context of the MT adaptation problem, to form a Cartesian map of the MT (multi-)domain adaptation area. The four following sections will explain how model-centric and data-centric categories solve 4 (multi-)domain adaptation cases. We believe that this coupling of methods and cases will allow us to better navigate in the (multi-)domain adaptation forest that has been growing for almost two decades, to put more research on unexplored adaptation problems, or to find another application of an existing method. The work of \citet{Pham21revisiting} recently gave a brief review of several well-known adaptation methods via a reevaluation with different domain adaptation cases. However, the experiments are still limited in the first case \ref{sec:case1} and the third case \ref{sec:case3} by excluding crawled corpora.

\section{Supervised (multi-)domain adaptation}
\label{sec:case1}
In the supervised (multi-)domain adaptation, the domain label is available in both training and testing. Furthermore, the domain(s) in testing is (are) available in training. In this problem, we would like to achieve a unique model, that performs the best in one or many target domains, given training data from those domains and probably from other domains. This setting is the most popular situation on which multi-domain machine translation research papers focus. The first case represents the least requirement as the MT need to achieve best performance on the domains it learned from. The difficulty of this situation is to both exploit the proximity between domains while mitigate the interference due to inter-domain heterogeneity. Effectively, similar topics, such as legal and administrative, might improve the vocabulary coverage of each other as both domains share the same specific terminologies. However, distant topics, such as religious and IT, might confuse MT system when sharing the same parameters. In the following sections, we will discover how model-centric methods and data-centric methods solve this case.
\subsection{Model-centric}
In the case of supervised multi-domain adaptation, model-centric methods focus on adding domain-specified parameters to reduce the interference between domains while keeping the number of parameters small. The simplest methods use domain tag. For example, \citet{Kobus17domain} proposed appending a special token to each source sequence indicating its domain such as $<Domain=IT>$ and train NMT model with this format. However, this format requires domain tags in testing, hence we have to predict them in case the source sentences are from unknown origin. \cite{Britz17effective} originally proposed appending domain tag to the target sequence so that the decoder will predict the domain in which it will generate the translation. Instead of using domain tag, \citep{Kobus17domain, Pham19generic} proposed using domain embedding to incorporate the domain information to the context of the translation. \citet{Kobus17domain} concatenated an embedding of small size (e.g., 4) to the embedding of each token in the input sequence. Each small embedding corresponds to a domain. Instead of using the same domain embedding for the tokens in the same domain \citet{Pham19generic} used lexicalized domain representation, which is a small embedding correspond to the domain and the token. Besides domain embedding, we can use domain specified layers that can be plugged between 2 consecutive layers of NMT model without changing the architecture. There are 2 types of plug-in layers: 1) residual adapter \citep{Bapna19simple, Pham20Study} and 2) learning hidden unit contribution \citep{Vilar18learning}. Residual adapters were first introduced by \citet{Rebuffi17learning} in computer vision. \citet{Bapna19simple} proposed this fine-tuning paradigm for domain adaptation. They introduced a new version of residual adapter composed of 2 linear projection and the ReLU activation function. The adapters are plugged to the NMT model as follow
\begin{equation}
\begin{array}{rcl}
h_{enc/dec}^l = h_{enc/dec}^{l} + ADAP_{enc/dec}^l(h_{enc/dec}^{l})
\end{array}
\end{equation}
where $ADAP_{enc/dec}^l$ is the adapter corresponding to the $l^{th}$ layer of the encoder/the decoder. \cite{Pham20Study} studied the use of the residual adapters for multi-domain adaptation and propose several techniques, including regularization, gating mechanism, to improve the robustness of the model. The learning hidden unit contribution (LHUC \nomenclature[lhuc]{LHUC}{Learning hidden unit contribution}) method was proposed by \cite{Vilar18learning} to adapt an NMT model to a domain. The author applied LHUC layer to the model as follow
\begin{equation}
h_{enc/dec}^l = h_{enc/dec}^{l} \odot a(\rho^{l}_{enc/dec})
\end{equation}
where $\rho^{l}_{enc/dec}$ is the adapter corresponding to the $l^{th}$ layer of the encoder/the decoder and $\rho^{l}_{enc/dec} \in \mathbb{R}^d$. $a(.)$ is a scaled
element-wise sigmoid function.
$$a(x) = \frac{2}{1+e^{-x}}$$
Residual adapter and learning hidden unit contribution layer are used to adapt a pretrained model without changing its parameters. 

While the previous methods aims to discrimante domains to reduce the interference,\cite{Britz17effective} was motivated to learn hidden representations which are invariant between domains. More precisely, the authors use a binary classifier, that takes the output of the encoder as input, to distinguish the source sequences of 2 source domains. They inverse the sign of the gradient corresponding to the loss of the classifier to confuse it, i.e. making the hidden representation of the encoder invariant between 2 domains. This technique is related to A-distance which is a measure of similarity between two probability distributions. \cite{Ben07analysis} showed that the A-distance between the
source and target distributions is a crucial part of an upper generalization bound for domain adaptation. They hypothesized that it should be difficult to discriminate between the source and target domains in order to have a good transfer between them, because
this would imply similar feature distributions. \cite{Zeng18multidomain}'s work was also inspired by this idea. However, instead of forcing the encoder's output to be invariant between domains, the authors focus on extracting domain-agnostic features and domain-specific features from the output of the encoder, and feeding the features to the decoder. To do that, they use two different non-linear transformations, which map a hidden representation to two vectors of same size. Then, they apply one domain classifier on each extracted feature vector. The domain-agnostic feature vector is trained to confuse its corresponding classifier whereas the domain-specific feature vector is trained to facilitate its classifier.

\cite{Michel18extreme} adapted pretrained model to personalized model by finetuning the
bias of the output softmax to each particular user of the MT system. The number of additional parameters is $|S| \times |\Sigma_y|$, in which $S$ is the set of the users. The author reduce the size of the bias matrix $B \in \mathbb{R}^{|S| \times |\Sigma_y|}$, whose each row is a bias vector of one user, by factoring it into lower dimension representation, i.e.
\begin{equation}
\begin{array}{rcl}
B &=& S \times \tilde{B}, \\
&where& \\
S & \in & \mathbb{R}^{|S| \times r}, \\
\tilde{B} & \in & \mathbb{R}^{r \times |\Sigma_y|}.
\end{array}
\end{equation}

\citet{Jiang20Multi}'s work was inspired by mixture of expert paradigm. Their novel model is based on Transformer architecture \citep{Vaswani17attention} as they integrate a domain-mixing mechanism to the multi-head attention layer. As we explained in Section ~\ref{ssec:transformer-enc}, each head of multi-head attention layer is computed as follow
\begin{equation}
head_i = Attention \big(QW_i^Q, VW_i^V, KW_i^K \big).
\end{equation}
Suppose that there are $K$ domains, at the $i^th$ head, for each query, key and value component, there are $K$ transformation matrix $W_{i,j}^Q | j \in [1,K]$, $W_{i,j}^K | j \in [1,K]$, $W_{i,j}^V | j \in [1,K]$ respectively. The domain-mixing mechanism is applied on query, key and value components as follow
\begin{equation}
\begin{array}{rcl}
Q_i^t &=& \sum_{j=1}^K Q^tW_{i,j}^Q*\mathit{D}_j(x_t),\\
K_i^t &=& \sum_{j=1}^K K^tW_{i,j}^K*\mathit{D}_j(x_t),\\
V_i^t &=& \sum_{j=1}^K V^tW_{i,j}^V*\mathit{D}_j(x_t),\\
head_i &=& Attention \big( Q_i, K_i, V_i\big),
\end{array}
\end{equation}
where the subscript $t$ indicates the position of the vector in the sequence of hidden representations, $\mathit{D}(x_t) \in \mathbb{R}^{K}$ is the domain proportion of the corresponding token $x_t$. The proportion of domains of a token is computed by a domain classifier, that take the input embedding of that token as input. The novel model adapt the hidden representation of a token to a domain to which the token likely belongs. Therefore, the model is able to discriminate domains in domain-specific tokens while transfering knowledge between domains in domain-agnostic tokens. However, the size of the model is proportional to the number of domains, which is not practical in the real applications. However, in our opinion, the idea can be applied to lightweight adapter or learning hidden unit contribution layer.

Besides the model-centric methods related to the architecture, multi-task training is another solution to the supervised (multi-)domain adaptation problem. 

We illustrate several well-known model-centric methods for supervised domain adaptation problem in Figure~\ref{fig:model-centric-case1-case2}.
\begin{figure}[htbp]
\begin{subfigure}{1.0\textwidth}
  \centering
  \includegraphics[width=1.0\textwidth]{graphics/supervised_mdmt}
\end{subfigure}
\newline
\begin{subfigure}{1.0\textwidth}
  \centering
  \fbox{\begin{tabular}{ll}
\textcolor{red}{$\blacksquare$} & \citep{Zeng18multidomain} \\
\textcolor{violet}{$\blacksquare$} & \citep{Vilar18learning} \\
\end{tabular}}
\end{subfigure}
\caption{Each color different from the bleu corresponds to one model-centric method. The bleu represents the NMT model.}
\label{fig:model-centric-case1-case2}
\end{figure}

\subsection{Data-centric}
\subsubsection{Data sampling}
Curriculum learning.


Because data-centric methods usually consider the training as a generic domain, we will discuss how they solve both the first case and the second case in Section ~\ref{ssec:case-2-data}.

\section{Unlabeled training, labeled testing}
\label{sec:case2}
The second case focuses on the adaptation of NMT model with unknown source domain while the target domain is well defined. There are 2 situations: 1) there exist parallel data in the target domain 2) there exist only monolingual data in the target domain. The two following sections will discuss how each group of method solve these cases.
\subsection{Model-centric}
First, we discuss on the case with one target domain in which there exist parallel data. In this case, we can apply the same techniques proposed for supervised (multi-)domain adaptation by considering the domain of the training as a generic domain. Besides, finetuning is very efficient approach for this problem\citep{Luong15stanford,Miceli17regularization,Servan16Domain,Freitag16fast}. We first train an NMT model with the mixture of source domains, then continue training this model with the parallel data of the target domain. According to a recent review of multi-domain adaptation conducted by \citet{Pham20Priming}, finetuning is the strongest baseline in supervised domain adaptation. However, finetuned NMT models usually suffer from catastrophic forgetting \citep{Michael89catastrophic} as their performances drop dramatically in the source domains. To mitigate the catastrophic forgetting, several regularization techniques were introduced, including mixed finetuning \citep{Chu17empirical}, uniform weight-decay \citep{Miceli17regularization}, elastic weight consolidation (EWC \nomenclature[ewc]{EWC}{Elastic weight consolidation}) \citep{Brian19overcoming, Kirk16overcoming, Saunders19domain} and knowledge distillation \citep{Dakwle17fine}. 

Mixed finetuning \citep{Chu17empirical} adapts the NMT model with the mixture of the source domain and the target domain (by oversampling the target domain). The resulted NMT model is specialized in the domain of interest thanks to the oversampling while being robust to generic text as it is trained with the source domain.

Weight decay \citep{Miceli17regularization} continues the training on the target domain's data with a regularized the loss 
\begin{equation}
L_{CE}(\theta,\mathit{D}_{target}) + \alpha * \parallel \theta - \theta^{A} \parallel_{L_2},
\end{equation}
where $\theta^{A}$ is the value of the pretrained model. Finetuning with the new loss fits the model to the target domain while preserving the old pretrained values of the parameters, therefore prevents the overfitting of NMT model in the target domain and maintains the generality over source domains. \citet{Brian19overcoming, Kirk16overcoming, Saunders19domain} were also motivated to penalize the changes of the parameters compared to the old model. However, the authors argued that not every parameter has the same contribution to maintain the generality to the old domains and that parameters unimportant to the old domains can be tuned to the target domain. The contribution of each parameter of the pretrained model is approximated by the diagonal of the Fisher matrix computed over the data of the source domains. Therefore, finetuning with EWC uses the following loss
\begin{equation}
L_{CE}(\theta,\mathit{D}_{target}) + \sum_{i} \frac{\lambda}{2} F_i * (\theta_i - \theta_i^{A})^2,
\end{equation}
where the $F_i$ is the $i^th$ element of the diagonal of the Fischer matrix approximated as follow
\begin{equation}
\bar{F} = \frac{1}{|\mathit{D}_{source}|} \displaystyle{\mathop{\sum}_{(x,y)\in \mathit{D}_{source}}} \nabla log(p(y|x,\theta))_{| \theta^{A}} \nabla log(p(y|x,\theta))_{| \theta^{A}}^{T}
\end{equation}
\citet{Dakwle17fine}'s method was motivated by knowledge distillation paradigm \citep{Hinton15Distilling}. The authors propose regularizing the standard cross-entropy loss with the Kullback-Leibner distance \citep{Kullback51On} between two predicting distributions produced by old model and new model as follow
\begin{equation}
L_{CE}(\theta,\mathit{D}_{target}) + \alpha * \displaystyle{\mathop{\sum}_{(x,y)\in \mathit{D}_{source}}} KL(p(.|x,\theta) | p(.|x,\theta_{A})).
\end{equation}

Instead of continuing the training with target domain only, \citet{Chen17cost} differentiated directly domain-relevant instances and irrelevant instances via weighted cross-entropy loss. The authors compute the weight of each instance by a domain classifier, that is trained with source sequences. The training will maximize the following objective 
\begin{equation}
\begin{array}{rcl}
\hat{\theta} = \displaystyle{\mathop{\arg max}_{\theta} \mathop{\sum}_{(x,y)\in D_{in} \cap D_{out}}} (1+p_d(x))log(P(y|x,\theta))
\end{array}
\end{equation}
where $D_{in}$, $D_{out}$ are the target domain and other domains, $p_d(x)$ is the probability that $x$ comes from the target domain. 

Besides the auxiliary losses, ensemble methods are also promising. For example, \citet{Freitag16fast} proposed ensembling the pretrained model and the finetuned model in order to combine the advantage of both models: the specialization in the target domain and the generalization over general text.

Still in the case of uni-domain adaptation, but without parallel data. The model-centric approaches mostly uses monolingual data in the target language of the target domain. The methods mostly adapt the decoder to the target domain. For example, \citet{Gulcehre16monolingual} proposed training a language model adapted to the target domain and fusing the language model to the decoder. The fusion can be deep or shallow. The deep fusion combines the hidden representation of the decoder and one of the language model before computing the prediction probability. The shallow fusion combines the prediction probability computed by the decoder and the one computed by the language model. \citet{Domhan2017using} was also motivated by this idea. However, the authors proposed jointly training the language model and the NMT model via a multi-task training. Furthermore, the decoder and the language model share the word embedding of the target side.

All previous methods are motivated to adapt an NMT model to a specific domain. We realize that they can be hardly applied to multi-domain adaptation because all the parameters of the MT model are adapted to one domain. However, in the case where there are parallel data of the target domains, model-centric methods proposed in the supervised adaptation could be used by considering the training domain a generic domain. Recently, \cite{Dou19unsupervised} proposed using domain-embedding and task-embedding to adapt an NMT model to the target domain using reconstruction loss on the monolingual data. More precisely, for each layer $l^{th}$ of an ANMT model, there are 2 task embeddings $\theta_{task}^{\gamma,l}$, $\gamma \in \{ MT, LM \}$, which correspond to translation task and language modeling task respectively. Furthermore, for each layer $l^{th}$, and for each domain $d$, there is a domain task $\theta^{d,l}_{dom}$. The layer $l^{th}$ of encoder/decoder will be as follow
\begin{equation}
h^{l} = LAYER^l(h^{l_1}) + \theta^{d,l}_{domain} + \theta_{task}^{\gamma,l}
\end{equation}
Now, the parallel data will be used to compute translation loss while the monolingual data is used to compute language modeling loss. The authors proposing adding noises to the source sequence while computing the LM loss. Using only domain-specific embeddings enables the model to be adapted to multiple domains at once.

\subsection{Data-centric}
\label{ssec:case-2-data}
According to our study, the previous data-centric methods, which aimed to solve this problem, belong to all 3 paradigms, including data selection, data synthesization and data sampling. The two following sections will discuss on the methods of each paradigm.
\subsubsection{Data selection}
Data selection approaches collect parallel data, which resemble the target domain. The selection is usually based on a score of proximity between a parallel example and the domain. The score of proximity can be computed via sentence embedding, or variants of Moore $\&$ Lewis score \cite{Moore10intelligent}. For example, given two corpus of a target domain $D_{I-src}$, $D_{I-tgt}$ and two corpus of the source domain $D_{O-src}$, $D_{0-tgt}$, \cite{Axelrod11domain} computed a bilingual version of Moore $\&$ Lewis score of a sentence pair as follow
\begin{equation}
\begin{array}{rcl}
S_{bi} (x,y) &=& H_{I-src}(x) - H_{O-src}(x) + H_{I-tgt}(y) - H_{O-tgt}(y), \\
\end{array}
\end{equation}
where the cross-entropy $H_{*}(z)_{| * \in [I-src, I-tgt, O-src, O-tgt], z \in [x,y]}$ of the sentence $z$ is computed by a language model trained only with the corpus $D_{*}$. \citet{Duh13adaptation} proposed the same formulation as proposed \citet{Axelrod11domain} but used neural language model instead of statistic language model. In a survey of data selection methods for neural machine translation, \citet{Silva18extracting} evaluated 3 popular methods in the domain adaptation task, including cross-entropy difference, Term Frequency-Inverse Document Frequency (TF-IDF \nomenclature[tf-idf]{TF-IDF}{Term Frequency-Inverse Document Frequency}) \citep{Salton73On} and Feature Decay Algorithm (FDA) \citep{Poncelas18Feature}. More precisely, cross-entropy difference was performed as described above and normalized by sentence length. To perform TF-IDF, \citet{Silva18extracting} consider each sentence of the target domain as a query , and every sentence in the source domain as a key. The tf-idf vectors of the querys and the keys are computed as in \citet{Salton73On}. The score of proximity between a query and a key is the consine-similarity of theirs tf-idf vectors. Based on the score of proximity, for each sentence of the target domain, we retrieve K nearest-neighbors in the source domain. The collection of the retrieved sentences is the result of the method. The last method FDA extract from the source domain a set of sentences which better represent a given test set provided by the source side of the target domain. The detail of the algorithm is referred to the work of \citet{Poncelas18Feature}. 

\cite{Wang17sentence} proposed using sentence embedding to represent a sentence instead of a tf-idf vector. For each language side (source/target) the authors computed the centroid of the target domain and one of the source domain. Assume $C_{E_in}$, $C_{E_out}$ are the centroid of the target domain and the source domain in the source language,  $C_{F_in}$, $C_{F_out}$ are the centroid of the target domain and the source domain in the source language, $v_{\mathit{e}}$ is the sentence embedding of a source sentence $\mathit{e}$, $v_{\mathit{f}}$ is the sentence embedding of a source sentence $\mathit{f}$, then the proximity of the example $(\mathit{e},\mathit{f})$ to the target domains is defined as follow
\begin{equation}
d(v_{\mathit{e}}, C_{E_in}) - d(v_{\mathit{e}}, C_{E_out}) + d(v_{\mathit{f}}, C_{F_in}) - d(v_{\mathit{f}}, C_{F_out}),
\end{equation} 
where $d(.,.)$ is the Euclidean distance in $\mathbb{R}^d$.

\subsubsection{Data synthesization}
The most efficient approach of this paradigm is backtranslation \citep{Sennrich16improving}, which consists of translating the monolingual data of the target language to the source language. \citet{Burlot18using} showed significant improvement of NMT model in the target domain when trained with the mixture of parallel data and the in-domain backtranslated data. Without backtranslating the target-side data, \citet{Currey17copied} simply created artificial sentence pairs from the monolingual data in the target language so that each source sentence is identical to the target sentence. Training an NMT model with the mixture of parallel data and the artificial sentences improves the accuracy of the translation on named entities and other words that should remain identical between the source and target languages. 

\subsubsection{Data sampling}
Data sampling methods change the 
\citet{Wang19dynamically}

\citet{Wees17dynamic}

\section{Labeled training, unlabeled testing}
\label{sec:case3}
\subsection{Model-centric}
Ensemble decoding, mixture-model, domain robustness
\cite{Muller20domain}
\subsection{Data-centric}
Cannot be applied as there is no fixed target domain. 
\section{Unlabeled training, unlabeled testing}
\label{sec:case4}
\subsection{Model-centric}
Distributional robustness
\subsection{Data-centric}
Cannot be applied as there is no fixed target domain. 













































































































































































































































































































































































