\chapter{MT (multi-)domain adaptation's review}
In this chapter, we would like to do our best to give a thorough review of the research literature on the MT (multi-)domain adaptation problem. MT (multi-)domain adaptation has a long history as the area has been actively studied for almost two decades and is still being explored to better communicate many languages in many fields. Earliest domain adaptation methods was explored for SMT systems. Recently, most of the effort has been done to better adapt NMT systems to domains for NMT has surpassed SMT to be state-of-the art architecture.

We divide this chapter into 5 sections. In the first section \ref{sec:domain}, we would like to discuss about how we usually define a domain, how translations differ between domains and the importance of domain adaptation in the real applications. In the second section \ref{sec:multi-facet} we would like to regroup 2 notions domain adaptation and multi-domain adaptation; then divide the large problem MT (multi-)domain adaptation into 4 sub-problems. We dedicate 4 following sections to those sub-problems. In each of 4 following sections, we review groups of methods according to \citet{Chu18survey}, that match the requirement of the corresponding problem.

\section{What is a domain?}
\label{sec:domain}
In the context of binary classification problem, \citet{Shai10A} defines a domain by a pair consisting of a distribution $\mathcal{D_x}$ on inputs $\mathcal{X}$ and a labeling function $f: \mathcal{X} \rightarrow [0,1]$. In machine translation context, the labeling function will be $f: \mathcal{X} \rightarrow \mathcal{Y}$ where $\mathcal{X}$ and $\mathcal{Y}$ are the set of sentences of the source language and the target language respectively. Denote 2 different domains, $\big( \mathcal{D_S}, \mathcal{F_S} \big)$ and $\big( \mathcal{D_T}, \mathcal{F_T} \big)$. Domain adaptation is required when we train a machine learning model (statistical or neural) with data generated by $\big(\mathcal{D_S}, \mathcal{F_S} \big)$ but test it with data generated by $\big(\mathcal{D_S}, \mathcal{F_S} \big)$. In principle, there is no hope that the model works in the second domain. However, we could aim to exploit the sharing knowledge between the 2 domains, for example \citet{Blitzer06Domain} uses pivot features, which are features which occur frequently in the two domains and behave similarly in both. The context of machine translation is much more complicated than binary classification as the label is a structured sequence of symbols, that has to satisfy both the adaquecy according to the source sequence and the fluency according to the target language. \cite{Wees15Whats,Wees17Whats} identified the following elements of text, which influence the translation the most.
\begin{itemize}
	\item Topic: subject of the text such as medical, news, IT or religious. A topic owns its own specific vocabulary, terminologies. These items can not be transferred between distant topic such as medical and religious.
	\item Genre: purpose of the text such as education, talk, report or instruction. It identifies groups of texts that share a common form of transmission, purpose and discourse properties. A genre is characterized by textual style, structure of text, etc.
\end{itemize}

Despite many specifications, domains share the same grammar of the corresponding language. The grammar, the textual style or the structure of text are abstract to machine translation models. MT models learn to translate from examples without being explicitly explained how to do it. They are supposed to learn this knowledge to translate but until now, we are not able to tell whether they do it.

Solving the MT (multi-)domain adaptation problem is essential for deploying MT in real context. Machine Translation has applications in many sectors, such as translating legal documents, news, scientific documents, books, movie subtitles, etc. Every domain has its own specific vocabulary, registers(formal or informal), and genres (e.g., talk, instruction). Therefore, tailoring MT models to a target domain is essential to achieve good translation in that domain. In practice, MT models (SMT, NMT) trained with domain-related data always perform much better in the domain of interest than ones trained with the same amount of less relevant data \citep{Rico13domain, Saunders21domain}. The more domain-relevant data is available, the better the MT system performs in the target domain. However, not every domain has enough data to train an MT models. The state-of-the-art architecture ANMT will need millions parallel sentence pairs to learn its parameters. Therefore, we have to work around in the situation where there is very few data or even no data at all. Domain Adaptation aims to improve performance of MT model in low-resourced domains. Besides, multi-domain adaptation aims to achieve best performance in more than one domain. However, the domains of interest in multi-domain adaptation are not limited to be low-resourced domains. The motivation of having one model adapted to many domains is to optimize the storage, the training time and the deploying time. Having one model per domain increases the storage, the time accessing to a model, thus the latency of the translation. Online translation services, such as Google Translate, Systran Translate or DL Translate, have to translate text from any possible domain while minimizing the latency of translation in order to be beneficial.

In conclusion, the variety of text between fields requires domain adaptation while being fast and robust requires multi-domain adaptation.

\section{Multi-facet of MT (multi-)domain adaptation problem}
\label{sec:multi-facet}
\subsection{From domain adaptation MT to multi-domain adaptation MT}
Even though domain adaptation and multi-domain adaptation do not have same motivation as one focuses on low-resourced domain whereas the other focuses on adapting to as many as possible domains, they can be cast on one general framework. Moreover, the methods proposed for domain adaptation can be applied to multi-domain adaptation and vice versa. Both problems are under a typical setting in machine translation in which we collect the largest possible collection of parallel data for the chosen language pair, with the intent to achieve optimal performance for the task of interest. In such situations, the training data distribution is opportunistic, while the test data distribution is chosen and fixed; a key component in training is then to mitigate the detrimental effects of a possible mismatch between these distributions. 

\subsection{4 main sub-problems}
Recently the work of \citet{Chu18survey} significantly captured the landscape of MT domain adaptation by categorizing methods in 2 main classes: data-centric and model-centric. More precisely, the data-centric category includes methods that manipulate the training distribution to better resemble to the distribution of the target domain. Model-centric methods improve the internalization of domain dependence by alternating the different properties of NMT models, including the architecture, the optimization procedure, and the inference procedure. This taxonomy is largely adopted in MT domain adaptation's research. However, we find a naivety in this classification as it misses to deliver an answer for the most ultimate question that is "which method solves which problem?". We propose adding another dimension, which is the context of MT adaptation problem, to form a Cartesian map of the MT (multi-)domain adaptation area. In this work, we would like to categorize the domain adaptation context by two following questions.
\begin{itemize}
	\item Is the domain label is available in training?
	\item Is the domain label is available in testing?
\end{itemize}
Therefore, there are 4 contexts in 
We believe that this coupling of methods and problems will allow us to better navigate in this forest that has been growing for almost two decades, to put more research on unexplored adaptation problems or to find another application of an existing method.

MT (multi-)domain adaptation is indeed a rich landscape where many problems have not been given a shot. The work of \citet{Pham21revisiting} recently gave an attempt of formulating the context of (multi-)domain adaptation problems.

The domain label is usually extracted from the metadata of the corpora. A domain label can be the name of the field from which the text is collected or just the name of the corpora. Domain labeling depends on the source and the clustering strategy of the researcher. For example, Ubuntu and Gnome can be two different domains or mixed into one domain. The separation facilitates the capturing of the subtle unique properties of the text but loses the generalization, and on the contrary, the combination helps the generalization but loses the in-domain uniqueness. We envy such (multi-)domain adaptation method that is robust to any domain labeling strategy. Domain label is important for almost all adaptation methods. However, this additional information is not always available. Effectively, enormous corpora such as Paracrawl, Commoncrawl, and Newscrawl, which are grossly collected from webs, contain many different text genres and topics but lack domain labels. Exploiting those corpora in (multi-)domain adaptation needs more effort. Unsupervised clustering can be a good direction for this problem \citep{Aharoni20unsupervised,Pham21revisiting}.

We present in 4 following sections 4 principle sub-problems of the (multi-)domain adaptation problem and the groups of methods that matches the requirement of each sub-problem.

\section{Supervised (multi-)domain adaptation}
In the supervised (multi-)domain adaptation, the domain label is available in both training and testing. Furthermore, the domain(s) in testing is (are) available in training. In this problem, we would like to achieve a unique model, that performs the best in one or many target domains, given training data from those domains and probably from other domains. This setting is the most popular situation on which DA research papers focus. 

\subsection{Model-centric}

\subsection{Data-centric}

\section{Unlabeled training, labeled testing}

\subsection{Model-centric}

\subsection{Data-centric}

\section{Labeled training, unlabeled testing}

\subsection{Model-centric}

\subsection{Data-centric}

\section{Unlabeled training, unlabeled testing}

\subsection{Model-centric}

\subsection{Data-centric}













































































































































































































































































































































































